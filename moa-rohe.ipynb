{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014525,
     "end_time": "2020-11-30T13:12:24.683322",
     "exception": false,
     "start_time": "2020-11-30T13:12:24.668797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sixth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:12:24.817960Z",
     "iopub.status.busy": "2020-11-30T13:12:24.721010Z",
     "iopub.status.idle": "2020-11-30T13:14:15.299646Z",
     "shell.execute_reply": "2020-11-30T13:14:15.300238Z"
    },
    "papermill": {
     "duration": 110.603472,
     "end_time": "2020-11-30T13:14:15.300439",
     "exception": false,
     "start_time": "2020-11-30T13:12:24.696967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features: (23814, 876)\n",
      "train_targets_scored: (23814, 207)\n",
      "train_targets_nonscored: (23814, 403)\n",
      "train_drug: (23814, 2)\n",
      "test_features: (3982, 876)\n",
      "sample_submission: (3982, 207)\n",
      "train_features: (23814, 1476)\n",
      "test_features: (3982, 1476)\n",
      "train_features: (23814, 1476)\n",
      "test_features: (3982, 1476)\n",
      "train_features: (23814, 1526)\n",
      "test_features: (3982, 1526)\n",
      "train_features: (23814, 1526)\n",
      "test_features: (3982, 1526)\n",
      "train_features: (23814, 1040)\n",
      "test_features: (3982, 1040)\n",
      "num_targets: 206\n",
      "num_aux_targets: 402\n",
      "num_all_targets: 608\n",
      "(21948, 1648)\n",
      "(3624, 1039)\n",
      "(3982, 207)\n",
      "0.01633610462769866\n",
      "0.016783640049397947\n",
      "0.017410865686833858\n",
      "0.017929619923233987\n",
      "0.01687615916132927\n",
      "0.01757936831563711\n",
      "0.01752924982458353\n",
      "0.0170696235448122\n",
      "0.017208770886063575\n",
      "0.0175430304184556\n",
      "0.017243110612034797\n",
      "0.01680464144796133\n",
      "0.01746361854020506\n",
      "0.017206531018018723\n",
      "0.017807090058922767\n",
      "0.016953775621950627\n",
      "0.017045310996472836\n",
      "0.017147383987903594\n",
      "0.017353615909814834\n",
      "0.017451734356582163\n",
      "0.01708003558218479\n",
      "0.017091768421232702\n",
      "0.016872369572520255\n",
      "0.016994797550141813\n",
      "0.01740357290953398\n",
      "0.017897211015224457\n",
      "0.01730555046349764\n",
      "0.017108676917850972\n",
      "0.017421926520764828\n",
      "0.01788410611450672\n",
      "0.017257814044849232\n",
      "0.017097573533343773\n",
      "0.017109424434602262\n",
      "0.01702553577721119\n",
      "0.016760897003114224\n",
      "0.016990674622356892\n",
      "0.017636973410844803\n",
      "0.01685772731900215\n",
      "0.016965830288827418\n",
      "0.017001787200570107\n",
      "0.017972813166964512\n",
      "0.01709505308419466\n",
      "0.01742395803332329\n",
      "0.01756497487425804\n",
      "0.016708861887454986\n",
      "0.017391960546374322\n",
      "0.01781581446528435\n",
      "0.017246031686663627\n",
      "0.0168968803063035\n",
      "CV log_loss:  0.015610840082896373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3982, 207)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from pickle import load,dump\n",
    "\n",
    "\n",
    "# %%\n",
    "data_dir = '../input/lish-moa/'\n",
    "train_features = pd.read_csv(data_dir + 'train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n",
    "train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n",
    "test_features = pd.read_csv(data_dir + 'test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('train_targets_scored: {}'.format(train_targets_scored.shape))\n",
    "print('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\n",
    "print('train_drug: {}'.format(train_drug.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "print('sample_submission: {}'.format(sample_submission.shape))\n",
    "\n",
    "\n",
    "# %%\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "\n",
    "# %% [markdown]\n",
    "# # RankGauss\n",
    "\n",
    "# %%\n",
    "for col in (GENES + CELLS):\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n",
    "\n",
    "\n",
    "# %%\n",
    "SEED_VALUE = 42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=SEED_VALUE)\n",
    "\n",
    "# %% [markdown]\n",
    "# # PCA features + Existing features\n",
    "\n",
    "# %%\n",
    "# GENES\n",
    "n_comp = 600\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "pca_g = load(open('../input/moa-nn-model4/gpca.pkl', 'rb'))\n",
    "data2 = pca_g.transform(data[GENES])\n",
    "train2 = pca_g.transform(train_features[GENES])\n",
    "test2 = pca_g.transform(test_features[GENES])\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)\n",
    "\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "\n",
    "dump(pca_g, open('gpca.pkl', 'wb'))\n",
    "\n",
    "\n",
    "# %%\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "\n",
    "\n",
    "# %%\n",
    "train_features.head(10)\n",
    "\n",
    "\n",
    "# %%\n",
    "test_features.head(10)\n",
    "\n",
    "\n",
    "# %%\n",
    "# CELLS\n",
    "n_comp = 50\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "\n",
    "pca_c =load(open('../input/moa-nn-model4/cpca.pkl', 'rb'))\n",
    "data2 = pca_c.transform(data[CELLS])\n",
    "train2 = data2[:train_features.shape[0]]\n",
    "test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)\n",
    "\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "\n",
    "dump(pca_c, open('cpca.pkl', 'wb'))\n",
    "\n",
    "\n",
    "# %%\n",
    "train_features.head(10)\n",
    "\n",
    "\n",
    "# %%\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "\n",
    "# %% [markdown]\n",
    "# # feature Selection using Variance Encoding\n",
    "\n",
    "# %%\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thresh = load(open('../input/moa-nn-model4/var_thresh.pkl', 'rb'))\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "\n",
    "dump(var_thresh, open('var_thresh.pkl', 'wb'))\n",
    "\n",
    "\n",
    "# %%\n",
    "train_drug.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "train = train.merge(train_drug, on='sig_id')\n",
    "train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "train.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n",
    "aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n",
    "all_target_cols = target_cols + aux_target_cols\n",
    "\n",
    "num_targets = len(target_cols)\n",
    "num_aux_targets = len(aux_target_cols)\n",
    "num_all_targets = len(all_target_cols)\n",
    "\n",
    "print('num_targets: {}'.format(num_targets))\n",
    "print('num_aux_targets: {}'.format(num_aux_targets))\n",
    "print('num_all_targets: {}'.format(num_all_targets))\n",
    "\n",
    "\n",
    "# %%\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(sample_submission.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Dataset Classes\n",
    "\n",
    "# %%\n",
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "        return dct\n",
    "\n",
    "\n",
    "# %%\n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    return final_loss\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    return preds\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            \n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# %% [markdown]\n",
    "# # Model\n",
    "\n",
    "# %%\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = [1500, 1250, 1000, 750]\n",
    "        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n",
    "        self.dropout2 = nn.Dropout(self.dropout_value[0])\n",
    "        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n",
    "        self.dropout3 = nn.Dropout(self.dropout_value[1])\n",
    "        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n",
    "\n",
    "        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n",
    "        self.dropout4 = nn.Dropout(self.dropout_value[2])\n",
    "        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n",
    "\n",
    "        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n",
    "        self.dropout5 = nn.Dropout(self.dropout_value[3])\n",
    "        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.dense3(x))\n",
    "\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.leaky_relu(self.dense4(x))\n",
    "\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.dense5(x)\n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n",
    "\n",
    "\n",
    "# %%\n",
    "class FineTuneScheduler:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.epochs_per_step = 0\n",
    "        self.frozen_layers = []\n",
    "\n",
    "    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n",
    "        self.frozen_layers = []\n",
    "\n",
    "        model_new = Model(num_features, num_targets)\n",
    "        model_new.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Freeze all weights\n",
    "        for name, param in model_new.named_parameters():\n",
    "            layer_index = name.split('.')[0][-1]\n",
    "\n",
    "            if layer_index == 5:\n",
    "                continue\n",
    "\n",
    "            param.requires_grad = False\n",
    "\n",
    "            # Save frozen layer names\n",
    "            if layer_index not in self.frozen_layers:\n",
    "                self.frozen_layers.append(layer_index)\n",
    "\n",
    "        self.epochs_per_step = self.epochs // len(self.frozen_layers)\n",
    "\n",
    "        # Replace the top layers with another ones\n",
    "        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n",
    "        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n",
    "        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n",
    "        model_new.to(DEVICE)\n",
    "        return model_new\n",
    "\n",
    "    def step(self, epoch, model):\n",
    "        if len(self.frozen_layers) == 0:\n",
    "            return\n",
    "\n",
    "        if epoch % self.epochs_per_step == 0:\n",
    "            last_frozen_index = self.frozen_layers[-1]\n",
    "            \n",
    "            # Unfreeze parameters of the last frozen layer\n",
    "            for name, param in model.named_parameters():\n",
    "                layer_index = name.split('.')[0][-1]\n",
    "\n",
    "                if layer_index == last_frozen_index:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n",
    "\n",
    "# %% [markdown]\n",
    "# # Preprocessing steps\n",
    "\n",
    "# %%\n",
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data\n",
    "\n",
    "\n",
    "# %%\n",
    "feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n",
    "num_features = len(feature_cols)\n",
    "num_features\n",
    "\n",
    "\n",
    "# %%\n",
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 24\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n",
    "MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n",
    "DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n",
    "PCT_START = 0.1\n",
    "\n",
    "\n",
    "# %%\n",
    "# Show model architecture\n",
    "model = Model(num_features, num_all_targets)\n",
    "model\n",
    "\n",
    "# %% [markdown]\n",
    "# # Single fold training\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n",
    "    vc = train.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n",
    "    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n",
    "\n",
    "    for seed_id in range(SEEDS):\n",
    "        kfold_col = 'kfold_{}'.format(seed_id)\n",
    "        \n",
    "        # STRATIFY DRUGS 18X OR LESS\n",
    "        dct1 = {}\n",
    "        dct2 = {}\n",
    "\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.index[idxV].values}\n",
    "            dct1.update(dd)\n",
    "\n",
    "        # STRATIFY DRUGS MORE THAN 18X\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
    "            dct2.update(dd)\n",
    "\n",
    "        # ASSIGN FOLDS\n",
    "        train[kfold_col] = train.drug_id.map(dct1)\n",
    "        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n",
    "        train[kfold_col] = train[kfold_col].astype('int8')\n",
    "        \n",
    "    return train\n",
    "\n",
    "SEEDS = 7\n",
    "NFOLDS = 7\n",
    "DRUG_THRESH = 18\n",
    "\n",
    "train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n",
    "train.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_training(fold_id, seed_id):\n",
    "    seed_everything(seed_id)\n",
    "    \n",
    "    train_ = process_data(train)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    kfold_col = f'kfold_{seed_id}'\n",
    "    trn_idx = train_[train_[kfold_col] != fold_id].index\n",
    "    val_idx = train_[train_[kfold_col] == fold_id].index\n",
    "    \n",
    "    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n",
    "    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n",
    "    # Load the fine-tuned model with the best loss\n",
    "    model = Model(num_features, num_targets)\n",
    "   # model.load_state_dict(torch.load(f\"../input/moa-nn-train-transfer-copy/SCORED_ONLY_FOLD{fold_id}_{seed_id}.pth\", map_location=torch.device('cpu')))\n",
    "    model.load_state_dict(torch.load(f\"../input/moa-nn-model4/SCORED_ONLY_FOLD{fold_id}_{seed_id}.pth\"))\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    #--------------------- PREDICTION---------------------\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "    oof[val_idx] = valid_preds\n",
    "    print(valid_loss)\n",
    "    \n",
    "    \n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), num_targets))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_k_fold(NFOLDS, seed_id):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold_id in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold_id, seed_id)\n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "from time import time\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "SEED = [0, 1, 2, 3, 4, 5, 6]\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "time_begin = time()\n",
    "\n",
    "for seed_id in SEED:\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "time_diff = time() - time_begin\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "from datetime import timedelta\n",
    "str(timedelta(seconds=time_diff))\n",
    "\n",
    "\n",
    "# %%\n",
    "train_targets_scored.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "len(target_cols)\n",
    "\n",
    "\n",
    "# %%\n",
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "oof = train_targets_scored.copy()\n",
    "valid_results = valid_results.add_prefix('pre_')\n",
    "valid_results.rename(columns={'pre_sig_id':'sig_id'}, inplace=True)\n",
    "oof = oof.merge(valid_results, on=['sig_id'], how='left')\n",
    "oof.to_csv('moa_nn_oof.csv', index=False)\n",
    "oof.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "score = 0\n",
    "for i in (target_cols):\n",
    "    score_ = log_loss(oof[i], oof['pre_'+i])\n",
    "    score += score_ / len(target_cols)\n",
    "    \n",
    "print(\"CV log_loss: \", score)\n",
    "\n",
    "\n",
    "# %%\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('sub_NN_4.csv', index=False)\n",
    "\n",
    "\n",
    "# %%\n",
    "sub.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036051,
     "end_time": "2020-11-30T13:14:15.373569",
     "exception": false,
     "start_time": "2020-11-30T13:14:15.337518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fifth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:14:15.470924Z",
     "iopub.status.busy": "2020-11-30T13:14:15.460150Z",
     "iopub.status.idle": "2020-11-30T13:23:46.239571Z",
     "shell.execute_reply": "2020-11-30T13:23:46.238430Z"
    },
    "papermill": {
     "duration": 570.831735,
     "end_time": "2020-11-30T13:23:46.239706",
     "exception": false,
     "start_time": "2020-11-30T13:14:15.407971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004558374926161308\n",
      "0.004570748155506758\n",
      "0.004619356907474307\n",
      "0.0046814738128047725\n",
      "0.004728117527870031\n",
      "0.0044900713345179194\n",
      "0.004582344339444087\n",
      "0.004673159717080684\n",
      "0.004697562625201849\n",
      "0.004590600675258499\n",
      "0.00460310339426192\n",
      "0.004492716290629827\n",
      "0.0046640113354302366\n",
      "0.004628742364450143\n",
      "0.004634617469631708\n",
      "0.004577914969279216\n",
      "0.004656070592598273\n",
      "0.00464775560137171\n",
      "0.004549873061478138\n",
      "0.004593311522442561\n",
      "0.004493510887886469\n",
      "0.004552198824687646\n",
      "0.004635226751606052\n",
      "0.004675115530307476\n",
      "0.004604360142436165\n",
      "0.004679117996532183\n",
      "0.004477441812363954\n",
      "0.004602666209953336\n",
      "0.004655239817041617\n",
      "0.004627885619321695\n",
      "0.004573930108633179\n",
      "0.004708580207079649\n",
      "0.004562148466133154\n",
      "0.004533711104438855\n",
      "0.004545553336636378\n",
      "0.004498718163141837\n",
      "0.004650001432030247\n",
      "0.00472230867196161\n",
      "0.004682630282611801\n",
      "0.004644182098742861\n",
      "0.00459530632584714\n",
      "0.004470884799957275\n",
      "0.004555099822867375\n",
      "0.004623329696746973\n",
      "0.0046087484090374066\n",
      "0.0046526843037169715\n",
      "0.004584927147684189\n",
      "0.004555709839153748\n",
      "0.00460619071068672\n",
      "0.01598810189618514\n",
      "0.016124980022700932\n",
      "0.015952707196657475\n",
      "0.01615025060108075\n",
      "0.015749271027743816\n",
      "0.015802384855655525\n",
      "0.01609184442517849\n",
      "0.016138885743342914\n",
      "0.01588719696379625\n",
      "0.016036266461014748\n",
      "0.016347970240391217\n",
      "0.015871709785782374\n",
      "0.01583733853812401\n",
      "0.01593867954439842\n",
      "0.015975246062645547\n",
      "0.01605141764649978\n",
      "0.015796840119247254\n",
      "0.015950395009265497\n",
      "0.016124909815306846\n",
      "0.015861743201430027\n",
      "0.015911353178895436\n",
      "0.016175769842587985\n",
      "0.01571515095061981\n",
      "0.015899538349073667\n",
      "0.01592327604213586\n",
      "0.0161684389011218\n",
      "0.015828129119024828\n",
      "0.016063766195797004\n",
      "0.0161304000335244\n",
      "0.015997079415963247\n",
      "0.01578772598161147\n",
      "0.01606027280481962\n",
      "0.0157223127495784\n",
      "0.015958361542568758\n",
      "0.016321220578482516\n",
      "0.01610978563817648\n",
      "0.01582002525146191\n",
      "0.016007683526438016\n",
      "0.015920632232267123\n",
      "0.016047699806781914\n",
      "0.015994284946758013\n",
      "0.016075771516905382\n",
      "0.01575101947841736\n",
      "0.016054068978589315\n",
      "0.01583872878780732\n",
      "0.01601427191725144\n",
      "0.016045042385275547\n",
      "0.015938819815906193\n",
      "0.016094872942910746\n",
      "CV log_loss:  0.014541765738774975\n"
     ]
    }
   ],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pickle import load,dump\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# %%\n",
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "df = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "train_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n",
    "\n",
    "\n",
    "# %%\n",
    "train_features2=train_features.copy()\n",
    "test_features2=test_features.copy()\n",
    "\n",
    "\n",
    "# %%\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "\n",
    "\n",
    "# %%\n",
    "for col in (GENES + CELLS):\n",
    "\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n",
    "\n",
    "\n",
    "# %%\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "# %%\n",
    "n_comp = 600  #<--Update\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "gpca = load(open('../input/moa-nn-model/gpca.pkl', 'rb'))\n",
    "train2= (gpca.transform(train_features[GENES]))\n",
    "test2 = (gpca.transform(test_features[GENES]))\n",
    "\n",
    "train_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train_gpca), axis=1)\n",
    "test_features = pd.concat((test_features, test_gpca), axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "#CELLS\n",
    "n_comp = 50  #<--Update\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "cpca = load(open('../input/moa-nn-model/cpca.pkl', 'rb'))\n",
    "train2= (cpca.transform(train_features[CELLS]))\n",
    "test2 = (cpca.transform(test_features[CELLS]))\n",
    "\n",
    "train_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train_cpca), axis=1)\n",
    "test_features = pd.concat((test_features, test_cpca), axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "c_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n",
    "mask = (train_features[c_n].var() >= 0.85).values\n",
    "tmp = train_features[c_n].loc[:, mask]\n",
    "train_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n",
    "tmp = test_features[c_n].loc[:, mask]\n",
    "test_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.cluster import KMeans\n",
    "def fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n",
    "    \n",
    "    features_g = GENES\n",
    "    #features_c = CELLS\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans_genes = load(open('../input/moa-nn-model/kmeans_genes.pkl', 'rb'))\n",
    "        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n",
    "        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "   # train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "    return train, test\n",
    "\n",
    "train_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)\n",
    "\n",
    "\n",
    "# %%\n",
    "def fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n",
    "    \n",
    "    #features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans_cells = load(open('../input/moa-nn-model/kmeans_cells.pkl', 'rb'))\n",
    "        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n",
    "        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "   # train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "    return train, test\n",
    "\n",
    "train_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)\n",
    "\n",
    "\n",
    "# %%\n",
    "train_pca=pd.concat((train_gpca,train_cpca),axis=1)\n",
    "test_pca=pd.concat((test_gpca,test_cpca),axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n",
    "        data=pd.concat([train,test],axis=0)\n",
    "        kmeans_pca = load(open('../input/moa-nn-model/kmeans_pca.pkl', 'rb'))\n",
    "        \n",
    "        train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n",
    "        test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n",
    "        return train, test\n",
    "train_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n",
    "\n",
    "\n",
    "# %%\n",
    "train_cluster_pca = train_cluster_pca.iloc[:,650:]\n",
    "test_cluster_pca = test_cluster_pca.iloc[:,650:]\n",
    "\n",
    "\n",
    "# %%\n",
    "train_features_cluster=train_features2.iloc[:,876:]\n",
    "test_features_cluster=test_features2.iloc[:,876:]\n",
    "\n",
    "\n",
    "# %%\n",
    "gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n",
    "\n",
    "\n",
    "# %%\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "        df['c52_c42'] = df['c-52'] * df['c-42']\n",
    "        df['c13_c73'] = df['c-13'] * df['c-73']\n",
    "        df['c26_c13'] = df['c-23'] * df['c-13']\n",
    "        df['c33_c6'] = df['c-33'] * df['c-6']\n",
    "        df['c11_c55'] = df['c-11'] * df['c-55']\n",
    "        df['c38_c63'] = df['c-38'] * df['c-63']\n",
    "        df['c38_c94'] = df['c-38'] * df['c-94']\n",
    "        df['c13_c94'] = df['c-13'] * df['c-94']\n",
    "        df['c4_c52'] = df['c-4'] * df['c-52']\n",
    "        df['c4_c42'] = df['c-4'] * df['c-42']\n",
    "        df['c13_c38'] = df['c-13'] * df['c-38']\n",
    "        df['c55_c2'] = df['c-55'] * df['c-2']\n",
    "        df['c55_c4'] = df['c-55'] * df['c-4']\n",
    "        df['c4_c13'] = df['c-4'] * df['c-13']\n",
    "        df['c82_c42'] = df['c-82'] * df['c-42']\n",
    "        df['c66_c42'] = df['c-66'] * df['c-42']\n",
    "        df['c6_c38'] = df['c-6'] * df['c-38']\n",
    "        df['c2_c13'] = df['c-2'] * df['c-13']\n",
    "        df['c62_c42'] = df['c-62'] * df['c-42']\n",
    "        df['c90_c55'] = df['c-90'] * df['c-55']\n",
    "        \n",
    "        \n",
    "        for feature in features_c:\n",
    "             df[f'{feature}_squared'] = df[feature] ** 2     \n",
    "                \n",
    "        for feature in gsquarecols:\n",
    "            df[f'{feature}_squared'] = df[feature] ** 2        \n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features2,test_features2=fe_stats(train_features2,test_features2)\n",
    "\n",
    "\n",
    "# %%\n",
    "train_features_stats=train_features2.iloc[:,902:]\n",
    "test_features_stats=test_features2.iloc[:,902:]\n",
    "\n",
    "\n",
    "# %%\n",
    "train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\n",
    "test_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train_features.merge(train_targets_nonscored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_nonscored.columns]\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "\n",
    "# %%\n",
    "train = pd.get_dummies(train, columns=['cp_time','cp_dose'])\n",
    "test_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])\n",
    "\n",
    "\n",
    "# %%\n",
    "feature_cols = [c for c in train.columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['sig_id']]\n",
    "\n",
    "\n",
    "# %%\n",
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "\n",
    "\n",
    "# %%\n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# %%\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.dense1(x), 1e-3)\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# %%\n",
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 26\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 6e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = True\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=2048\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    mskf = MultilabelStratifiedKFold(n_splits=7,random_state=seed)\n",
    "    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "         train.loc[v_idx, 'kfold'] = int(f)\n",
    "    train['kfold'] = train['kfold'].astype(int)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f\"../input/moa-nn-model/SEED{seed}_FOLD{fold}_nonscored.pth\"))\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "    oof[val_idx] = valid_preds\n",
    "    print(valid_loss)    \n",
    "    \n",
    "    \n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [0,1,2,3,4,5,6]  #<-- Update\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test_[target_cols] = predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train.merge(train_targets_scored, on='sig_id')\n",
    "target = train[train_targets_scored.columns]\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "\n",
    "# %%\n",
    "feature_cols = [c for c in train.columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['sig_id','kfold']]\n",
    "\n",
    "\n",
    "# %%\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 26\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 6e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = True\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=2048\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    mskf = MultilabelStratifiedKFold(n_splits=7,random_state=seed)\n",
    "    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "         train.loc[v_idx, 'kfold'] = int(f)\n",
    "    train['kfold'] = train['kfold'].astype(int)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f\"../input/moa-nn-model/SEED{seed}_FOLD{fold}_scored.pth\"))\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "    oof[val_idx] = valid_preds\n",
    "    print(valid_loss)    \n",
    "    \n",
    "    \n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [0,1,2,3,4,5,6]  #<-- Update\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / len(target_cols)\n",
    "    \n",
    "print(\"CV log_loss: \", score)\n",
    "    \n",
    "\n",
    "\n",
    "# %%\n",
    "public_id = list(df['sig_id'].values)\n",
    "test_id = list(test_features['sig_id'].values)\n",
    "private_id = list(set(test_id)-set(public_id))\n",
    "df_submit = pd.DataFrame(index = public_id+private_id, columns=target_cols)\n",
    "df_submit.index.name = 'sig_id'\n",
    "df_submit[:] = 0\n",
    "df_submit.loc[test.sig_id,:] = test[target_cols].values\n",
    "df_submit.loc[test_features[test_features.cp_type=='ctl_vehicle'].sig_id]= 0\n",
    "df_submit.to_csv('sub_NN_3.csv',index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062794,
     "end_time": "2020-11-30T13:23:46.369001",
     "exception": false,
     "start_time": "2020-11-30T13:23:46.306207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:23:46.542856Z",
     "iopub.status.busy": "2020-11-30T13:23:46.527082Z",
     "iopub.status.idle": "2020-11-30T13:25:30.255139Z",
     "shell.execute_reply": "2020-11-30T13:25:30.255768Z"
    },
    "papermill": {
     "duration": 103.82236,
     "end_time": "2020-11-30T13:25:30.255973",
     "exception": false,
     "start_time": "2020-11-30T13:23:46.433613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: typing 3.7.4.3\r\n",
      "Uninstalling typing-3.7.4.3:\r\n",
      "  Successfully uninstalled typing-3.7.4.3\r\n",
      "Processing /kaggle/input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.18.5)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.4.1)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (4.45.0)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.6.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==2.0.1) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==2.0.1) (0.14.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==2.0.1) (0.18.2)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.1\r\n",
      "(21948, 1086)\n",
      "(21948, 1086)\n",
      "(3624, 875)\n",
      "(21948, 207)\n",
      "(3982, 207)\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "CV log_loss:  0.015665928517910092\n",
      "CV AUC:  0.6707558040207743\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.016006</td>\n",
       "      <td>0.022213</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.002685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.014072</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.021083</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.002190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.024183</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.001820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001091                0.000990   \n",
       "1  id_001897cda                     0.000570                0.000950   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.001157                0.001207   \n",
       "4  id_0027f1083                     0.001734                0.001559   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.001917                        0.016006   \n",
       "1        0.002104                        0.002340   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.001672                        0.009913   \n",
       "4        0.001958                        0.019246   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.022213                        0.005090   \n",
       "1                           0.001574                        0.001966   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.014072                        0.004203   \n",
       "4                           0.024183                        0.004626   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002669                       0.006430   \n",
       "1                    0.002473                       0.010145   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.002665                       0.005191   \n",
       "4                    0.004750                       0.002949   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.000350  ...                               0.000937   \n",
       "1                    0.002058  ...                               0.000871   \n",
       "2                    0.000000  ...                               0.000000   \n",
       "3                    0.000488  ...                               0.000669   \n",
       "4                    0.000631  ...                               0.000797   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.000786         0.002975           0.000965   \n",
       "1      0.001378         0.003699           0.001091   \n",
       "2      0.000000         0.000000           0.000000   \n",
       "3      0.002070         0.002777           0.021083   \n",
       "4      0.000975         0.003703           0.001108   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.000799                               0.000615   \n",
       "1                   0.007105                               0.000663   \n",
       "2                   0.000000                               0.000000   \n",
       "3                   0.006821                               0.000766   \n",
       "4                   0.001424                               0.000743   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.000716   0.002160                    0.003511       0.001535  \n",
       "1         0.011186   0.001294                    0.010900       0.002685  \n",
       "2         0.000000   0.000000                    0.000000       0.000000  \n",
       "3         0.002139   0.001855                    0.000905       0.002190  \n",
       "4         0.001039   0.002154                    0.000452       0.001820  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cp ../input/moa-tabnet-train-copy/*.zip ./\n",
    "!pip uninstall -y typing\n",
    "!pip install ../input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# %%\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "seed_everything(42)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Data\n",
    "# \n",
    "\n",
    "# %%\n",
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/moa-make-foldsmoa-make-folds/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/moa-make-foldsmoa-make-folds/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "\n",
    "\n",
    "# %%\n",
    "#PCA\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "\n",
    "\n",
    "# %%\n",
    "for col in (GENES + CELLS):\n",
    "    transformer = QuantileTransformer(n_quantiles=115, random_state=0, output_distribution=\"normal\")   #50 75 100 125 150 25 \n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "target = train[train_targets_scored.columns[:207]]\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "\n",
    "# %%\n",
    "folds = train.copy()\n",
    "folds.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)\n",
    "print(sample_submission.shape)\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "def process_data(data):\n",
    "    \n",
    "    data['cp_time'] = lb.fit_transform(data['cp_time'])\n",
    "    data['cp_dose'] = lb.fit_transform(data['cp_dose'])\n",
    "    return data\n",
    "\n",
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['fold_42','sig_id','fold_0','fold_1','fold_2','fold_3']]\n",
    "len(feature_cols)\n",
    "\n",
    "\n",
    "# %%\n",
    "def evals(model, X, y, verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict(X)\n",
    "        y_preds = torch.clamp(y_preds, 0.0,1.0).detach().numpy()\n",
    "    score = log_loss_multi(y, y_preds)\n",
    "    #print(\"Logloss = \", score)\n",
    "    return y_preds, score\n",
    "\n",
    "\n",
    "def inference_fn(model, X ,verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict( X )\n",
    "        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n",
    "    return y_preds\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "    p1 = actual * np.log(predicted+eps)\n",
    "    p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "    loss = p0 + p1\n",
    "\n",
    "    return -loss.mean()\n",
    "\n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()\n",
    "\n",
    "def check_targets(targets):\n",
    "    ### check if targets are all binary in training set\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        if len(np.unique(targets[:,i])) != 2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def auc_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        try:\n",
    "            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
    "        except:\n",
    "            pass\n",
    "    return results.mean()\n",
    "\n",
    "\n",
    "# %%\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = len(target_cols)\n",
    "        self.verbose=False\n",
    "        self.seed = 0\n",
    "        self.device =  \"cuda\"\n",
    "        self.EPOCHS = 300\n",
    "        self.num_ensembling = 1\n",
    "        # Parameters model\n",
    "        self.cat_emb_dim=[1] * 2 #to choose\n",
    "        self.cats_idx = list(range(2))\n",
    "        self.cat_dims = [3,2]\n",
    "        self.num_numericals= len(feature_cols)-2\n",
    "        self.patience = 50\n",
    "        self.batch_size=1024\n",
    "        self.NFOLDS = 7\n",
    "    \n",
    "        # save\n",
    "        self.save_name = \"/kaggle/working/tabnet_raw_step1\"\n",
    "        \n",
    "        self.strategy = \"KFOLD\" # \n",
    "cfg = Config()\n",
    "\n",
    "\n",
    "# %%\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "\n",
    "# %%\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "class LogitsLogLoss(Metric):\n",
    "    \"\"\"\n",
    "    LogLoss with sigmoid applied\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute LogLoss of predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.ndarray\n",
    "            Target matrix or vector\n",
    "        y_score: np.ndarray\n",
    "            Score matrix or vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float\n",
    "            LogLoss of predictions vs targets.\n",
    "        \"\"\"\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n",
    "        return np.mean(-aux)\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train[f'fold_{seed}'] != fold].index\n",
    "    val_idx = train[train[f'fold_{seed}'] == fold].index\n",
    "    \n",
    "    train_df = train[train[f'fold_{seed}'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train[f'fold_{seed}'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    X_train, y_train = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    X_val, y_val = valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "            \n",
    "    model = TabNetRegressor(n_d=24, n_a=64, n_steps=1, \n",
    "                            n_independent=1, n_shared=1,\n",
    "                            gamma=1.2, lambda_sparse=0, \n",
    "                            cat_dims=cfg.cat_dims, \n",
    "                            cat_emb_dim=cfg.cat_emb_dim, \n",
    "                            cat_idxs=cfg.cats_idx, \n",
    "                            optimizer_fn=torch.optim.Adam,\n",
    "                            optimizer_params=dict(lr=2e-2, weight_decay=1e-5), \n",
    "                            mask_type='entmax', \n",
    "                            device_name=cfg.device, \n",
    "                            scheduler_params=dict(mode='min', factor=0.1, patience=10), \n",
    "                            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "    \n",
    "\n",
    "    name = cfg.save_name + f\"_fold{fold}_{seed}.zip\"\n",
    "    model.load_model(name)\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    valid_preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "    score = log_loss_multi(y_val, preds)\n",
    "    \n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    oof[val_idx] = valid_preds\n",
    "    \n",
    "    x_test = torch.as_tensor(test[feature_cols].values)\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = torch.sigmoid(torch.as_tensor(predictions)).detach().cpu().numpy()\n",
    "    \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "# Averaging on multiple SEEDS\n",
    "SEED = [42,0,1,2,3] #<-- Update\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(cfg.NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "len(target_cols)\n",
    "\n",
    "\n",
    "# %%\n",
    "valid_results = train_targets_scored.drop(columns=target_cols+['fold_42','fold_0','fold_1','fold_2','fold_3']).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "valid_results = valid_results.add_prefix('pre_')\n",
    "valid_results.rename(columns={'pre_sig_id':'sig_id'}, inplace=True)\n",
    "\n",
    "\n",
    "# %%\n",
    "oof = train_targets_scored.drop(columns=['fold_42','fold_0','fold_1','fold_2','fold_3'],axis=1)\n",
    "oof = oof.merge(valid_results, on=['sig_id'], how='left')\n",
    "oof.to_csv('moa_nn_oof.csv', index=False)\n",
    "\n",
    "\n",
    "# %%\n",
    "oof.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "score = 0\n",
    "for i in (target_cols):\n",
    "    score_ = log_loss(oof[i], oof['pre_'+i])\n",
    "    score += score_ / 206\n",
    "    \n",
    "print(\"CV log_loss: \", score)\n",
    "\n",
    "\n",
    "# %%\n",
    "auc_score = 0\n",
    "for i in (target_cols):\n",
    "    score_ = roc_auc_score(oof[i], oof['pre_'+i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV AUC: \", score)\n",
    "\n",
    "\n",
    "# %%\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('sub_tabnet_new.csv', index=False)\n",
    "sub.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.159049,
     "end_time": "2020-11-30T13:25:30.566962",
     "exception": false,
     "start_time": "2020-11-30T13:25:30.407913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:25:30.911868Z",
     "iopub.status.busy": "2020-11-30T13:25:30.897356Z",
     "iopub.status.idle": "2020-11-30T13:26:30.655926Z",
     "shell.execute_reply": "2020-11-30T13:26:30.654935Z"
    },
    "papermill": {
     "duration": 59.960857,
     "end_time": "2020-11-30T13:26:30.656057",
     "exception": false,
     "start_time": "2020-11-30T13:25:30.695200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features: (23814, 876)\n",
      "train_targets_scored: (23814, 207)\n",
      "train_targets_nonscored: (23814, 403)\n",
      "train_drug: (23814, 2)\n",
      "test_features: (3982, 876)\n",
      "sample_submission: (3982, 207)\n",
      "train_features: (23814, 876)\n",
      "test_features: (3982, 876)\n",
      "num_targets: 206\n",
      "num_aux_targets: 402\n",
      "num_all_targets: 608\n",
      "(21948, 1484)\n",
      "(3624, 875)\n",
      "(3982, 207)\n",
      "0.016176991108804942\n",
      "0.016749752685427666\n",
      "0.017320991829037666\n",
      "0.01788912810385227\n",
      "0.016869103349745274\n",
      "0.01758917175233364\n",
      "0.017477143965661527\n",
      "0.017077555134892463\n",
      "0.017257048450410366\n",
      "0.017480614744126797\n",
      "0.01716004692018032\n",
      "0.016816565282642842\n",
      "0.01749589150616278\n",
      "0.0171658967807889\n",
      "0.01767233368009329\n",
      "0.016906789019703865\n",
      "0.016980206184089183\n",
      "0.017165562696754934\n",
      "0.017245474234223367\n",
      "0.017523513846099376\n",
      "0.01708345178514719\n",
      "0.017390556409955026\n",
      "0.01683481063693762\n",
      "0.01697103600949049\n",
      "0.01739305965602398\n",
      "0.01783582702279091\n",
      "0.017240106500685214\n",
      "0.017017642818391323\n",
      "CV log_loss:  0.015672487052094577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3982, 207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# %%\n",
    "data_dir = '../input/lish-moa/'\n",
    "train_features = pd.read_csv(data_dir + 'train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n",
    "train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n",
    "test_features = pd.read_csv(data_dir + 'test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('train_targets_scored: {}'.format(train_targets_scored.shape))\n",
    "print('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\n",
    "print('train_drug: {}'.format(train_drug.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "print('sample_submission: {}'.format(sample_submission.shape))\n",
    "\n",
    "\n",
    "# %%\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "\n",
    "# %% [markdown]\n",
    "# # RankGauss\n",
    "\n",
    "# %%\n",
    "for col in (GENES + CELLS):\n",
    "    transformer = QuantileTransformer(n_quantiles=90,random_state=0, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n",
    "\n",
    "\n",
    "# %%\n",
    "SEED_VALUE = 42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=SEED_VALUE)\n",
    "\n",
    "# %% [markdown]\n",
    "# # feature Selection using Variance Encoding\n",
    "\n",
    "# %%\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "print('train_features: {}'.format(train_features.shape))\n",
    "print('test_features: {}'.format(test_features.shape))\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "train = train.merge(train_drug, on='sig_id')\n",
    "train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# %%\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n",
    "aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n",
    "all_target_cols = target_cols + aux_target_cols\n",
    "\n",
    "num_targets = len(target_cols)\n",
    "num_aux_targets = len(aux_target_cols)\n",
    "num_all_targets = len(all_target_cols)\n",
    "\n",
    "print('num_targets: {}'.format(num_targets))\n",
    "print('num_aux_targets: {}'.format(num_aux_targets))\n",
    "print('num_all_targets: {}'.format(num_all_targets))\n",
    "\n",
    "\n",
    "# %%\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(sample_submission.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Dataset Classes\n",
    "\n",
    "# %%\n",
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "        return dct\n",
    "\n",
    "\n",
    "# %%\n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    return final_loss\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    return preds\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            \n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# %% [markdown]\n",
    "# # Model\n",
    "\n",
    "# %%\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = [1500, 1250, 1000, 750]\n",
    "        self.dropout_value = [0.5, 0.4, 0.3, 0.25]\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n",
    "        self.dropout2 = nn.Dropout(self.dropout_value[0])\n",
    "        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n",
    "        self.dropout3 = nn.Dropout(self.dropout_value[1])\n",
    "        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n",
    "\n",
    "        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n",
    "        self.dropout4 = nn.Dropout(self.dropout_value[2])\n",
    "        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n",
    "\n",
    "        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n",
    "        self.dropout5 = nn.Dropout(self.dropout_value[3])\n",
    "        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.dense3(x))\n",
    "\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.leaky_relu(self.dense4(x))\n",
    "\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.dense5(x)\n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n",
    "\n",
    "\n",
    "# %%\n",
    "class FineTuneScheduler:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.epochs_per_step = 0\n",
    "        self.frozen_layers = []\n",
    "\n",
    "    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n",
    "        self.frozen_layers = []\n",
    "\n",
    "        model_new = Model(num_features, num_targets)\n",
    "        model_new.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Freeze all weights\n",
    "        for name, param in model_new.named_parameters():\n",
    "            layer_index = name.split('.')[0][-1]\n",
    "\n",
    "            if layer_index == 5:\n",
    "                continue\n",
    "\n",
    "            param.requires_grad = False\n",
    "\n",
    "            # Save frozen layer names\n",
    "            if layer_index not in self.frozen_layers:\n",
    "                self.frozen_layers.append(layer_index)\n",
    "\n",
    "        self.epochs_per_step = self.epochs // len(self.frozen_layers)\n",
    "\n",
    "        # Replace the top layers with another ones\n",
    "        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n",
    "        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n",
    "        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n",
    "        model_new.to(DEVICE)\n",
    "        return model_new\n",
    "\n",
    "    def step(self, epoch, model):\n",
    "        if len(self.frozen_layers) == 0:\n",
    "            return\n",
    "\n",
    "        if epoch % self.epochs_per_step == 0:\n",
    "            last_frozen_index = self.frozen_layers[-1]\n",
    "            \n",
    "            # Unfreeze parameters of the last frozen layer\n",
    "            for name, param in model.named_parameters():\n",
    "                layer_index = name.split('.')[0][-1]\n",
    "\n",
    "                if layer_index == last_frozen_index:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n",
    "\n",
    "# %% [markdown]\n",
    "# # Preprocessing steps\n",
    "\n",
    "# %%\n",
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data\n",
    "\n",
    "\n",
    "# %%\n",
    "feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n",
    "num_features = len(feature_cols)\n",
    "num_features\n",
    "\n",
    "\n",
    "# %%\n",
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 48\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n",
    "MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n",
    "DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n",
    "PCT_START = 0.1\n",
    "\n",
    "\n",
    "# %%\n",
    "# Show model architecture\n",
    "model = Model(num_features, num_all_targets)\n",
    "model\n",
    "\n",
    "# %% [markdown]\n",
    "# # Single fold training\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n",
    "    vc = train.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n",
    "    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n",
    "\n",
    "    for seed_id in range(SEEDS):\n",
    "        kfold_col = 'kfold_{}'.format(seed_id)\n",
    "        \n",
    "        # STRATIFY DRUGS 18X OR LESS\n",
    "        dct1 = {}\n",
    "        dct2 = {}\n",
    "\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.index[idxV].values}\n",
    "            dct1.update(dd)\n",
    "\n",
    "        # STRATIFY DRUGS MORE THAN 18X\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
    "            dct2.update(dd)\n",
    "\n",
    "        # ASSIGN FOLDS\n",
    "        train[kfold_col] = train.drug_id.map(dct1)\n",
    "        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n",
    "        train[kfold_col] = train[kfold_col].astype('int8')\n",
    "        \n",
    "    return train\n",
    "\n",
    "SEEDS = 7\n",
    "NFOLDS = 7\n",
    "DRUG_THRESH = 18\n",
    "\n",
    "train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n",
    "train.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_training(fold_id, seed_id):\n",
    "    seed_everything(seed_id)\n",
    "    \n",
    "    train_ = process_data(train)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    kfold_col = f'kfold_{seed_id}'\n",
    "    trn_idx = train_[train_[kfold_col] != fold_id].index\n",
    "    val_idx = train_[train_[kfold_col] == fold_id].index\n",
    "    \n",
    "    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n",
    "    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # Load the fine-tuned model with the best loss\n",
    "    model = Model(num_features, num_targets)\n",
    "    model.load_state_dict(torch.load(f\"../input/moa-nn-transfer-dataset/SCORED_ONLY_FOLD{fold_id}_{seed_id}.pth\"))\n",
    "    #model.load_state_dict(torch.load(f\"../input/moa-nn-train-transfer/SCORED_ONLY_FOLD{fold_id}_{seed_id}.pth\"))\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    #--------------------- PREDICTION---------------------\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "    oof[val_idx] = valid_preds\n",
    "    print(valid_loss)\n",
    "    \n",
    "    \n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), num_targets))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_k_fold(NFOLDS, seed_id):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold_id in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold_id, seed_id)\n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "from time import time\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "SEED = [0,1,2,3]\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "time_begin = time()\n",
    "\n",
    "for seed_id in SEED:\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "time_diff = time() - time_begin\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "\n",
    "# %%\n",
    "from datetime import timedelta\n",
    "str(timedelta(seconds=time_diff))\n",
    "\n",
    "\n",
    "# %%\n",
    "train_targets_scored.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "len(target_cols)\n",
    "\n",
    "\n",
    "# %%\n",
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "oof = train_targets_scored.copy()\n",
    "valid_results = valid_results.add_prefix('pre_')\n",
    "valid_results.rename(columns={'pre_sig_id':'sig_id'}, inplace=True)\n",
    "oof = oof.merge(valid_results, on=['sig_id'], how='left')\n",
    "#oof.to_csv('moa_nn_oof.csv', index=False)\n",
    "oof.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "score = 0\n",
    "for i in (target_cols):\n",
    "    score_ = log_loss(oof[i], oof['pre_'+i])\n",
    "    score += score_ / len(target_cols)\n",
    "    \n",
    "print(\"CV log_loss: \", score)\n",
    "\n",
    "\n",
    "# %%\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('sub_NN_transfer.csv', index=False)\n",
    "\n",
    "\n",
    "# %%\n",
    "sub.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.09251,
     "end_time": "2020-11-30T13:26:30.848201",
     "exception": false,
     "start_time": "2020-11-30T13:26:30.755691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Seventh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:26:31.069398Z",
     "iopub.status.busy": "2020-11-30T13:26:31.053743Z",
     "iopub.status.idle": "2020-11-30T13:31:47.510926Z",
     "shell.execute_reply": "2020-11-30T13:31:47.509799Z"
    },
    "papermill": {
     "duration": 316.572084,
     "end_time": "2020-11-30T13:31:47.511098",
     "exception": false,
     "start_time": "2020-11-30T13:26:30.939014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "OOF score is  0.0165783074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n",
    "non_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\n",
    "train_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\n",
    "train_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_scored = train_targets_scored.drop('sig_id',axis=1)\n",
    "labels_train = train_targets_scored.values\n",
    "\n",
    "# Drop training data with ctl vehicle\n",
    "\n",
    "train_features = train_features.iloc[non_ctl_idx]\n",
    "labels_train = labels_train[non_ctl_idx]\n",
    "\n",
    "# Import test data\n",
    "\n",
    "test_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n",
    "test_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n",
    "\n",
    "# Import predictors from public kernel\n",
    "\n",
    "json_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json'\n",
    "\n",
    "with open(json_file_path, 'r') as j:\n",
    "    predictors = json.loads(j.read())\n",
    "    predictors = predictors['start_predictors']\n",
    "    \n",
    "    \n",
    "cs = train_features.columns.str.contains('c-')\n",
    "gs = train_features.columns.str.contains('g-')\n",
    "\n",
    "def preprocessor(train,test):\n",
    "    \n",
    "    # PCA\n",
    "    \n",
    "    n_gs = 2 # No of PCA comps to include\n",
    "    n_cs = 100 # No of PCA comps to include\n",
    "    \n",
    "    pca_cs = PCA(n_components = n_cs)\n",
    "    pca_gs = PCA(n_components = n_gs)\n",
    "\n",
    "    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n",
    "    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n",
    "    test_pca_gs = pca_gs.transform(test[:,gs])\n",
    "    test_pca_cs = pca_cs.transform(test[:,cs])\n",
    "    \n",
    "    # c-mean, g-mean\n",
    "    \n",
    "    train_c_mean = train[:,cs].mean(axis=1)\n",
    "    test_c_mean = test[:,cs].mean(axis=1)\n",
    "    train_g_mean = train[:,gs].mean(axis=1)\n",
    "    test_g_mean = test[:,gs].mean(axis=1)\n",
    "    \n",
    "    # Append Features\n",
    "    \n",
    "    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n",
    "                            ,train_g_mean[:,np.newaxis]),axis=1)\n",
    "    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n",
    "                           test_g_mean[:,np.newaxis]),axis=1)\n",
    "    \n",
    "    # Scaler for numerical values\n",
    "\n",
    "    # Scale train data\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    train = scaler.fit_transform(train)\n",
    "\n",
    "    # Scale Test data\n",
    "    test = scaler.transform(test)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "n_labels = train_targets_scored.shape[1]\n",
    "n_train = train_features.shape[0]\n",
    "n_test = test_features.shape[0]\n",
    "\n",
    "\n",
    "# Prediction Clipping Thresholds\n",
    "\n",
    "p_min = 0.0005\n",
    "p_max = 0.9995\n",
    "\n",
    "# OOF Evaluation Metric with clipping and no label smoothing\n",
    "\n",
    "def logloss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n",
    "    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n",
    "\n",
    "\n",
    "def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n",
    "    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n",
    "    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n",
    "\n",
    "    head_1 = Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(512, activation=\"elu\"), \n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation = \"elu\")\n",
    "        ],name='Head1') \n",
    "\n",
    "    input_3 = head_1(input_1)\n",
    "    input_3_concat = layers.Concatenate()([input_2, input_3])\n",
    "\n",
    "    head_2 = Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(512, \"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, \"elu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, \"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, \"elu\")\n",
    "        ],name='Head2')\n",
    "\n",
    "    input_4 = head_2(input_3_concat)\n",
    "    input_4_avg = layers.Average()([input_3, input_4]) \n",
    "\n",
    "    head_3 = Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(n_labels, activation=\"sigmoid\")\n",
    "        ],name='Head3')\n",
    "\n",
    "    output = head_3(input_4_avg)\n",
    "\n",
    "\n",
    "    model = Model(inputs = [input_1, input_2], outputs = output)\n",
    "    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "n_seeds = 7\n",
    "np.random.seed(1)\n",
    "seeds = np.random.randint(0,100,size=n_seeds)\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "n_folds = 10\n",
    "y_pred = np.zeros((n_test,n_labels))\n",
    "oof = tf.constant(0.0)\n",
    "hists = []\n",
    "for seed in seeds:\n",
    "    fold = 0\n",
    "    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n",
    "    for train, test in kf.split(train_features):\n",
    "        X_train, X_test = preprocessor(train_features.iloc[train].values,\n",
    "                                       train_features.iloc[test].values)\n",
    "        _,data_test = preprocessor(train_features.iloc[train].values,\n",
    "                                   test_features.drop('cp_type',axis=1).values)\n",
    "        X_train_2 = train_features.iloc[train][predictors].values\n",
    "        X_test_2 = train_features.iloc[test][predictors].values\n",
    "        data_test_2 = test_features[predictors].values\n",
    "        y_train = labels_train[train]\n",
    "        y_test = labels_train[test]\n",
    "        n_features = X_train.shape[1]\n",
    "        n_features_2 = X_train_2.shape[1]\n",
    "\n",
    "        model = build_model(n_features, n_features_2, n_labels)\n",
    "        \n",
    "#         reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n",
    "#         early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n",
    "\n",
    "#         hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=192,verbose=1,validation_data = ([X_test,X_test_2],y_test),\n",
    "#                          callbacks=[reduce_lr, early_stopping])\n",
    "#         hists.append(hist)\n",
    "        \n",
    "        # Save Model\n",
    "        #model.save(f'TwoHeads_seed_{seed}_fold_{fold}.h5')\n",
    "        \n",
    "        model.load_weights(f'../input/moa-resnet-model2/TwoHeads_seed_{seed}_fold_{fold}.h5')\n",
    "        print('True')\n",
    "\n",
    "        # OOF Score\n",
    "        y_val = model.predict([X_test,X_test_2])\n",
    "        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n",
    "\n",
    "        # Run prediction\n",
    "        y_pred += model.predict([data_test,data_test_2])/(n_folds*n_seeds)\n",
    "\n",
    "        fold += 1\n",
    "        \n",
    "        \n",
    "tf.print('OOF score is ',oof)\n",
    "\n",
    "# Generate submission file, Clip Predictions\n",
    "\n",
    "sub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n",
    "#sub.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n",
    "sub.iloc[:,1:] = y_pred\n",
    "\n",
    "# Set ctl_vehicle to 0\n",
    "sub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n",
    "\n",
    "# Save Submission\n",
    "sub.to_csv('sub_resnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:31:47.747042Z",
     "iopub.status.busy": "2020-11-30T13:31:47.743961Z",
     "iopub.status.idle": "2020-11-30T13:31:47.750399Z",
     "shell.execute_reply": "2020-11-30T13:31:47.750861Z"
    },
    "papermill": {
     "duration": 0.125317,
     "end_time": "2020-11-30T13:31:47.751003",
     "exception": false,
     "start_time": "2020-11-30T13:31:47.625686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tabnet_raw_step1_fold1_3.zip',\n",
       " 'tabnet_raw_step1_fold2_42.zip',\n",
       " 'tabnet_raw_step1_fold0_1.zip',\n",
       " 'tabnet_raw_step1_fold5_1.zip',\n",
       " 'tabnet_raw_step1_fold1_2.zip',\n",
       " 'tabnet_raw_step1_fold4_2.zip',\n",
       " 'tabnet_raw_step1_fold2_0.zip',\n",
       " 'tabnet_raw_step1_fold3_42.zip',\n",
       " 'tabnet_raw_step1_fold4_0.zip',\n",
       " 'tabnet_raw_step1_fold2_2.zip',\n",
       " 'tabnet_raw_step1_fold5_3.zip',\n",
       " 'tabnet_raw_step1_fold3_3.zip',\n",
       " 'tabnet_raw_step1_fold3_2.zip',\n",
       " '__notebook__.ipynb',\n",
       " 'tabnet_raw_step1_fold4_42.zip',\n",
       " 'sub_resnet.csv',\n",
       " 'sub_tabnet_new.csv',\n",
       " 'tabnet_raw_step1_fold5_0.zip',\n",
       " 'tabnet_raw_step1_fold1_1.zip',\n",
       " 'tabnet_raw_step1_fold0_2.zip',\n",
       " 'tabnet_raw_step1_fold1_42.zip',\n",
       " 'tabnet_raw_step1_fold0_42.zip',\n",
       " 'gpca.pkl',\n",
       " 'cpca.pkl',\n",
       " 'tabnet_raw_step1_fold3_1.zip',\n",
       " 'tabnet_raw_step1_fold0_0.zip',\n",
       " 'var_thresh.pkl',\n",
       " 'tabnet_raw_step1_fold6_2.zip',\n",
       " 'tabnet_raw_step1_fold2_3.zip',\n",
       " 'tabnet_raw_step1_fold4_1.zip',\n",
       " 'tabnet_raw_step1_fold5_2.zip',\n",
       " 'tabnet_raw_step1_fold4_3.zip',\n",
       " 'sub_NN_4.csv',\n",
       " 'sub_NN_transfer.csv',\n",
       " 'tabnet_raw_step1_fold1_0.zip',\n",
       " 'moa_nn_oof.csv',\n",
       " 'tabnet_raw_step1_fold3_0.zip',\n",
       " 'tabnet_raw_step1_fold2_1.zip',\n",
       " 'tabnet_raw_step1_fold5_42.zip',\n",
       " 'sub_NN_3.csv',\n",
       " 'tabnet_raw_step1_fold6_1.zip',\n",
       " 'tabnet_raw_step1_fold6_0.zip',\n",
       " 'tabnet_raw_step1_fold0_3.zip',\n",
       " 'tabnet_raw_step1_fold6_3.zip',\n",
       " 'tabnet_raw_step1_fold6_42.zip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:31:47.997255Z",
     "iopub.status.busy": "2020-11-30T13:31:47.996425Z",
     "iopub.status.idle": "2020-11-30T13:31:51.165842Z",
     "shell.execute_reply": "2020-11-30T13:31:51.165293Z"
    },
    "papermill": {
     "duration": 3.295845,
     "end_time": "2020-11-30T13:31:51.165956",
     "exception": false,
     "start_time": "2020-11-30T13:31:47.870111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1=0.15\n",
    "w2=0.30\n",
    "w3=0.25\n",
    "w4 = 0.15\n",
    "w5 = 0.15\n",
    "submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "sub1 = pd.read_csv('sub_tabnet_new.csv')\n",
    "sub2 = pd.read_csv('sub_NN_transfer.csv')\n",
    "sub3 = pd.read_csv('sub_NN_3.csv')\n",
    "sub4 = pd.read_csv('sub_NN_4.csv')\n",
    "sub5 = pd.read_csv('sub_resnet.csv')\n",
    "submission.iloc[:, 1:] = 0\n",
    "submission.iloc[:, 1:] = sub1.iloc[:,1:]*w1 + sub2.iloc[:,1:]*w2 + sub3.iloc[:,1:]*w3 + sub4.iloc[:,1:]*w4+ sub5.iloc[:,1:]*w5\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T13:31:51.405250Z",
     "iopub.status.busy": "2020-11-30T13:31:51.404310Z",
     "iopub.status.idle": "2020-11-30T13:31:51.432454Z",
     "shell.execute_reply": "2020-11-30T13:31:51.431921Z"
    },
    "papermill": {
     "duration": 0.150715,
     "end_time": "2020-11-30T13:31:51.432556",
     "exception": false,
     "start_time": "2020-11-30T13:31:51.281841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.015453</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>0.001671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.015362</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.003348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.015948</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.010754</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.002898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.016138</td>\n",
       "      <td>0.021797</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.001664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001191                0.001497   \n",
       "1  id_001897cda                     0.000612                0.000964   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.001089                0.001099   \n",
       "4  id_0027f1083                     0.002168                0.001540   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.002283                        0.015453   \n",
       "1        0.001697                        0.002709   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.001794                        0.011657   \n",
       "4        0.002024                        0.016138   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.020859                        0.004941   \n",
       "1                           0.001616                        0.002177   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.015948                        0.004823   \n",
       "4                           0.021797                        0.004271   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002799                       0.005720   \n",
       "1                    0.004028                       0.015362   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.003362                       0.004810   \n",
       "4                    0.005013                       0.002191   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.000468  ...                               0.001143   \n",
       "1                    0.020358  ...                               0.000897   \n",
       "2                    0.000000  ...                               0.000000   \n",
       "3                    0.000558  ...                               0.000720   \n",
       "4                    0.000612  ...                               0.001046   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.001601         0.004132           0.001626   \n",
       "1      0.001309         0.003752           0.000600   \n",
       "2      0.000000         0.000000           0.000000   \n",
       "3      0.001398         0.002685           0.010754   \n",
       "4      0.000845         0.003681           0.002171   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.001145                               0.000911   \n",
       "1                   0.008784                               0.000639   \n",
       "2                   0.000000                               0.000000   \n",
       "3                   0.005675                               0.000779   \n",
       "4                   0.001423                               0.000892   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.001215   0.002236                    0.004346       0.001671  \n",
       "1         0.005746   0.001270                    0.003673       0.003348  \n",
       "2         0.000000   0.000000                    0.000000       0.000000  \n",
       "3         0.001750   0.002015                    0.001955       0.002898  \n",
       "4         0.001953   0.002105                    0.000639       0.001664  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1173.275016,
   "end_time": "2020-11-30T13:31:53.618106",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-30T13:12:20.343090",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
