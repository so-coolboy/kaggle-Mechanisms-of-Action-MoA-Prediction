{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:16.802260Z",
     "iopub.status.busy": "2020-11-26T02:27:16.801387Z",
     "iopub.status.idle": "2020-11-26T02:28:01.704807Z",
     "shell.execute_reply": "2020-11-26T02:28:01.703471Z"
    },
    "papermill": {
     "duration": 44.936216,
     "end_time": "2020-11-26T02:28:01.707757",
     "exception": false,
     "start_time": "2020-11-26T02:27:16.771541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: typing 3.7.4.3\r\n",
      "Uninstalling typing-3.7.4.3:\r\n",
      "  Successfully uninstalled typing-3.7.4.3\r\n",
      "Processing /kaggle/input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.4.1)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.5.1)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (4.45.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.18.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==2.0.1) (0.18.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==2.0.1) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==2.0.1) (0.14.1)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y typing\n",
    "!pip install ../input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:01.795527Z",
     "iopub.status.busy": "2020-11-26T02:28:01.794537Z",
     "iopub.status.idle": "2020-11-26T02:28:04.206338Z",
     "shell.execute_reply": "2020-11-26T02:28:04.205039Z"
    },
    "papermill": {
     "duration": 2.459506,
     "end_time": "2020-11-26T02:28:04.206478",
     "exception": false,
     "start_time": "2020-11-26T02:28:01.746972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:04.270878Z",
     "iopub.status.busy": "2020-11-26T02:28:04.270027Z",
     "iopub.status.idle": "2020-11-26T02:28:04.636822Z",
     "shell.execute_reply": "2020-11-26T02:28:04.636150Z"
    },
    "papermill": {
     "duration": 0.403444,
     "end_time": "2020-11-26T02:28:04.636965",
     "exception": false,
     "start_time": "2020-11-26T02:28:04.233521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:04.704954Z",
     "iopub.status.busy": "2020-11-26T02:28:04.703997Z",
     "iopub.status.idle": "2020-11-26T02:28:11.471756Z",
     "shell.execute_reply": "2020-11-26T02:28:11.470548Z"
    },
    "papermill": {
     "duration": 6.808426,
     "end_time": "2020-11-26T02:28:11.471899",
     "exception": false,
     "start_time": "2020-11-26T02:28:04.663473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/moa-make-foldsmoa-make-folds/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/moa-make-foldsmoa-make-folds/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:11.529545Z",
     "iopub.status.busy": "2020-11-26T02:28:11.527704Z",
     "iopub.status.idle": "2020-11-26T02:28:11.530307Z",
     "shell.execute_reply": "2020-11-26T02:28:11.530780Z"
    },
    "papermill": {
     "duration": 0.033431,
     "end_time": "2020-11-26T02:28:11.530925",
     "exception": false,
     "start_time": "2020-11-26T02:28:11.497494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#使用PCA制造出一部分特征\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:11.589457Z",
     "iopub.status.busy": "2020-11-26T02:28:11.588722Z",
     "iopub.status.idle": "2020-11-26T02:28:20.915163Z",
     "shell.execute_reply": "2020-11-26T02:28:20.914530Z"
    },
    "papermill": {
     "duration": 9.36064,
     "end_time": "2020-11-26T02:28:20.915326",
     "exception": false,
     "start_time": "2020-11-26T02:28:11.554686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in (GENES + CELLS):\n",
    "    transformer = QuantileTransformer(n_quantiles=115, random_state=0, output_distribution=\"normal\")   #50 75 100 125 150 25 \n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:20.972435Z",
     "iopub.status.busy": "2020-11-26T02:28:20.971160Z",
     "iopub.status.idle": "2020-11-26T02:28:21.319015Z",
     "shell.execute_reply": "2020-11-26T02:28:21.318441Z"
    },
    "papermill": {
     "duration": 0.379183,
     "end_time": "2020-11-26T02:28:21.319124",
     "exception": false,
     "start_time": "2020-11-26T02:28:20.939941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "target = train[train_targets_scored.columns[:207]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:21.458361Z",
     "iopub.status.busy": "2020-11-26T02:28:21.433328Z",
     "iopub.status.idle": "2020-11-26T02:28:21.461159Z",
     "shell.execute_reply": "2020-11-26T02:28:21.460678Z"
    },
    "papermill": {
     "duration": 0.116761,
     "end_time": "2020-11-26T02:28:21.461286",
     "exception": false,
     "start_time": "2020-11-26T02:28:21.344525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:21.516882Z",
     "iopub.status.busy": "2020-11-26T02:28:21.515604Z",
     "iopub.status.idle": "2020-11-26T02:28:21.615133Z",
     "shell.execute_reply": "2020-11-26T02:28:21.615816Z"
    },
    "papermill": {
     "duration": 0.13002,
     "end_time": "2020-11-26T02:28:21.615978",
     "exception": false,
     "start_time": "2020-11-26T02:28:21.485958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>...</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "      <th>fold_42</th>\n",
       "      <th>fold_0</th>\n",
       "      <th>fold_1</th>\n",
       "      <th>fold_2</th>\n",
       "      <th>fold_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.134770</td>\n",
       "      <td>0.908153</td>\n",
       "      <td>-0.416081</td>\n",
       "      <td>-0.967866</td>\n",
       "      <td>-0.254718</td>\n",
       "      <td>-1.015497</td>\n",
       "      <td>-1.364912</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.681885</td>\n",
       "      <td>0.272164</td>\n",
       "      <td>0.079996</td>\n",
       "      <td>1.204608</td>\n",
       "      <td>0.685581</td>\n",
       "      <td>0.314454</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.780234</td>\n",
       "      <td>0.944877</td>\n",
       "      <td>1.422334</td>\n",
       "      <td>-0.132034</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>1.492153</td>\n",
       "      <td>0.235410</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.735342</td>\n",
       "      <td>-0.274628</td>\n",
       "      <td>-0.438384</td>\n",
       "      <td>0.759605</td>\n",
       "      <td>2.389946</td>\n",
       "      <td>-0.859146</td>\n",
       "      <td>-2.287594</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.452845</td>\n",
       "      <td>-0.477300</td>\n",
       "      <td>0.971824</td>\n",
       "      <td>0.970578</td>\n",
       "      <td>1.463978</td>\n",
       "      <td>-0.871062</td>\n",
       "      <td>-0.376199</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1086 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  cp_time cp_dose       g-0       g-1       g-2       g-3  \\\n",
       "0  id_000644bb2       24      D1  1.134770  0.908153 -0.416081 -0.967866   \n",
       "1  id_000779bfc       72      D1  0.119600  0.681885  0.272164  0.079996   \n",
       "2  id_000a6266a       48      D1  0.780234  0.944877  1.422334 -0.132034   \n",
       "3  id_0015fd391       48      D1 -0.735342 -0.274628 -0.438384  0.759605   \n",
       "4  id_001626bd3       72      D2 -0.452845 -0.477300  0.971824  0.970578   \n",
       "\n",
       "        g-4       g-5       g-6  ...  ubiquitin_specific_protease_inhibitor  \\\n",
       "0 -0.254718 -1.015497 -1.364912  ...                                      0   \n",
       "1  1.204608  0.685581  0.314454  ...                                      0   \n",
       "2 -0.007450  1.492153  0.235410  ...                                      0   \n",
       "3  2.389946 -0.859146 -2.287594  ...                                      0   \n",
       "4  1.463978 -0.871062 -0.376199  ...                                      0   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \\\n",
       "0                0          0                           0              0   \n",
       "1                0          0                           0              0   \n",
       "2                0          0                           0              0   \n",
       "3                0          0                           0              0   \n",
       "4                0          0                           0              0   \n",
       "\n",
       "   fold_42  fold_0  fold_1  fold_2  fold_3  \n",
       "0        6       1       3       3       5  \n",
       "1        0       0       3       6       3  \n",
       "2        0       5       3       3       1  \n",
       "3        6       4       3       1       2  \n",
       "4        1       6       5       3       3  \n",
       "\n",
       "[5 rows x 1086 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:21.679521Z",
     "iopub.status.busy": "2020-11-26T02:28:21.678657Z",
     "iopub.status.idle": "2020-11-26T02:28:21.686177Z",
     "shell.execute_reply": "2020-11-26T02:28:21.686830Z"
    },
    "papermill": {
     "duration": 0.042278,
     "end_time": "2020-11-26T02:28:21.686962",
     "exception": false,
     "start_time": "2020-11-26T02:28:21.644684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1086)\n",
      "(21948, 1086)\n",
      "(3624, 875)\n",
      "(21948, 207)\n",
      "(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:21.752970Z",
     "iopub.status.busy": "2020-11-26T02:28:21.751904Z",
     "iopub.status.idle": "2020-11-26T02:28:21.770305Z",
     "shell.execute_reply": "2020-11-26T02:28:21.770913Z"
    },
    "papermill": {
     "duration": 0.055254,
     "end_time": "2020-11-26T02:28:21.771020",
     "exception": false,
     "start_time": "2020-11-26T02:28:21.715766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "def process_data(data):\n",
    "    \n",
    "    data['cp_time'] = lb.fit_transform(data['cp_time'])\n",
    "    data['cp_dose'] = lb.fit_transform(data['cp_dose'])\n",
    "    return data\n",
    "\n",
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['fold_42','sig_id','fold_0','fold_1','fold_2','fold_3']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:21.841189Z",
     "iopub.status.busy": "2020-11-26T02:28:21.836237Z",
     "iopub.status.idle": "2020-11-26T02:28:21.844361Z",
     "shell.execute_reply": "2020-11-26T02:28:21.843798Z"
    },
    "papermill": {
     "duration": 0.047199,
     "end_time": "2020-11-26T02:28:21.844476",
     "exception": false,
     "start_time": "2020-11-26T02:28:21.797277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evals(model, X, y, verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict(X)\n",
    "        y_preds = torch.clamp(y_preds, 0.0,1.0).detach().numpy()\n",
    "    score = log_loss_multi(y, y_preds)\n",
    "    #print(\"Logloss = \", score)\n",
    "    return y_preds, score\n",
    "\n",
    "\n",
    "def inference_fn(model, X ,verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict( X )\n",
    "        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n",
    "    return y_preds\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15): #eps=1e-15\n",
    "    p1 = actual * np.log(predicted+eps)\n",
    "    p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "    loss = p0 + p1\n",
    "\n",
    "    return -loss.mean()\n",
    "\n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()\n",
    "\n",
    "def check_targets(targets):\n",
    "    ### check if targets are all binary in training set\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        if len(np.unique(targets[:,i])) != 2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def auc_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        try:\n",
    "            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
    "        except:\n",
    "            pass\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:21.905825Z",
     "iopub.status.busy": "2020-11-26T02:28:21.905030Z",
     "iopub.status.idle": "2020-11-26T02:28:21.907840Z",
     "shell.execute_reply": "2020-11-26T02:28:21.907375Z"
    },
    "papermill": {
     "duration": 0.037417,
     "end_time": "2020-11-26T02:28:21.907932",
     "exception": false,
     "start_time": "2020-11-26T02:28:21.870515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = len(target_cols)\n",
    "        self.verbose=False\n",
    "        self.seed = 0\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.EPOCHS = 300\n",
    "        self.num_ensembling = 1\n",
    "        # Parameters model\n",
    "        self.cat_emb_dim=[1] * 2 #to choose\n",
    "        self.cats_idx = list(range(2))\n",
    "        self.cat_dims = [3,2]\n",
    "        self.num_numericals= len(feature_cols)-2\n",
    "        self.patience = 50\n",
    "        self.batch_size=1024\n",
    "        self.NFOLDS = 7\n",
    "    \n",
    "        # save\n",
    "        self.save_name = \"tabnet_raw_step1\"\n",
    "        \n",
    "        self.strategy = \"KFOLD\" # \n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:21.965492Z",
     "iopub.status.busy": "2020-11-26T02:28:21.964504Z",
     "iopub.status.idle": "2020-11-26T02:28:21.978033Z",
     "shell.execute_reply": "2020-11-26T02:28:21.977458Z"
    },
    "papermill": {
     "duration": 0.044103,
     "end_time": "2020-11-26T02:28:21.978123",
     "exception": false,
     "start_time": "2020-11-26T02:28:21.934020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:22.039442Z",
     "iopub.status.busy": "2020-11-26T02:28:22.038639Z",
     "iopub.status.idle": "2020-11-26T02:28:22.042183Z",
     "shell.execute_reply": "2020-11-26T02:28:22.041665Z"
    },
    "papermill": {
     "duration": 0.037248,
     "end_time": "2020-11-26T02:28:22.042299",
     "exception": false,
     "start_time": "2020-11-26T02:28:22.005051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "class LogitsLogLoss(Metric):\n",
    "    \"\"\"\n",
    "    LogLoss with sigmoid applied\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute LogLoss of predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.ndarray\n",
    "            Target matrix or vector\n",
    "        y_score: np.ndarray\n",
    "            Score matrix or vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float\n",
    "            LogLoss of predictions vs targets.\n",
    "        \"\"\"\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:22.117439Z",
     "iopub.status.busy": "2020-11-26T02:28:22.115724Z",
     "iopub.status.idle": "2020-11-26T02:28:22.118188Z",
     "shell.execute_reply": "2020-11-26T02:28:22.118711Z"
    },
    "papermill": {
     "duration": 0.049488,
     "end_time": "2020-11-26T02:28:22.118825",
     "exception": false,
     "start_time": "2020-11-26T02:28:22.069337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train[f'fold_{seed}'] != fold].index\n",
    "    val_idx = train[train[f'fold_{seed}'] == fold].index\n",
    "    \n",
    "    train_df = train[train[f'fold_{seed}'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train[f'fold_{seed}'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    X_train, y_train = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    X_val, y_val = valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "            \n",
    "    model = TabNetRegressor(n_d=24, n_a=64, n_steps=1, \n",
    "                            n_independent=1, n_shared=1,\n",
    "                            gamma=1.2, lambda_sparse=0, \n",
    "                            cat_dims=cfg.cat_dims, \n",
    "                            cat_emb_dim=cfg.cat_emb_dim, \n",
    "                            cat_idxs=cfg.cats_idx, \n",
    "                            optimizer_fn=torch.optim.Adam,\n",
    "                            optimizer_params=dict(lr=2e-2, weight_decay=1e-5), \n",
    "                            mask_type='entmax', \n",
    "                            device_name=cfg.device, \n",
    "                            scheduler_params=dict(mode='min', factor=0.1, patience=10), \n",
    "                            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "    \n",
    "    model.fit(X_train=X_train, y_train=y_train, \n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_name = [\"val\"],\n",
    "              eval_metric = [\"logits_ll\"],\n",
    "              max_epochs=cfg.EPOCHS, \n",
    "              patience=cfg.patience, \n",
    "              batch_size=cfg.batch_size, \n",
    "              virtual_batch_size=128,\n",
    "              num_workers=0, \n",
    "              drop_last=False, \n",
    "              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits) \n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    valid_preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "    score = log_loss_multi(y_val, preds)\n",
    "    name = cfg.save_name + f\"_fold{fold}_{seed}\"\n",
    "    model.save_model(name)\n",
    "    \n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    oof[val_idx] = valid_preds\n",
    "    \n",
    "    x_test = torch.as_tensor(test[feature_cols].values)\n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:22.180076Z",
     "iopub.status.busy": "2020-11-26T02:28:22.178178Z",
     "iopub.status.idle": "2020-11-26T02:28:22.180810Z",
     "shell.execute_reply": "2020-11-26T02:28:22.181338Z"
    },
    "papermill": {
     "duration": 0.036473,
     "end_time": "2020-11-26T02:28:22.181458",
     "exception": false,
     "start_time": "2020-11-26T02:28:22.144985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:28:22.241976Z",
     "iopub.status.busy": "2020-11-26T02:28:22.240979Z",
     "iopub.status.idle": "2020-11-26T04:03:29.167318Z",
     "shell.execute_reply": "2020-11-26T04:03:29.167977Z"
    },
    "papermill": {
     "duration": 5706.96089,
     "end_time": "2020-11-26T04:03:29.168439",
     "exception": false,
     "start_time": "2020-11-26T02:28:22.207549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 0.39931 | val_logits_ll: 0.10102 |  0:00:03s\n",
      "epoch 1  | loss: 0.03284 | val_logits_ll: 0.02737 |  0:00:04s\n",
      "epoch 2  | loss: 0.02398 | val_logits_ll: 0.02206 |  0:00:06s\n",
      "epoch 3  | loss: 0.02161 | val_logits_ll: 0.02114 |  0:00:07s\n",
      "epoch 4  | loss: 0.02086 | val_logits_ll: 0.02078 |  0:00:09s\n",
      "epoch 5  | loss: 0.02058 | val_logits_ll: 0.02052 |  0:00:11s\n",
      "epoch 6  | loss: 0.02027 | val_logits_ll: 0.02036 |  0:00:12s\n",
      "epoch 7  | loss: 0.02003 | val_logits_ll: 0.02021 |  0:00:14s\n",
      "epoch 8  | loss: 0.0197  | val_logits_ll: 0.01983 |  0:00:16s\n",
      "epoch 9  | loss: 0.0192  | val_logits_ll: 0.01977 |  0:00:17s\n",
      "epoch 10 | loss: 0.01881 | val_logits_ll: 0.01915 |  0:00:19s\n",
      "epoch 11 | loss: 0.01855 | val_logits_ll: 0.0191  |  0:00:20s\n",
      "epoch 12 | loss: 0.01825 | val_logits_ll: 0.01916 |  0:00:22s\n",
      "epoch 13 | loss: 0.01804 | val_logits_ll: 0.0188  |  0:00:24s\n",
      "epoch 14 | loss: 0.01777 | val_logits_ll: 0.01894 |  0:00:25s\n",
      "epoch 15 | loss: 0.01768 | val_logits_ll: 0.01862 |  0:00:27s\n",
      "epoch 16 | loss: 0.01754 | val_logits_ll: 0.01918 |  0:00:29s\n",
      "epoch 17 | loss: 0.01735 | val_logits_ll: 0.02011 |  0:00:31s\n",
      "epoch 18 | loss: 0.01729 | val_logits_ll: 0.02087 |  0:00:32s\n",
      "epoch 19 | loss: 0.01713 | val_logits_ll: 0.01842 |  0:00:34s\n",
      "epoch 20 | loss: 0.017   | val_logits_ll: 0.01842 |  0:00:36s\n",
      "epoch 21 | loss: 0.01698 | val_logits_ll: 0.02067 |  0:00:38s\n",
      "epoch 22 | loss: 0.01676 | val_logits_ll: 0.01919 |  0:00:39s\n",
      "epoch 23 | loss: 0.01674 | val_logits_ll: 0.01826 |  0:00:41s\n",
      "epoch 24 | loss: 0.01653 | val_logits_ll: 0.01936 |  0:00:43s\n",
      "epoch 25 | loss: 0.01647 | val_logits_ll: 0.01813 |  0:00:44s\n",
      "epoch 26 | loss: 0.01637 | val_logits_ll: 0.01814 |  0:00:46s\n",
      "epoch 27 | loss: 0.01632 | val_logits_ll: 0.01804 |  0:00:48s\n",
      "epoch 28 | loss: 0.01628 | val_logits_ll: 0.01818 |  0:00:49s\n",
      "epoch 29 | loss: 0.01627 | val_logits_ll: 0.01836 |  0:00:51s\n",
      "epoch 30 | loss: 0.01633 | val_logits_ll: 0.01815 |  0:00:52s\n",
      "epoch 31 | loss: 0.01613 | val_logits_ll: 0.01834 |  0:00:54s\n",
      "epoch 32 | loss: 0.01598 | val_logits_ll: 0.01828 |  0:00:56s\n",
      "epoch 33 | loss: 0.01589 | val_logits_ll: 0.01795 |  0:00:57s\n",
      "epoch 34 | loss: 0.01594 | val_logits_ll: 0.01794 |  0:00:59s\n",
      "epoch 35 | loss: 0.01587 | val_logits_ll: 0.01826 |  0:01:01s\n",
      "epoch 36 | loss: 0.0158  | val_logits_ll: 0.01785 |  0:01:02s\n",
      "epoch 37 | loss: 0.0158  | val_logits_ll: 0.01788 |  0:01:04s\n",
      "epoch 38 | loss: 0.01572 | val_logits_ll: 0.01788 |  0:01:05s\n",
      "epoch 39 | loss: 0.01575 | val_logits_ll: 0.01777 |  0:01:07s\n",
      "epoch 40 | loss: 0.01561 | val_logits_ll: 0.01785 |  0:01:09s\n",
      "epoch 41 | loss: 0.0155  | val_logits_ll: 0.018   |  0:01:11s\n",
      "epoch 42 | loss: 0.01563 | val_logits_ll: 0.01812 |  0:01:12s\n",
      "epoch 43 | loss: 0.01567 | val_logits_ll: 0.01781 |  0:01:14s\n",
      "epoch 44 | loss: 0.01547 | val_logits_ll: 0.01805 |  0:01:15s\n",
      "epoch 45 | loss: 0.01541 | val_logits_ll: 0.01796 |  0:01:17s\n",
      "epoch 46 | loss: 0.0154  | val_logits_ll: 0.01778 |  0:01:19s\n",
      "epoch 47 | loss: 0.01542 | val_logits_ll: 0.01791 |  0:01:20s\n",
      "epoch 48 | loss: 0.01548 | val_logits_ll: 0.01791 |  0:01:22s\n",
      "epoch 49 | loss: 0.01535 | val_logits_ll: 0.018   |  0:01:24s\n",
      "epoch 50 | loss: 0.01535 | val_logits_ll: 0.01812 |  0:01:25s\n",
      "epoch 51 | loss: 0.01474 | val_logits_ll: 0.01756 |  0:01:27s\n",
      "epoch 52 | loss: 0.01424 | val_logits_ll: 0.01765 |  0:01:28s\n",
      "epoch 53 | loss: 0.01399 | val_logits_ll: 0.01777 |  0:01:31s\n",
      "epoch 54 | loss: 0.0138  | val_logits_ll: 0.01785 |  0:01:32s\n",
      "epoch 55 | loss: 0.01363 | val_logits_ll: 0.0179  |  0:01:34s\n",
      "epoch 56 | loss: 0.01347 | val_logits_ll: 0.0181  |  0:01:35s\n",
      "epoch 57 | loss: 0.01332 | val_logits_ll: 0.01817 |  0:01:37s\n",
      "epoch 58 | loss: 0.01319 | val_logits_ll: 0.01823 |  0:01:39s\n",
      "epoch 59 | loss: 0.01306 | val_logits_ll: 0.0184  |  0:01:41s\n",
      "epoch 60 | loss: 0.01295 | val_logits_ll: 0.0185  |  0:01:43s\n",
      "epoch 61 | loss: 0.01284 | val_logits_ll: 0.0186  |  0:01:44s\n",
      "epoch 62 | loss: 0.01275 | val_logits_ll: 0.01884 |  0:01:46s\n",
      "epoch 63 | loss: 0.01247 | val_logits_ll: 0.0188  |  0:01:47s\n",
      "epoch 64 | loss: 0.01232 | val_logits_ll: 0.01883 |  0:01:49s\n",
      "epoch 65 | loss: 0.01227 | val_logits_ll: 0.01889 |  0:01:51s\n",
      "epoch 66 | loss: 0.01222 | val_logits_ll: 0.0189  |  0:01:52s\n",
      "epoch 67 | loss: 0.01221 | val_logits_ll: 0.01894 |  0:01:54s\n",
      "epoch 68 | loss: 0.01218 | val_logits_ll: 0.01893 |  0:01:56s\n",
      "epoch 69 | loss: 0.01217 | val_logits_ll: 0.01898 |  0:01:57s\n",
      "epoch 70 | loss: 0.01211 | val_logits_ll: 0.01897 |  0:01:59s\n",
      "epoch 71 | loss: 0.01211 | val_logits_ll: 0.01903 |  0:02:00s\n",
      "epoch 72 | loss: 0.01207 | val_logits_ll: 0.01903 |  0:02:02s\n",
      "epoch 73 | loss: 0.01204 | val_logits_ll: 0.01904 |  0:02:04s\n",
      "epoch 74 | loss: 0.01201 | val_logits_ll: 0.01906 |  0:02:05s\n",
      "epoch 75 | loss: 0.012   | val_logits_ll: 0.01908 |  0:02:07s\n",
      "epoch 76 | loss: 0.012   | val_logits_ll: 0.01909 |  0:02:09s\n",
      "epoch 77 | loss: 0.01199 | val_logits_ll: 0.0191  |  0:02:10s\n",
      "epoch 78 | loss: 0.012   | val_logits_ll: 0.0191  |  0:02:12s\n",
      "epoch 79 | loss: 0.01198 | val_logits_ll: 0.01909 |  0:02:14s\n",
      "epoch 80 | loss: 0.01201 | val_logits_ll: 0.0191  |  0:02:16s\n",
      "epoch 81 | loss: 0.012   | val_logits_ll: 0.01911 |  0:02:17s\n",
      "epoch 82 | loss: 0.01198 | val_logits_ll: 0.01911 |  0:02:19s\n",
      "epoch 83 | loss: 0.01198 | val_logits_ll: 0.01909 |  0:02:20s\n",
      "epoch 84 | loss: 0.01196 | val_logits_ll: 0.01912 |  0:02:22s\n",
      "epoch 85 | loss: 0.01196 | val_logits_ll: 0.0191  |  0:02:24s\n",
      "epoch 86 | loss: 0.01198 | val_logits_ll: 0.01911 |  0:02:25s\n",
      "epoch 87 | loss: 0.01198 | val_logits_ll: 0.01911 |  0:02:27s\n",
      "epoch 88 | loss: 0.01197 | val_logits_ll: 0.0191  |  0:02:29s\n",
      "epoch 89 | loss: 0.01197 | val_logits_ll: 0.01912 |  0:02:31s\n",
      "epoch 90 | loss: 0.01198 | val_logits_ll: 0.01911 |  0:02:32s\n",
      "epoch 91 | loss: 0.01197 | val_logits_ll: 0.01912 |  0:02:34s\n",
      "epoch 92 | loss: 0.01196 | val_logits_ll: 0.01913 |  0:02:36s\n",
      "epoch 93 | loss: 0.01198 | val_logits_ll: 0.01911 |  0:02:38s\n",
      "epoch 94 | loss: 0.01197 | val_logits_ll: 0.01912 |  0:02:39s\n",
      "epoch 95 | loss: 0.01196 | val_logits_ll: 0.01912 |  0:02:41s\n",
      "epoch 96 | loss: 0.01197 | val_logits_ll: 0.01912 |  0:02:42s\n",
      "epoch 97 | loss: 0.01196 | val_logits_ll: 0.01911 |  0:02:44s\n",
      "epoch 98 | loss: 0.01196 | val_logits_ll: 0.0191  |  0:02:46s\n",
      "epoch 99 | loss: 0.01196 | val_logits_ll: 0.01912 |  0:02:48s\n",
      "epoch 100| loss: 0.01197 | val_logits_ll: 0.01912 |  0:02:49s\n",
      "epoch 101| loss: 0.01197 | val_logits_ll: 0.01911 |  0:02:51s\n",
      "\n",
      "Early stopping occured at epoch 101 with best_epoch = 51 and best_val_logits_ll = 0.01756\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold0_42.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.39982 | val_logits_ll: 0.09788 |  0:00:01s\n",
      "epoch 1  | loss: 0.03296 | val_logits_ll: 0.02567 |  0:00:03s\n",
      "epoch 2  | loss: 0.02401 | val_logits_ll: 0.02145 |  0:00:05s\n",
      "epoch 3  | loss: 0.02165 | val_logits_ll: 0.02073 |  0:00:06s\n",
      "epoch 4  | loss: 0.02092 | val_logits_ll: 0.02015 |  0:00:08s\n",
      "epoch 5  | loss: 0.02045 | val_logits_ll: 0.0199  |  0:00:09s\n",
      "epoch 6  | loss: 0.02014 | val_logits_ll: 0.01961 |  0:00:11s\n",
      "epoch 7  | loss: 0.01974 | val_logits_ll: 0.01929 |  0:00:13s\n",
      "epoch 8  | loss: 0.01932 | val_logits_ll: 0.01904 |  0:00:14s\n",
      "epoch 9  | loss: 0.01894 | val_logits_ll: 0.01875 |  0:00:16s\n",
      "epoch 10 | loss: 0.01858 | val_logits_ll: 0.01847 |  0:00:17s\n",
      "epoch 11 | loss: 0.01836 | val_logits_ll: 0.0183  |  0:00:19s\n",
      "epoch 12 | loss: 0.01811 | val_logits_ll: 0.01826 |  0:00:21s\n",
      "epoch 13 | loss: 0.01786 | val_logits_ll: 0.01818 |  0:00:22s\n",
      "epoch 14 | loss: 0.01762 | val_logits_ll: 0.01818 |  0:00:24s\n",
      "epoch 15 | loss: 0.01761 | val_logits_ll: 0.01817 |  0:00:26s\n",
      "epoch 16 | loss: 0.01751 | val_logits_ll: 0.01811 |  0:00:28s\n",
      "epoch 17 | loss: 0.01725 | val_logits_ll: 0.01801 |  0:00:29s\n",
      "epoch 18 | loss: 0.01719 | val_logits_ll: 0.01797 |  0:00:31s\n",
      "epoch 19 | loss: 0.01706 | val_logits_ll: 0.01791 |  0:00:33s\n",
      "epoch 20 | loss: 0.01703 | val_logits_ll: 0.01794 |  0:00:34s\n",
      "epoch 21 | loss: 0.01694 | val_logits_ll: 0.01789 |  0:00:36s\n",
      "epoch 22 | loss: 0.01675 | val_logits_ll: 0.01784 |  0:00:38s\n",
      "epoch 23 | loss: 0.01677 | val_logits_ll: 0.01783 |  0:00:40s\n",
      "epoch 24 | loss: 0.01662 | val_logits_ll: 0.01783 |  0:00:42s\n",
      "epoch 25 | loss: 0.01658 | val_logits_ll: 0.01977 |  0:00:43s\n",
      "epoch 26 | loss: 0.01661 | val_logits_ll: 0.01781 |  0:00:45s\n",
      "epoch 27 | loss: 0.01644 | val_logits_ll: 0.01776 |  0:00:47s\n",
      "epoch 28 | loss: 0.01646 | val_logits_ll: 0.01776 |  0:00:49s\n",
      "epoch 29 | loss: 0.01625 | val_logits_ll: 0.01932 |  0:00:50s\n",
      "epoch 30 | loss: 0.01625 | val_logits_ll: 0.01832 |  0:00:52s\n",
      "epoch 31 | loss: 0.01631 | val_logits_ll: 0.01763 |  0:00:53s\n",
      "epoch 32 | loss: 0.01618 | val_logits_ll: 0.01893 |  0:00:55s\n",
      "epoch 33 | loss: 0.016   | val_logits_ll: 0.01803 |  0:00:57s\n",
      "epoch 34 | loss: 0.01601 | val_logits_ll: 0.01765 |  0:00:59s\n",
      "epoch 35 | loss: 0.01607 | val_logits_ll: 0.01771 |  0:01:00s\n",
      "epoch 36 | loss: 0.01604 | val_logits_ll: 0.01773 |  0:01:02s\n",
      "epoch 37 | loss: 0.01584 | val_logits_ll: 0.01768 |  0:01:04s\n",
      "epoch 38 | loss: 0.0159  | val_logits_ll: 0.0176  |  0:01:05s\n",
      "epoch 39 | loss: 0.01587 | val_logits_ll: 0.01771 |  0:01:07s\n",
      "epoch 40 | loss: 0.01578 | val_logits_ll: 0.01786 |  0:01:08s\n",
      "epoch 41 | loss: 0.01575 | val_logits_ll: 0.0177  |  0:01:10s\n",
      "epoch 42 | loss: 0.0157  | val_logits_ll: 0.01834 |  0:01:12s\n",
      "epoch 43 | loss: 0.01569 | val_logits_ll: 0.01762 |  0:01:13s\n",
      "epoch 44 | loss: 0.01555 | val_logits_ll: 0.01802 |  0:01:15s\n",
      "epoch 45 | loss: 0.01553 | val_logits_ll: 0.01773 |  0:01:17s\n",
      "epoch 46 | loss: 0.01548 | val_logits_ll: 0.01763 |  0:01:18s\n",
      "epoch 47 | loss: 0.01559 | val_logits_ll: 0.01766 |  0:01:20s\n",
      "epoch 48 | loss: 0.01544 | val_logits_ll: 0.01816 |  0:01:22s\n",
      "epoch 49 | loss: 0.01548 | val_logits_ll: 0.01752 |  0:01:23s\n",
      "epoch 50 | loss: 0.01539 | val_logits_ll: 0.01754 |  0:01:25s\n",
      "epoch 51 | loss: 0.01532 | val_logits_ll: 0.01772 |  0:01:26s\n",
      "epoch 52 | loss: 0.01536 | val_logits_ll: 0.01761 |  0:01:28s\n",
      "epoch 53 | loss: 0.01533 | val_logits_ll: 0.01792 |  0:01:30s\n",
      "epoch 54 | loss: 0.01527 | val_logits_ll: 0.01771 |  0:01:31s\n",
      "epoch 55 | loss: 0.01536 | val_logits_ll: 0.01761 |  0:01:33s\n",
      "epoch 56 | loss: 0.01524 | val_logits_ll: 0.0184  |  0:01:35s\n",
      "epoch 57 | loss: 0.01517 | val_logits_ll: 0.01773 |  0:01:37s\n",
      "epoch 58 | loss: 0.01515 | val_logits_ll: 0.01774 |  0:01:39s\n",
      "epoch 59 | loss: 0.01519 | val_logits_ll: 0.01777 |  0:01:40s\n",
      "epoch 60 | loss: 0.01529 | val_logits_ll: 0.01764 |  0:01:42s\n",
      "epoch 61 | loss: 0.01465 | val_logits_ll: 0.01738 |  0:01:44s\n",
      "epoch 62 | loss: 0.01415 | val_logits_ll: 0.01737 |  0:01:46s\n",
      "epoch 63 | loss: 0.01387 | val_logits_ll: 0.01741 |  0:01:47s\n",
      "epoch 64 | loss: 0.01367 | val_logits_ll: 0.01747 |  0:01:49s\n",
      "epoch 65 | loss: 0.01351 | val_logits_ll: 0.01761 |  0:01:50s\n",
      "epoch 66 | loss: 0.01334 | val_logits_ll: 0.01768 |  0:01:52s\n",
      "epoch 67 | loss: 0.01322 | val_logits_ll: 0.0178  |  0:01:54s\n",
      "epoch 68 | loss: 0.01308 | val_logits_ll: 0.01784 |  0:01:56s\n",
      "epoch 69 | loss: 0.01295 | val_logits_ll: 0.01798 |  0:01:57s\n",
      "epoch 70 | loss: 0.01284 | val_logits_ll: 0.01805 |  0:01:59s\n",
      "epoch 71 | loss: 0.01269 | val_logits_ll: 0.01827 |  0:02:00s\n",
      "epoch 72 | loss: 0.01259 | val_logits_ll: 0.01841 |  0:02:02s\n",
      "epoch 73 | loss: 0.01247 | val_logits_ll: 0.0185  |  0:02:04s\n",
      "epoch 74 | loss: 0.01219 | val_logits_ll: 0.01852 |  0:02:06s\n",
      "epoch 75 | loss: 0.01206 | val_logits_ll: 0.01856 |  0:02:07s\n",
      "epoch 76 | loss: 0.01201 | val_logits_ll: 0.01859 |  0:02:09s\n",
      "epoch 77 | loss: 0.01195 | val_logits_ll: 0.01861 |  0:02:10s\n",
      "epoch 78 | loss: 0.01192 | val_logits_ll: 0.01863 |  0:02:12s\n",
      "epoch 79 | loss: 0.0119  | val_logits_ll: 0.01867 |  0:02:14s\n",
      "epoch 80 | loss: 0.01186 | val_logits_ll: 0.0187  |  0:02:15s\n",
      "epoch 81 | loss: 0.01182 | val_logits_ll: 0.01874 |  0:02:17s\n",
      "epoch 82 | loss: 0.01178 | val_logits_ll: 0.01878 |  0:02:19s\n",
      "epoch 83 | loss: 0.01176 | val_logits_ll: 0.01878 |  0:02:20s\n",
      "epoch 84 | loss: 0.01173 | val_logits_ll: 0.01884 |  0:02:22s\n",
      "epoch 85 | loss: 0.01172 | val_logits_ll: 0.01884 |  0:02:23s\n",
      "epoch 86 | loss: 0.01169 | val_logits_ll: 0.01882 |  0:02:25s\n",
      "epoch 87 | loss: 0.0117  | val_logits_ll: 0.01881 |  0:02:27s\n",
      "epoch 88 | loss: 0.01169 | val_logits_ll: 0.01883 |  0:02:28s\n",
      "epoch 89 | loss: 0.01168 | val_logits_ll: 0.01886 |  0:02:30s\n",
      "epoch 90 | loss: 0.01168 | val_logits_ll: 0.01885 |  0:02:32s\n",
      "epoch 91 | loss: 0.01169 | val_logits_ll: 0.01886 |  0:02:33s\n",
      "epoch 92 | loss: 0.01169 | val_logits_ll: 0.01885 |  0:02:35s\n",
      "epoch 93 | loss: 0.01167 | val_logits_ll: 0.01886 |  0:02:37s\n",
      "epoch 94 | loss: 0.01167 | val_logits_ll: 0.01888 |  0:02:39s\n",
      "epoch 95 | loss: 0.01165 | val_logits_ll: 0.01888 |  0:02:41s\n",
      "epoch 96 | loss: 0.01166 | val_logits_ll: 0.01887 |  0:02:42s\n",
      "epoch 97 | loss: 0.01167 | val_logits_ll: 0.01887 |  0:02:44s\n",
      "epoch 98 | loss: 0.01165 | val_logits_ll: 0.01887 |  0:02:46s\n",
      "epoch 99 | loss: 0.01166 | val_logits_ll: 0.01888 |  0:02:47s\n",
      "epoch 100| loss: 0.01165 | val_logits_ll: 0.01886 |  0:02:49s\n",
      "epoch 101| loss: 0.01167 | val_logits_ll: 0.01888 |  0:02:51s\n",
      "epoch 102| loss: 0.01165 | val_logits_ll: 0.01887 |  0:02:52s\n",
      "epoch 103| loss: 0.01165 | val_logits_ll: 0.01887 |  0:02:54s\n",
      "epoch 104| loss: 0.01165 | val_logits_ll: 0.01886 |  0:02:56s\n",
      "epoch 105| loss: 0.01165 | val_logits_ll: 0.01886 |  0:02:57s\n",
      "epoch 106| loss: 0.01166 | val_logits_ll: 0.01885 |  0:02:59s\n",
      "epoch 107| loss: 0.01168 | val_logits_ll: 0.01886 |  0:03:01s\n",
      "epoch 108| loss: 0.01166 | val_logits_ll: 0.01887 |  0:03:02s\n",
      "epoch 109| loss: 0.01166 | val_logits_ll: 0.01884 |  0:03:04s\n",
      "epoch 110| loss: 0.01165 | val_logits_ll: 0.01886 |  0:03:05s\n",
      "epoch 111| loss: 0.01166 | val_logits_ll: 0.01885 |  0:03:07s\n",
      "epoch 112| loss: 0.01165 | val_logits_ll: 0.01887 |  0:03:09s\n",
      "\n",
      "Early stopping occured at epoch 112 with best_epoch = 62 and best_val_logits_ll = 0.01737\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold1_42.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40278 | val_logits_ll: 0.10294 |  0:00:01s\n",
      "epoch 1  | loss: 0.03285 | val_logits_ll: 0.02815 |  0:00:03s\n",
      "epoch 2  | loss: 0.02457 | val_logits_ll: 0.02322 |  0:00:04s\n",
      "epoch 3  | loss: 0.02159 | val_logits_ll: 0.02168 |  0:00:06s\n",
      "epoch 4  | loss: 0.02076 | val_logits_ll: 0.02118 |  0:00:08s\n",
      "epoch 5  | loss: 0.02034 | val_logits_ll: 0.02084 |  0:00:09s\n",
      "epoch 6  | loss: 0.02    | val_logits_ll: 0.02055 |  0:00:11s\n",
      "epoch 7  | loss: 0.01955 | val_logits_ll: 0.01997 |  0:00:13s\n",
      "epoch 8  | loss: 0.01901 | val_logits_ll: 0.01996 |  0:00:14s\n",
      "epoch 9  | loss: 0.01869 | val_logits_ll: 0.01957 |  0:00:16s\n",
      "epoch 10 | loss: 0.01843 | val_logits_ll: 0.01939 |  0:00:17s\n",
      "epoch 11 | loss: 0.01812 | val_logits_ll: 0.01947 |  0:00:19s\n",
      "epoch 12 | loss: 0.01791 | val_logits_ll: 0.01906 |  0:00:21s\n",
      "epoch 13 | loss: 0.01764 | val_logits_ll: 0.01898 |  0:00:22s\n",
      "epoch 14 | loss: 0.01744 | val_logits_ll: 0.01892 |  0:00:24s\n",
      "epoch 15 | loss: 0.01726 | val_logits_ll: 0.01881 |  0:00:26s\n",
      "epoch 16 | loss: 0.01713 | val_logits_ll: 0.01874 |  0:00:28s\n",
      "epoch 17 | loss: 0.01702 | val_logits_ll: 0.01875 |  0:00:30s\n",
      "epoch 18 | loss: 0.01681 | val_logits_ll: 0.0189  |  0:00:32s\n",
      "epoch 19 | loss: 0.01672 | val_logits_ll: 0.01847 |  0:00:33s\n",
      "epoch 20 | loss: 0.0167  | val_logits_ll: 0.01872 |  0:00:35s\n",
      "epoch 21 | loss: 0.01654 | val_logits_ll: 0.02185 |  0:00:37s\n",
      "epoch 22 | loss: 0.01651 | val_logits_ll: 0.01867 |  0:00:38s\n",
      "epoch 23 | loss: 0.01651 | val_logits_ll: 0.0187  |  0:00:40s\n",
      "epoch 24 | loss: 0.01631 | val_logits_ll: 0.01839 |  0:00:41s\n",
      "epoch 25 | loss: 0.01615 | val_logits_ll: 0.01858 |  0:00:43s\n",
      "epoch 26 | loss: 0.01608 | val_logits_ll: 0.01839 |  0:00:45s\n",
      "epoch 27 | loss: 0.01601 | val_logits_ll: 0.01874 |  0:00:46s\n",
      "epoch 28 | loss: 0.01595 | val_logits_ll: 0.01894 |  0:00:48s\n",
      "epoch 29 | loss: 0.01589 | val_logits_ll: 0.01862 |  0:00:49s\n",
      "epoch 30 | loss: 0.0158  | val_logits_ll: 0.01903 |  0:00:51s\n",
      "epoch 31 | loss: 0.01574 | val_logits_ll: 0.01848 |  0:00:53s\n",
      "epoch 32 | loss: 0.01581 | val_logits_ll: 0.01837 |  0:00:54s\n",
      "epoch 33 | loss: 0.01563 | val_logits_ll: 0.01827 |  0:00:56s\n",
      "epoch 34 | loss: 0.01556 | val_logits_ll: 0.01831 |  0:00:58s\n",
      "epoch 35 | loss: 0.01553 | val_logits_ll: 0.01839 |  0:00:59s\n",
      "epoch 36 | loss: 0.01541 | val_logits_ll: 0.01836 |  0:01:01s\n",
      "epoch 37 | loss: 0.01543 | val_logits_ll: 0.01835 |  0:01:03s\n",
      "epoch 38 | loss: 0.0154  | val_logits_ll: 0.01913 |  0:01:04s\n",
      "epoch 39 | loss: 0.01524 | val_logits_ll: 0.01848 |  0:01:06s\n",
      "epoch 40 | loss: 0.01518 | val_logits_ll: 0.01857 |  0:01:08s\n",
      "epoch 41 | loss: 0.0152  | val_logits_ll: 0.01854 |  0:01:10s\n",
      "epoch 42 | loss: 0.0152  | val_logits_ll: 0.01831 |  0:01:11s\n",
      "epoch 43 | loss: 0.01507 | val_logits_ll: 0.0189  |  0:01:13s\n",
      "epoch 44 | loss: 0.01517 | val_logits_ll: 0.01858 |  0:01:14s\n",
      "epoch 45 | loss: 0.01447 | val_logits_ll: 0.01824 |  0:01:16s\n",
      "epoch 46 | loss: 0.01396 | val_logits_ll: 0.01833 |  0:01:18s\n",
      "epoch 47 | loss: 0.01371 | val_logits_ll: 0.0184  |  0:01:20s\n",
      "epoch 48 | loss: 0.01352 | val_logits_ll: 0.01846 |  0:01:21s\n",
      "epoch 49 | loss: 0.01335 | val_logits_ll: 0.01852 |  0:01:23s\n",
      "epoch 50 | loss: 0.01321 | val_logits_ll: 0.01868 |  0:01:24s\n",
      "epoch 51 | loss: 0.01307 | val_logits_ll: 0.01878 |  0:01:26s\n",
      "epoch 52 | loss: 0.01296 | val_logits_ll: 0.01882 |  0:01:28s\n",
      "epoch 53 | loss: 0.01279 | val_logits_ll: 0.01892 |  0:01:30s\n",
      "epoch 54 | loss: 0.01267 | val_logits_ll: 0.01933 |  0:01:32s\n",
      "epoch 55 | loss: 0.01257 | val_logits_ll: 0.01913 |  0:01:34s\n",
      "epoch 56 | loss: 0.01244 | val_logits_ll: 0.01927 |  0:01:35s\n",
      "epoch 57 | loss: 0.01214 | val_logits_ll: 0.01931 |  0:01:37s\n",
      "epoch 58 | loss: 0.01202 | val_logits_ll: 0.01933 |  0:01:39s\n",
      "epoch 59 | loss: 0.01197 | val_logits_ll: 0.01939 |  0:01:40s\n",
      "epoch 60 | loss: 0.01193 | val_logits_ll: 0.01941 |  0:01:42s\n",
      "epoch 61 | loss: 0.01189 | val_logits_ll: 0.01945 |  0:01:44s\n",
      "epoch 62 | loss: 0.01186 | val_logits_ll: 0.01945 |  0:01:45s\n",
      "epoch 63 | loss: 0.01184 | val_logits_ll: 0.01947 |  0:01:47s\n",
      "epoch 64 | loss: 0.01181 | val_logits_ll: 0.01953 |  0:01:48s\n",
      "epoch 65 | loss: 0.01179 | val_logits_ll: 0.01951 |  0:01:50s\n",
      "epoch 66 | loss: 0.01175 | val_logits_ll: 0.01957 |  0:01:52s\n",
      "epoch 67 | loss: 0.01172 | val_logits_ll: 0.01957 |  0:01:53s\n",
      "epoch 68 | loss: 0.01169 | val_logits_ll: 0.01958 |  0:01:55s\n",
      "epoch 69 | loss: 0.01169 | val_logits_ll: 0.01958 |  0:01:57s\n",
      "epoch 70 | loss: 0.01167 | val_logits_ll: 0.0196  |  0:01:58s\n",
      "epoch 71 | loss: 0.0117  | val_logits_ll: 0.0196  |  0:02:00s\n",
      "epoch 72 | loss: 0.01168 | val_logits_ll: 0.0196  |  0:02:02s\n",
      "epoch 73 | loss: 0.01168 | val_logits_ll: 0.01961 |  0:02:03s\n",
      "epoch 74 | loss: 0.01166 | val_logits_ll: 0.0196  |  0:02:05s\n",
      "epoch 75 | loss: 0.01167 | val_logits_ll: 0.01961 |  0:02:07s\n",
      "epoch 76 | loss: 0.01165 | val_logits_ll: 0.0196  |  0:02:08s\n",
      "epoch 77 | loss: 0.01167 | val_logits_ll: 0.01962 |  0:02:10s\n",
      "epoch 78 | loss: 0.01165 | val_logits_ll: 0.01963 |  0:02:12s\n",
      "epoch 79 | loss: 0.01164 | val_logits_ll: 0.01964 |  0:02:14s\n",
      "epoch 80 | loss: 0.01165 | val_logits_ll: 0.01964 |  0:02:15s\n",
      "epoch 81 | loss: 0.01165 | val_logits_ll: 0.01963 |  0:02:17s\n",
      "epoch 82 | loss: 0.01165 | val_logits_ll: 0.01964 |  0:02:18s\n",
      "epoch 83 | loss: 0.01165 | val_logits_ll: 0.01963 |  0:02:20s\n",
      "epoch 84 | loss: 0.01167 | val_logits_ll: 0.01961 |  0:02:22s\n",
      "epoch 85 | loss: 0.01163 | val_logits_ll: 0.01962 |  0:02:23s\n",
      "epoch 86 | loss: 0.01164 | val_logits_ll: 0.01964 |  0:02:25s\n",
      "epoch 87 | loss: 0.01164 | val_logits_ll: 0.01962 |  0:02:27s\n",
      "epoch 88 | loss: 0.01165 | val_logits_ll: 0.01963 |  0:02:29s\n",
      "epoch 89 | loss: 0.01164 | val_logits_ll: 0.01963 |  0:02:31s\n",
      "epoch 90 | loss: 0.01166 | val_logits_ll: 0.01964 |  0:02:32s\n",
      "epoch 91 | loss: 0.01163 | val_logits_ll: 0.01963 |  0:02:34s\n",
      "epoch 92 | loss: 0.01164 | val_logits_ll: 0.01962 |  0:02:36s\n",
      "epoch 93 | loss: 0.01167 | val_logits_ll: 0.01964 |  0:02:38s\n",
      "epoch 94 | loss: 0.01164 | val_logits_ll: 0.01963 |  0:02:40s\n",
      "epoch 95 | loss: 0.01165 | val_logits_ll: 0.01962 |  0:02:41s\n",
      "\n",
      "Early stopping occured at epoch 95 with best_epoch = 45 and best_val_logits_ll = 0.01824\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold2_42.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40647 | val_logits_ll: 0.1033  |  0:00:01s\n",
      "epoch 1  | loss: 0.03356 | val_logits_ll: 0.02758 |  0:00:03s\n",
      "epoch 2  | loss: 0.02473 | val_logits_ll: 0.02339 |  0:00:05s\n",
      "epoch 3  | loss: 0.02196 | val_logits_ll: 0.02143 |  0:00:06s\n",
      "epoch 4  | loss: 0.02113 | val_logits_ll: 0.02086 |  0:00:08s\n",
      "epoch 5  | loss: 0.0207  | val_logits_ll: 0.02056 |  0:00:10s\n",
      "epoch 6  | loss: 0.02031 | val_logits_ll: 0.02034 |  0:00:11s\n",
      "epoch 7  | loss: 0.02003 | val_logits_ll: 0.02013 |  0:00:13s\n",
      "epoch 8  | loss: 0.01975 | val_logits_ll: 0.01993 |  0:00:15s\n",
      "epoch 9  | loss: 0.01943 | val_logits_ll: 0.01956 |  0:00:16s\n",
      "epoch 10 | loss: 0.0191  | val_logits_ll: 0.01935 |  0:00:18s\n",
      "epoch 11 | loss: 0.01876 | val_logits_ll: 0.01908 |  0:00:20s\n",
      "epoch 12 | loss: 0.01844 | val_logits_ll: 0.019   |  0:00:21s\n",
      "epoch 13 | loss: 0.0182  | val_logits_ll: 0.01887 |  0:00:23s\n",
      "epoch 14 | loss: 0.01795 | val_logits_ll: 0.01876 |  0:00:25s\n",
      "epoch 15 | loss: 0.01771 | val_logits_ll: 0.0185  |  0:00:26s\n",
      "epoch 16 | loss: 0.01753 | val_logits_ll: 0.01828 |  0:00:28s\n",
      "epoch 17 | loss: 0.01742 | val_logits_ll: 0.02057 |  0:00:30s\n",
      "epoch 18 | loss: 0.01719 | val_logits_ll: 0.01799 |  0:00:31s\n",
      "epoch 19 | loss: 0.01701 | val_logits_ll: 0.0182  |  0:00:33s\n",
      "epoch 20 | loss: 0.01687 | val_logits_ll: 0.0179  |  0:00:35s\n",
      "epoch 21 | loss: 0.01683 | val_logits_ll: 0.01806 |  0:00:37s\n",
      "epoch 22 | loss: 0.01669 | val_logits_ll: 0.01791 |  0:00:38s\n",
      "epoch 23 | loss: 0.01657 | val_logits_ll: 0.01773 |  0:00:40s\n",
      "epoch 24 | loss: 0.01645 | val_logits_ll: 0.01963 |  0:00:42s\n",
      "epoch 25 | loss: 0.0163  | val_logits_ll: 0.01908 |  0:00:43s\n",
      "epoch 26 | loss: 0.01636 | val_logits_ll: 0.01786 |  0:00:45s\n",
      "epoch 27 | loss: 0.01617 | val_logits_ll: 0.01964 |  0:00:47s\n",
      "epoch 28 | loss: 0.01614 | val_logits_ll: 0.01771 |  0:00:49s\n",
      "epoch 29 | loss: 0.01604 | val_logits_ll: 0.01875 |  0:00:50s\n",
      "epoch 30 | loss: 0.01585 | val_logits_ll: 0.01755 |  0:00:52s\n",
      "epoch 31 | loss: 0.01591 | val_logits_ll: 0.01831 |  0:00:54s\n",
      "epoch 32 | loss: 0.01594 | val_logits_ll: 0.01742 |  0:00:55s\n",
      "epoch 33 | loss: 0.01576 | val_logits_ll: 0.01763 |  0:00:57s\n",
      "epoch 34 | loss: 0.01576 | val_logits_ll: 0.01781 |  0:00:59s\n",
      "epoch 35 | loss: 0.01581 | val_logits_ll: 0.01791 |  0:01:00s\n",
      "epoch 36 | loss: 0.0157  | val_logits_ll: 0.01772 |  0:01:02s\n",
      "epoch 37 | loss: 0.01557 | val_logits_ll: 0.01804 |  0:01:04s\n",
      "epoch 38 | loss: 0.01552 | val_logits_ll: 0.0196  |  0:01:06s\n",
      "epoch 39 | loss: 0.01565 | val_logits_ll: 0.01774 |  0:01:07s\n",
      "epoch 40 | loss: 0.01564 | val_logits_ll: 0.01776 |  0:01:09s\n",
      "epoch 41 | loss: 0.01553 | val_logits_ll: 0.01737 |  0:01:11s\n",
      "epoch 42 | loss: 0.01547 | val_logits_ll: 0.01747 |  0:01:12s\n",
      "epoch 43 | loss: 0.01541 | val_logits_ll: 0.01766 |  0:01:14s\n",
      "epoch 44 | loss: 0.01541 | val_logits_ll: 0.01771 |  0:01:15s\n",
      "epoch 45 | loss: 0.01546 | val_logits_ll: 0.01787 |  0:01:17s\n",
      "epoch 46 | loss: 0.01544 | val_logits_ll: 0.01774 |  0:01:19s\n",
      "epoch 47 | loss: 0.01541 | val_logits_ll: 0.01745 |  0:01:21s\n",
      "epoch 48 | loss: 0.01535 | val_logits_ll: 0.01747 |  0:01:22s\n",
      "epoch 49 | loss: 0.01527 | val_logits_ll: 0.01766 |  0:01:24s\n",
      "epoch 50 | loss: 0.01528 | val_logits_ll: 0.01767 |  0:01:25s\n",
      "epoch 51 | loss: 0.01524 | val_logits_ll: 0.01785 |  0:01:27s\n",
      "epoch 52 | loss: 0.01526 | val_logits_ll: 0.01769 |  0:01:29s\n",
      "epoch 53 | loss: 0.01463 | val_logits_ll: 0.01713 |  0:01:31s\n",
      "epoch 54 | loss: 0.01412 | val_logits_ll: 0.0172  |  0:01:32s\n",
      "epoch 55 | loss: 0.01387 | val_logits_ll: 0.01726 |  0:01:34s\n",
      "epoch 56 | loss: 0.01368 | val_logits_ll: 0.01736 |  0:01:36s\n",
      "epoch 57 | loss: 0.0135  | val_logits_ll: 0.01745 |  0:01:37s\n",
      "epoch 58 | loss: 0.01336 | val_logits_ll: 0.01757 |  0:01:39s\n",
      "epoch 59 | loss: 0.01324 | val_logits_ll: 0.01766 |  0:01:41s\n",
      "epoch 60 | loss: 0.01309 | val_logits_ll: 0.01774 |  0:01:43s\n",
      "epoch 61 | loss: 0.01294 | val_logits_ll: 0.01786 |  0:01:44s\n",
      "epoch 62 | loss: 0.0128  | val_logits_ll: 0.01802 |  0:01:46s\n",
      "epoch 63 | loss: 0.01271 | val_logits_ll: 0.01816 |  0:01:48s\n",
      "epoch 64 | loss: 0.01258 | val_logits_ll: 0.01826 |  0:01:50s\n",
      "epoch 65 | loss: 0.01226 | val_logits_ll: 0.01832 |  0:01:51s\n",
      "epoch 66 | loss: 0.01214 | val_logits_ll: 0.01832 |  0:01:53s\n",
      "epoch 67 | loss: 0.01209 | val_logits_ll: 0.01835 |  0:01:55s\n",
      "epoch 68 | loss: 0.01205 | val_logits_ll: 0.01838 |  0:01:56s\n",
      "epoch 69 | loss: 0.012   | val_logits_ll: 0.01841 |  0:01:58s\n",
      "epoch 70 | loss: 0.01199 | val_logits_ll: 0.01844 |  0:02:00s\n",
      "epoch 71 | loss: 0.01197 | val_logits_ll: 0.01843 |  0:02:01s\n",
      "epoch 72 | loss: 0.01193 | val_logits_ll: 0.01848 |  0:02:03s\n",
      "epoch 73 | loss: 0.0119  | val_logits_ll: 0.0185  |  0:02:05s\n",
      "epoch 74 | loss: 0.01187 | val_logits_ll: 0.01851 |  0:02:06s\n",
      "epoch 75 | loss: 0.01185 | val_logits_ll: 0.01852 |  0:02:08s\n",
      "epoch 76 | loss: 0.01182 | val_logits_ll: 0.01854 |  0:02:10s\n",
      "epoch 77 | loss: 0.01183 | val_logits_ll: 0.01856 |  0:02:11s\n",
      "epoch 78 | loss: 0.01179 | val_logits_ll: 0.01858 |  0:02:13s\n",
      "epoch 79 | loss: 0.0118  | val_logits_ll: 0.01856 |  0:02:15s\n",
      "epoch 80 | loss: 0.01178 | val_logits_ll: 0.01856 |  0:02:16s\n",
      "epoch 81 | loss: 0.01179 | val_logits_ll: 0.01856 |  0:02:18s\n",
      "epoch 82 | loss: 0.01178 | val_logits_ll: 0.01856 |  0:02:19s\n",
      "epoch 83 | loss: 0.01177 | val_logits_ll: 0.01855 |  0:02:21s\n",
      "epoch 84 | loss: 0.01177 | val_logits_ll: 0.01856 |  0:02:23s\n",
      "epoch 85 | loss: 0.01179 | val_logits_ll: 0.01858 |  0:02:25s\n",
      "epoch 86 | loss: 0.01177 | val_logits_ll: 0.01857 |  0:02:26s\n",
      "epoch 87 | loss: 0.01176 | val_logits_ll: 0.01857 |  0:02:28s\n",
      "epoch 88 | loss: 0.01175 | val_logits_ll: 0.01857 |  0:02:29s\n",
      "epoch 89 | loss: 0.01178 | val_logits_ll: 0.01858 |  0:02:31s\n",
      "epoch 90 | loss: 0.01178 | val_logits_ll: 0.01858 |  0:02:33s\n",
      "epoch 91 | loss: 0.01176 | val_logits_ll: 0.01857 |  0:02:34s\n",
      "epoch 92 | loss: 0.01177 | val_logits_ll: 0.01858 |  0:02:36s\n",
      "epoch 93 | loss: 0.01176 | val_logits_ll: 0.01857 |  0:02:38s\n",
      "epoch 94 | loss: 0.01178 | val_logits_ll: 0.01856 |  0:02:39s\n",
      "epoch 95 | loss: 0.01177 | val_logits_ll: 0.01857 |  0:02:41s\n",
      "epoch 96 | loss: 0.01175 | val_logits_ll: 0.01858 |  0:02:43s\n",
      "epoch 97 | loss: 0.01175 | val_logits_ll: 0.01858 |  0:02:45s\n",
      "epoch 98 | loss: 0.01177 | val_logits_ll: 0.01857 |  0:02:47s\n",
      "epoch 99 | loss: 0.01176 | val_logits_ll: 0.01858 |  0:02:49s\n",
      "epoch 100| loss: 0.01177 | val_logits_ll: 0.01858 |  0:02:50s\n",
      "epoch 101| loss: 0.01177 | val_logits_ll: 0.01857 |  0:02:52s\n",
      "epoch 102| loss: 0.01175 | val_logits_ll: 0.01857 |  0:02:54s\n",
      "epoch 103| loss: 0.01175 | val_logits_ll: 0.01858 |  0:02:55s\n",
      "\n",
      "Early stopping occured at epoch 103 with best_epoch = 53 and best_val_logits_ll = 0.01713\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold3_42.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40297 | val_logits_ll: 0.10418 |  0:00:01s\n",
      "epoch 1  | loss: 0.03232 | val_logits_ll: 0.02681 |  0:00:03s\n",
      "epoch 2  | loss: 0.02392 | val_logits_ll: 0.02219 |  0:00:04s\n",
      "epoch 3  | loss: 0.02159 | val_logits_ll: 0.02139 |  0:00:06s\n",
      "epoch 4  | loss: 0.02084 | val_logits_ll: 0.02092 |  0:00:07s\n",
      "epoch 5  | loss: 0.02043 | val_logits_ll: 0.02058 |  0:00:09s\n",
      "epoch 6  | loss: 0.02014 | val_logits_ll: 0.02041 |  0:00:11s\n",
      "epoch 7  | loss: 0.01983 | val_logits_ll: 0.02012 |  0:00:12s\n",
      "epoch 8  | loss: 0.01946 | val_logits_ll: 0.0199  |  0:00:14s\n",
      "epoch 9  | loss: 0.01912 | val_logits_ll: 0.01968 |  0:00:16s\n",
      "epoch 10 | loss: 0.0187  | val_logits_ll: 0.0201  |  0:00:18s\n",
      "epoch 11 | loss: 0.01842 | val_logits_ll: 0.01916 |  0:00:19s\n",
      "epoch 12 | loss: 0.01811 | val_logits_ll: 0.01919 |  0:00:21s\n",
      "epoch 13 | loss: 0.01795 | val_logits_ll: 0.01914 |  0:00:23s\n",
      "epoch 14 | loss: 0.01772 | val_logits_ll: 0.01892 |  0:00:24s\n",
      "epoch 15 | loss: 0.01749 | val_logits_ll: 0.01908 |  0:00:26s\n",
      "epoch 16 | loss: 0.01737 | val_logits_ll: 0.0197  |  0:00:27s\n",
      "epoch 17 | loss: 0.01716 | val_logits_ll: 0.01947 |  0:00:29s\n",
      "epoch 18 | loss: 0.01711 | val_logits_ll: 0.02037 |  0:00:31s\n",
      "epoch 19 | loss: 0.01725 | val_logits_ll: 0.01873 |  0:00:32s\n",
      "epoch 20 | loss: 0.01697 | val_logits_ll: 0.01868 |  0:00:34s\n",
      "epoch 21 | loss: 0.01684 | val_logits_ll: 0.01908 |  0:00:36s\n",
      "epoch 22 | loss: 0.01669 | val_logits_ll: 0.01879 |  0:00:37s\n",
      "epoch 23 | loss: 0.01678 | val_logits_ll: 0.01875 |  0:00:39s\n",
      "epoch 24 | loss: 0.01662 | val_logits_ll: 0.0185  |  0:00:41s\n",
      "epoch 25 | loss: 0.0164  | val_logits_ll: 0.02042 |  0:00:42s\n",
      "epoch 26 | loss: 0.01629 | val_logits_ll: 0.02076 |  0:00:44s\n",
      "epoch 27 | loss: 0.01625 | val_logits_ll: 0.01833 |  0:00:46s\n",
      "epoch 28 | loss: 0.0162  | val_logits_ll: 0.01825 |  0:00:48s\n",
      "epoch 29 | loss: 0.01608 | val_logits_ll: 0.01884 |  0:00:50s\n",
      "epoch 30 | loss: 0.01614 | val_logits_ll: 0.01826 |  0:00:52s\n",
      "epoch 31 | loss: 0.016   | val_logits_ll: 0.01977 |  0:00:53s\n",
      "epoch 32 | loss: 0.01599 | val_logits_ll: 0.01869 |  0:00:55s\n",
      "epoch 33 | loss: 0.01594 | val_logits_ll: 0.02065 |  0:00:57s\n",
      "epoch 34 | loss: 0.01589 | val_logits_ll: 0.01965 |  0:00:58s\n",
      "epoch 35 | loss: 0.01587 | val_logits_ll: 0.01827 |  0:01:00s\n",
      "epoch 36 | loss: 0.01577 | val_logits_ll: 0.01851 |  0:01:01s\n",
      "epoch 37 | loss: 0.01571 | val_logits_ll: 0.01874 |  0:01:03s\n",
      "epoch 38 | loss: 0.01572 | val_logits_ll: 0.01834 |  0:01:05s\n",
      "epoch 39 | loss: 0.0156  | val_logits_ll: 0.01863 |  0:01:07s\n",
      "epoch 40 | loss: 0.01509 | val_logits_ll: 0.01783 |  0:01:08s\n",
      "epoch 41 | loss: 0.01464 | val_logits_ll: 0.01788 |  0:01:10s\n",
      "epoch 42 | loss: 0.01444 | val_logits_ll: 0.01792 |  0:01:11s\n",
      "epoch 43 | loss: 0.01431 | val_logits_ll: 0.01801 |  0:01:13s\n",
      "epoch 44 | loss: 0.01418 | val_logits_ll: 0.01805 |  0:01:15s\n",
      "epoch 45 | loss: 0.01402 | val_logits_ll: 0.01811 |  0:01:16s\n",
      "epoch 46 | loss: 0.01391 | val_logits_ll: 0.01824 |  0:01:18s\n",
      "epoch 47 | loss: 0.0138  | val_logits_ll: 0.0183  |  0:01:20s\n",
      "epoch 48 | loss: 0.01369 | val_logits_ll: 0.01846 |  0:01:21s\n",
      "epoch 49 | loss: 0.0136  | val_logits_ll: 0.01849 |  0:01:23s\n",
      "epoch 50 | loss: 0.01348 | val_logits_ll: 0.01855 |  0:01:25s\n",
      "epoch 51 | loss: 0.01339 | val_logits_ll: 0.01863 |  0:01:26s\n",
      "epoch 52 | loss: 0.01306 | val_logits_ll: 0.01868 |  0:01:28s\n",
      "epoch 53 | loss: 0.01295 | val_logits_ll: 0.0187  |  0:01:30s\n",
      "epoch 54 | loss: 0.0129  | val_logits_ll: 0.01872 |  0:01:31s\n",
      "epoch 55 | loss: 0.01287 | val_logits_ll: 0.01878 |  0:01:33s\n",
      "epoch 56 | loss: 0.01283 | val_logits_ll: 0.01879 |  0:01:35s\n",
      "epoch 57 | loss: 0.01278 | val_logits_ll: 0.01882 |  0:01:36s\n",
      "epoch 58 | loss: 0.01276 | val_logits_ll: 0.01884 |  0:01:38s\n",
      "epoch 59 | loss: 0.01273 | val_logits_ll: 0.01886 |  0:01:40s\n",
      "epoch 60 | loss: 0.01271 | val_logits_ll: 0.01887 |  0:01:41s\n",
      "epoch 61 | loss: 0.01266 | val_logits_ll: 0.01893 |  0:01:43s\n",
      "epoch 62 | loss: 0.01264 | val_logits_ll: 0.01893 |  0:01:45s\n",
      "epoch 63 | loss: 0.01261 | val_logits_ll: 0.01896 |  0:01:46s\n",
      "epoch 64 | loss: 0.0126  | val_logits_ll: 0.01894 |  0:01:48s\n",
      "epoch 65 | loss: 0.01258 | val_logits_ll: 0.01893 |  0:01:50s\n",
      "epoch 66 | loss: 0.01259 | val_logits_ll: 0.01895 |  0:01:52s\n",
      "epoch 67 | loss: 0.0126  | val_logits_ll: 0.01895 |  0:01:53s\n",
      "epoch 68 | loss: 0.01258 | val_logits_ll: 0.01897 |  0:01:55s\n",
      "epoch 69 | loss: 0.01258 | val_logits_ll: 0.01896 |  0:01:57s\n",
      "epoch 70 | loss: 0.01259 | val_logits_ll: 0.01896 |  0:01:58s\n",
      "epoch 71 | loss: 0.01258 | val_logits_ll: 0.01898 |  0:02:00s\n",
      "epoch 72 | loss: 0.01259 | val_logits_ll: 0.01897 |  0:02:02s\n",
      "epoch 73 | loss: 0.01257 | val_logits_ll: 0.01899 |  0:02:03s\n",
      "epoch 74 | loss: 0.01257 | val_logits_ll: 0.01898 |  0:02:05s\n",
      "epoch 75 | loss: 0.01258 | val_logits_ll: 0.01898 |  0:02:07s\n",
      "epoch 76 | loss: 0.01257 | val_logits_ll: 0.01899 |  0:02:08s\n",
      "epoch 77 | loss: 0.01257 | val_logits_ll: 0.01898 |  0:02:10s\n",
      "epoch 78 | loss: 0.01258 | val_logits_ll: 0.01898 |  0:02:12s\n",
      "epoch 79 | loss: 0.01257 | val_logits_ll: 0.01898 |  0:02:13s\n",
      "epoch 80 | loss: 0.01257 | val_logits_ll: 0.01899 |  0:02:15s\n",
      "epoch 81 | loss: 0.01259 | val_logits_ll: 0.01899 |  0:02:17s\n",
      "epoch 82 | loss: 0.01257 | val_logits_ll: 0.01899 |  0:02:18s\n",
      "epoch 83 | loss: 0.01257 | val_logits_ll: 0.01898 |  0:02:20s\n",
      "epoch 84 | loss: 0.01257 | val_logits_ll: 0.01896 |  0:02:21s\n",
      "epoch 85 | loss: 0.01255 | val_logits_ll: 0.01898 |  0:02:23s\n",
      "epoch 86 | loss: 0.01257 | val_logits_ll: 0.01899 |  0:02:25s\n",
      "epoch 87 | loss: 0.01255 | val_logits_ll: 0.01899 |  0:02:27s\n",
      "epoch 88 | loss: 0.01256 | val_logits_ll: 0.01898 |  0:02:28s\n",
      "epoch 89 | loss: 0.01256 | val_logits_ll: 0.01897 |  0:02:30s\n",
      "epoch 90 | loss: 0.01256 | val_logits_ll: 0.01898 |  0:02:32s\n",
      "\n",
      "Early stopping occured at epoch 90 with best_epoch = 40 and best_val_logits_ll = 0.01783\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold4_42.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4036  | val_logits_ll: 0.09972 |  0:00:01s\n",
      "epoch 1  | loss: 0.03191 | val_logits_ll: 0.02737 |  0:00:03s\n",
      "epoch 2  | loss: 0.02405 | val_logits_ll: 0.02284 |  0:00:04s\n",
      "epoch 3  | loss: 0.02144 | val_logits_ll: 0.02131 |  0:00:06s\n",
      "epoch 4  | loss: 0.02072 | val_logits_ll: 0.02091 |  0:00:07s\n",
      "epoch 5  | loss: 0.02031 | val_logits_ll: 0.02063 |  0:00:09s\n",
      "epoch 6  | loss: 0.02005 | val_logits_ll: 0.02042 |  0:00:11s\n",
      "epoch 7  | loss: 0.01972 | val_logits_ll: 0.02001 |  0:00:13s\n",
      "epoch 8  | loss: 0.01926 | val_logits_ll: 0.01977 |  0:00:15s\n",
      "epoch 9  | loss: 0.01886 | val_logits_ll: 0.01941 |  0:00:16s\n",
      "epoch 10 | loss: 0.0185  | val_logits_ll: 0.01918 |  0:00:18s\n",
      "epoch 11 | loss: 0.0182  | val_logits_ll: 0.02141 |  0:00:20s\n",
      "epoch 12 | loss: 0.01797 | val_logits_ll: 0.01879 |  0:00:21s\n",
      "epoch 13 | loss: 0.01777 | val_logits_ll: 0.01874 |  0:00:23s\n",
      "epoch 14 | loss: 0.01762 | val_logits_ll: 0.01881 |  0:00:25s\n",
      "epoch 15 | loss: 0.01746 | val_logits_ll: 0.01875 |  0:00:26s\n",
      "epoch 16 | loss: 0.01737 | val_logits_ll: 0.01878 |  0:00:28s\n",
      "epoch 17 | loss: 0.0172  | val_logits_ll: 0.01871 |  0:00:30s\n",
      "epoch 18 | loss: 0.01707 | val_logits_ll: 0.01949 |  0:00:31s\n",
      "epoch 19 | loss: 0.01695 | val_logits_ll: 0.02053 |  0:00:33s\n",
      "epoch 20 | loss: 0.01683 | val_logits_ll: 0.01833 |  0:00:35s\n",
      "epoch 21 | loss: 0.01684 | val_logits_ll: 0.01834 |  0:00:36s\n",
      "epoch 22 | loss: 0.01667 | val_logits_ll: 0.01842 |  0:00:38s\n",
      "epoch 23 | loss: 0.01657 | val_logits_ll: 0.01855 |  0:00:40s\n",
      "epoch 24 | loss: 0.01652 | val_logits_ll: 0.02059 |  0:00:41s\n",
      "epoch 25 | loss: 0.0165  | val_logits_ll: 0.01825 |  0:00:43s\n",
      "epoch 26 | loss: 0.01636 | val_logits_ll: 0.01817 |  0:00:45s\n",
      "epoch 27 | loss: 0.01629 | val_logits_ll: 0.01842 |  0:00:46s\n",
      "epoch 28 | loss: 0.01617 | val_logits_ll: 0.0187  |  0:00:48s\n",
      "epoch 29 | loss: 0.01606 | val_logits_ll: 0.01986 |  0:00:49s\n",
      "epoch 30 | loss: 0.016   | val_logits_ll: 0.01868 |  0:00:51s\n",
      "epoch 31 | loss: 0.01603 | val_logits_ll: 0.01918 |  0:00:53s\n",
      "epoch 32 | loss: 0.01592 | val_logits_ll: 0.01851 |  0:00:55s\n",
      "epoch 33 | loss: 0.0159  | val_logits_ll: 0.0184  |  0:00:56s\n",
      "epoch 34 | loss: 0.01582 | val_logits_ll: 0.01952 |  0:00:58s\n",
      "epoch 35 | loss: 0.01577 | val_logits_ll: 0.01867 |  0:01:00s\n",
      "epoch 36 | loss: 0.01576 | val_logits_ll: 0.01839 |  0:01:01s\n",
      "epoch 37 | loss: 0.01568 | val_logits_ll: 0.01813 |  0:01:03s\n",
      "epoch 38 | loss: 0.01574 | val_logits_ll: 0.01816 |  0:01:05s\n",
      "epoch 39 | loss: 0.01558 | val_logits_ll: 0.01819 |  0:01:06s\n",
      "epoch 40 | loss: 0.01559 | val_logits_ll: 0.01819 |  0:01:08s\n",
      "epoch 41 | loss: 0.01552 | val_logits_ll: 0.01831 |  0:01:10s\n",
      "epoch 42 | loss: 0.01547 | val_logits_ll: 0.01802 |  0:01:12s\n",
      "epoch 43 | loss: 0.0155  | val_logits_ll: 0.01848 |  0:01:14s\n",
      "epoch 44 | loss: 0.01547 | val_logits_ll: 0.01827 |  0:01:15s\n",
      "epoch 45 | loss: 0.01543 | val_logits_ll: 0.01842 |  0:01:17s\n",
      "epoch 46 | loss: 0.01534 | val_logits_ll: 0.01841 |  0:01:19s\n",
      "epoch 47 | loss: 0.0153  | val_logits_ll: 0.01839 |  0:01:20s\n",
      "epoch 48 | loss: 0.01524 | val_logits_ll: 0.01836 |  0:01:22s\n",
      "epoch 49 | loss: 0.01528 | val_logits_ll: 0.0183  |  0:01:24s\n",
      "epoch 50 | loss: 0.01521 | val_logits_ll: 0.01851 |  0:01:25s\n",
      "epoch 51 | loss: 0.01507 | val_logits_ll: 0.01858 |  0:01:27s\n",
      "epoch 52 | loss: 0.01521 | val_logits_ll: 0.01858 |  0:01:29s\n",
      "epoch 53 | loss: 0.01512 | val_logits_ll: 0.01867 |  0:01:31s\n",
      "epoch 54 | loss: 0.0145  | val_logits_ll: 0.01805 |  0:01:32s\n",
      "epoch 55 | loss: 0.01403 | val_logits_ll: 0.01812 |  0:01:34s\n",
      "epoch 56 | loss: 0.01377 | val_logits_ll: 0.01823 |  0:01:35s\n",
      "epoch 57 | loss: 0.01358 | val_logits_ll: 0.01834 |  0:01:37s\n",
      "epoch 58 | loss: 0.01344 | val_logits_ll: 0.01846 |  0:01:39s\n",
      "epoch 59 | loss: 0.01329 | val_logits_ll: 0.01854 |  0:01:40s\n",
      "epoch 60 | loss: 0.01314 | val_logits_ll: 0.01863 |  0:01:42s\n",
      "epoch 61 | loss: 0.013   | val_logits_ll: 0.01869 |  0:01:43s\n",
      "epoch 62 | loss: 0.01288 | val_logits_ll: 0.01885 |  0:01:45s\n",
      "epoch 63 | loss: 0.01277 | val_logits_ll: 0.01901 |  0:01:47s\n",
      "epoch 64 | loss: 0.01263 | val_logits_ll: 0.01917 |  0:01:48s\n",
      "epoch 65 | loss: 0.01237 | val_logits_ll: 0.01914 |  0:01:50s\n",
      "epoch 66 | loss: 0.01226 | val_logits_ll: 0.01918 |  0:01:52s\n",
      "epoch 67 | loss: 0.01221 | val_logits_ll: 0.01921 |  0:01:53s\n",
      "epoch 68 | loss: 0.01218 | val_logits_ll: 0.01924 |  0:01:55s\n",
      "epoch 69 | loss: 0.01214 | val_logits_ll: 0.01925 |  0:01:56s\n",
      "epoch 70 | loss: 0.0121  | val_logits_ll: 0.01927 |  0:01:58s\n",
      "epoch 71 | loss: 0.01209 | val_logits_ll: 0.01931 |  0:01:59s\n",
      "epoch 72 | loss: 0.01207 | val_logits_ll: 0.01933 |  0:02:01s\n",
      "epoch 73 | loss: 0.01203 | val_logits_ll: 0.01936 |  0:02:03s\n",
      "epoch 74 | loss: 0.01198 | val_logits_ll: 0.01938 |  0:02:05s\n",
      "epoch 75 | loss: 0.01196 | val_logits_ll: 0.01941 |  0:02:06s\n",
      "epoch 76 | loss: 0.01192 | val_logits_ll: 0.01942 |  0:02:08s\n",
      "epoch 77 | loss: 0.01191 | val_logits_ll: 0.01943 |  0:02:10s\n",
      "epoch 78 | loss: 0.01191 | val_logits_ll: 0.01941 |  0:02:12s\n",
      "epoch 79 | loss: 0.01191 | val_logits_ll: 0.01944 |  0:02:14s\n",
      "epoch 80 | loss: 0.01192 | val_logits_ll: 0.01943 |  0:02:15s\n",
      "epoch 81 | loss: 0.01189 | val_logits_ll: 0.01944 |  0:02:17s\n",
      "epoch 82 | loss: 0.0119  | val_logits_ll: 0.01944 |  0:02:19s\n",
      "epoch 83 | loss: 0.01189 | val_logits_ll: 0.01943 |  0:02:20s\n",
      "epoch 84 | loss: 0.01187 | val_logits_ll: 0.01944 |  0:02:22s\n",
      "epoch 85 | loss: 0.01189 | val_logits_ll: 0.01944 |  0:02:24s\n",
      "epoch 86 | loss: 0.01188 | val_logits_ll: 0.01945 |  0:02:25s\n",
      "epoch 87 | loss: 0.01189 | val_logits_ll: 0.01948 |  0:02:27s\n",
      "epoch 88 | loss: 0.01189 | val_logits_ll: 0.01945 |  0:02:28s\n",
      "epoch 89 | loss: 0.01189 | val_logits_ll: 0.01946 |  0:02:30s\n",
      "epoch 90 | loss: 0.01186 | val_logits_ll: 0.01946 |  0:02:32s\n",
      "epoch 91 | loss: 0.01188 | val_logits_ll: 0.01948 |  0:02:34s\n",
      "epoch 92 | loss: 0.01187 | val_logits_ll: 0.01945 |  0:02:35s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 42 and best_val_logits_ll = 0.01802\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold5_42.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.405   | val_logits_ll: 0.10264 |  0:00:01s\n",
      "epoch 1  | loss: 0.03339 | val_logits_ll: 0.02617 |  0:00:03s\n",
      "epoch 2  | loss: 0.02423 | val_logits_ll: 0.0216  |  0:00:04s\n",
      "epoch 3  | loss: 0.02155 | val_logits_ll: 0.0206  |  0:00:06s\n",
      "epoch 4  | loss: 0.02078 | val_logits_ll: 0.02012 |  0:00:08s\n",
      "epoch 5  | loss: 0.02035 | val_logits_ll: 0.01985 |  0:00:09s\n",
      "epoch 6  | loss: 0.02014 | val_logits_ll: 0.01969 |  0:00:11s\n",
      "epoch 7  | loss: 0.01982 | val_logits_ll: 0.0196  |  0:00:13s\n",
      "epoch 8  | loss: 0.01948 | val_logits_ll: 0.01921 |  0:00:14s\n",
      "epoch 9  | loss: 0.01925 | val_logits_ll: 0.01914 |  0:00:16s\n",
      "epoch 10 | loss: 0.01886 | val_logits_ll: 0.01875 |  0:00:18s\n",
      "epoch 11 | loss: 0.01854 | val_logits_ll: 0.01858 |  0:00:19s\n",
      "epoch 12 | loss: 0.01819 | val_logits_ll: 0.01842 |  0:00:21s\n",
      "epoch 13 | loss: 0.01799 | val_logits_ll: 0.01818 |  0:00:22s\n",
      "epoch 14 | loss: 0.01777 | val_logits_ll: 0.01804 |  0:00:24s\n",
      "epoch 15 | loss: 0.0176  | val_logits_ll: 0.01824 |  0:00:26s\n",
      "epoch 16 | loss: 0.01739 | val_logits_ll: 0.01856 |  0:00:27s\n",
      "epoch 17 | loss: 0.01729 | val_logits_ll: 0.018   |  0:00:30s\n",
      "epoch 18 | loss: 0.01715 | val_logits_ll: 0.01826 |  0:00:31s\n",
      "epoch 19 | loss: 0.01705 | val_logits_ll: 0.01803 |  0:00:33s\n",
      "epoch 20 | loss: 0.01701 | val_logits_ll: 0.01793 |  0:00:35s\n",
      "epoch 21 | loss: 0.01692 | val_logits_ll: 0.0181  |  0:00:36s\n",
      "epoch 22 | loss: 0.01678 | val_logits_ll: 0.01823 |  0:00:38s\n",
      "epoch 23 | loss: 0.01664 | val_logits_ll: 0.0177  |  0:00:40s\n",
      "epoch 24 | loss: 0.01662 | val_logits_ll: 0.01776 |  0:00:42s\n",
      "epoch 25 | loss: 0.01653 | val_logits_ll: 0.0178  |  0:00:43s\n",
      "epoch 26 | loss: 0.01649 | val_logits_ll: 0.01799 |  0:00:45s\n",
      "epoch 27 | loss: 0.01644 | val_logits_ll: 0.01768 |  0:00:47s\n",
      "epoch 28 | loss: 0.01641 | val_logits_ll: 0.01747 |  0:00:48s\n",
      "epoch 29 | loss: 0.01625 | val_logits_ll: 0.01745 |  0:00:50s\n",
      "epoch 30 | loss: 0.0162  | val_logits_ll: 0.01757 |  0:00:52s\n",
      "epoch 31 | loss: 0.01612 | val_logits_ll: 0.0176  |  0:00:53s\n",
      "epoch 32 | loss: 0.01611 | val_logits_ll: 0.01742 |  0:00:55s\n",
      "epoch 33 | loss: 0.01606 | val_logits_ll: 0.0174  |  0:00:56s\n",
      "epoch 34 | loss: 0.01594 | val_logits_ll: 0.01737 |  0:00:58s\n",
      "epoch 35 | loss: 0.01594 | val_logits_ll: 0.01742 |  0:00:59s\n",
      "epoch 36 | loss: 0.01599 | val_logits_ll: 0.02043 |  0:01:02s\n",
      "epoch 37 | loss: 0.01585 | val_logits_ll: 0.01754 |  0:01:03s\n",
      "epoch 38 | loss: 0.01574 | val_logits_ll: 0.01928 |  0:01:05s\n",
      "epoch 39 | loss: 0.01575 | val_logits_ll: 0.01739 |  0:01:07s\n",
      "epoch 40 | loss: 0.01566 | val_logits_ll: 0.01757 |  0:01:08s\n",
      "epoch 41 | loss: 0.01563 | val_logits_ll: 0.01763 |  0:01:10s\n",
      "epoch 42 | loss: 0.01559 | val_logits_ll: 0.01754 |  0:01:11s\n",
      "epoch 43 | loss: 0.0156  | val_logits_ll: 0.01737 |  0:01:13s\n",
      "epoch 44 | loss: 0.01552 | val_logits_ll: 0.01755 |  0:01:15s\n",
      "epoch 45 | loss: 0.01547 | val_logits_ll: 0.01791 |  0:01:16s\n",
      "epoch 46 | loss: 0.015   | val_logits_ll: 0.01725 |  0:01:18s\n",
      "epoch 47 | loss: 0.01447 | val_logits_ll: 0.01727 |  0:01:20s\n",
      "epoch 48 | loss: 0.01423 | val_logits_ll: 0.01733 |  0:01:21s\n",
      "epoch 49 | loss: 0.01408 | val_logits_ll: 0.0174  |  0:01:23s\n",
      "epoch 50 | loss: 0.01391 | val_logits_ll: 0.01748 |  0:01:25s\n",
      "epoch 51 | loss: 0.01377 | val_logits_ll: 0.01754 |  0:01:26s\n",
      "epoch 52 | loss: 0.01364 | val_logits_ll: 0.01758 |  0:01:28s\n",
      "epoch 53 | loss: 0.01353 | val_logits_ll: 0.01774 |  0:01:29s\n",
      "epoch 54 | loss: 0.01337 | val_logits_ll: 0.01779 |  0:01:31s\n",
      "epoch 55 | loss: 0.01327 | val_logits_ll: 0.01789 |  0:01:33s\n",
      "epoch 56 | loss: 0.01315 | val_logits_ll: 0.01804 |  0:01:36s\n",
      "epoch 57 | loss: 0.01305 | val_logits_ll: 0.01808 |  0:01:37s\n",
      "epoch 58 | loss: 0.01279 | val_logits_ll: 0.01812 |  0:01:39s\n",
      "epoch 59 | loss: 0.01269 | val_logits_ll: 0.01817 |  0:01:41s\n",
      "epoch 60 | loss: 0.01263 | val_logits_ll: 0.0182  |  0:01:42s\n",
      "epoch 61 | loss: 0.01259 | val_logits_ll: 0.01823 |  0:01:44s\n",
      "epoch 62 | loss: 0.01256 | val_logits_ll: 0.01826 |  0:01:46s\n",
      "epoch 63 | loss: 0.01252 | val_logits_ll: 0.0183  |  0:01:47s\n",
      "epoch 64 | loss: 0.01249 | val_logits_ll: 0.01834 |  0:01:49s\n",
      "epoch 65 | loss: 0.01246 | val_logits_ll: 0.01837 |  0:01:51s\n",
      "epoch 66 | loss: 0.01245 | val_logits_ll: 0.01837 |  0:01:52s\n",
      "epoch 67 | loss: 0.01242 | val_logits_ll: 0.01839 |  0:01:54s\n",
      "epoch 68 | loss: 0.01241 | val_logits_ll: 0.01842 |  0:01:55s\n",
      "epoch 69 | loss: 0.01236 | val_logits_ll: 0.01842 |  0:01:57s\n",
      "epoch 70 | loss: 0.01236 | val_logits_ll: 0.01842 |  0:01:59s\n",
      "epoch 71 | loss: 0.01232 | val_logits_ll: 0.01841 |  0:02:01s\n",
      "epoch 72 | loss: 0.01233 | val_logits_ll: 0.01843 |  0:02:02s\n",
      "epoch 73 | loss: 0.01234 | val_logits_ll: 0.01843 |  0:02:04s\n",
      "epoch 74 | loss: 0.01232 | val_logits_ll: 0.01844 |  0:02:06s\n",
      "epoch 75 | loss: 0.01233 | val_logits_ll: 0.01844 |  0:02:08s\n",
      "epoch 76 | loss: 0.01233 | val_logits_ll: 0.01844 |  0:02:09s\n",
      "epoch 77 | loss: 0.01231 | val_logits_ll: 0.01846 |  0:02:11s\n",
      "epoch 78 | loss: 0.01233 | val_logits_ll: 0.01845 |  0:02:12s\n",
      "epoch 79 | loss: 0.01233 | val_logits_ll: 0.01844 |  0:02:14s\n",
      "epoch 80 | loss: 0.01233 | val_logits_ll: 0.01845 |  0:02:16s\n",
      "epoch 81 | loss: 0.01231 | val_logits_ll: 0.01845 |  0:02:17s\n",
      "epoch 82 | loss: 0.01231 | val_logits_ll: 0.01843 |  0:02:19s\n",
      "epoch 83 | loss: 0.0123  | val_logits_ll: 0.01844 |  0:02:21s\n",
      "epoch 84 | loss: 0.0123  | val_logits_ll: 0.01845 |  0:02:22s\n",
      "epoch 85 | loss: 0.01231 | val_logits_ll: 0.01845 |  0:02:24s\n",
      "epoch 86 | loss: 0.01229 | val_logits_ll: 0.01845 |  0:02:26s\n",
      "epoch 87 | loss: 0.0123  | val_logits_ll: 0.01846 |  0:02:27s\n",
      "epoch 88 | loss: 0.01232 | val_logits_ll: 0.01846 |  0:02:29s\n",
      "epoch 89 | loss: 0.0123  | val_logits_ll: 0.01845 |  0:02:31s\n",
      "epoch 90 | loss: 0.01231 | val_logits_ll: 0.01845 |  0:02:33s\n",
      "epoch 91 | loss: 0.0123  | val_logits_ll: 0.01846 |  0:02:34s\n",
      "epoch 92 | loss: 0.01231 | val_logits_ll: 0.01845 |  0:02:36s\n",
      "epoch 93 | loss: 0.01232 | val_logits_ll: 0.01845 |  0:02:38s\n",
      "epoch 94 | loss: 0.0123  | val_logits_ll: 0.01845 |  0:02:40s\n",
      "epoch 95 | loss: 0.01232 | val_logits_ll: 0.01845 |  0:02:42s\n",
      "epoch 96 | loss: 0.0123  | val_logits_ll: 0.01847 |  0:02:43s\n",
      "\n",
      "Early stopping occured at epoch 96 with best_epoch = 46 and best_val_logits_ll = 0.01725\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold6_42.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40545 | val_logits_ll: 0.11108 |  0:00:01s\n",
      "epoch 1  | loss: 0.03374 | val_logits_ll: 0.02659 |  0:00:03s\n",
      "epoch 2  | loss: 0.02492 | val_logits_ll: 0.02162 |  0:00:04s\n",
      "epoch 3  | loss: 0.02189 | val_logits_ll: 0.02041 |  0:00:06s\n",
      "epoch 4  | loss: 0.021   | val_logits_ll: 0.01996 |  0:00:08s\n",
      "epoch 5  | loss: 0.02055 | val_logits_ll: 0.01971 |  0:00:09s\n",
      "epoch 6  | loss: 0.02026 | val_logits_ll: 0.01948 |  0:00:11s\n",
      "epoch 7  | loss: 0.01999 | val_logits_ll: 0.01914 |  0:00:12s\n",
      "epoch 8  | loss: 0.01965 | val_logits_ll: 0.01884 |  0:00:14s\n",
      "epoch 9  | loss: 0.01925 | val_logits_ll: 0.0186  |  0:00:16s\n",
      "epoch 10 | loss: 0.01895 | val_logits_ll: 0.01846 |  0:00:18s\n",
      "epoch 11 | loss: 0.01873 | val_logits_ll: 0.01855 |  0:00:19s\n",
      "epoch 12 | loss: 0.01847 | val_logits_ll: 0.01823 |  0:00:21s\n",
      "epoch 13 | loss: 0.01819 | val_logits_ll: 0.01786 |  0:00:22s\n",
      "epoch 14 | loss: 0.01794 | val_logits_ll: 0.01773 |  0:00:24s\n",
      "epoch 15 | loss: 0.01786 | val_logits_ll: 0.01775 |  0:00:26s\n",
      "epoch 16 | loss: 0.01756 | val_logits_ll: 0.02087 |  0:00:28s\n",
      "epoch 17 | loss: 0.01739 | val_logits_ll: 0.0177  |  0:00:29s\n",
      "epoch 18 | loss: 0.01723 | val_logits_ll: 0.01755 |  0:00:31s\n",
      "epoch 19 | loss: 0.01714 | val_logits_ll: 0.01767 |  0:00:33s\n",
      "epoch 20 | loss: 0.01693 | val_logits_ll: 0.01781 |  0:00:34s\n",
      "epoch 21 | loss: 0.01687 | val_logits_ll: 0.01759 |  0:00:36s\n",
      "epoch 22 | loss: 0.01676 | val_logits_ll: 0.01765 |  0:00:38s\n",
      "epoch 23 | loss: 0.01671 | val_logits_ll: 0.01729 |  0:00:39s\n",
      "epoch 24 | loss: 0.01651 | val_logits_ll: 0.01743 |  0:00:41s\n",
      "epoch 25 | loss: 0.01633 | val_logits_ll: 0.01958 |  0:00:43s\n",
      "epoch 26 | loss: 0.01623 | val_logits_ll: 0.01726 |  0:00:44s\n",
      "epoch 27 | loss: 0.01605 | val_logits_ll: 0.0183  |  0:00:46s\n",
      "epoch 28 | loss: 0.01597 | val_logits_ll: 0.0171  |  0:00:48s\n",
      "epoch 29 | loss: 0.01606 | val_logits_ll: 0.01764 |  0:00:50s\n",
      "epoch 30 | loss: 0.016   | val_logits_ll: 0.01712 |  0:00:52s\n",
      "epoch 31 | loss: 0.01589 | val_logits_ll: 0.01718 |  0:00:53s\n",
      "epoch 32 | loss: 0.01574 | val_logits_ll: 0.01712 |  0:00:55s\n",
      "epoch 33 | loss: 0.0156  | val_logits_ll: 0.01723 |  0:00:57s\n",
      "epoch 34 | loss: 0.01563 | val_logits_ll: 0.01712 |  0:00:59s\n",
      "epoch 35 | loss: 0.01562 | val_logits_ll: 0.01724 |  0:01:01s\n",
      "epoch 36 | loss: 0.01551 | val_logits_ll: 0.01731 |  0:01:02s\n",
      "epoch 37 | loss: 0.01547 | val_logits_ll: 0.01719 |  0:01:04s\n",
      "epoch 38 | loss: 0.01539 | val_logits_ll: 0.01738 |  0:01:05s\n",
      "epoch 39 | loss: 0.01534 | val_logits_ll: 0.01736 |  0:01:07s\n",
      "epoch 40 | loss: 0.01473 | val_logits_ll: 0.01691 |  0:01:08s\n",
      "epoch 41 | loss: 0.01426 | val_logits_ll: 0.017   |  0:01:10s\n",
      "epoch 42 | loss: 0.014   | val_logits_ll: 0.01705 |  0:01:12s\n",
      "epoch 43 | loss: 0.01383 | val_logits_ll: 0.01715 |  0:01:13s\n",
      "epoch 44 | loss: 0.01367 | val_logits_ll: 0.01726 |  0:01:15s\n",
      "epoch 45 | loss: 0.01354 | val_logits_ll: 0.01735 |  0:01:17s\n",
      "epoch 46 | loss: 0.01339 | val_logits_ll: 0.01748 |  0:01:18s\n",
      "epoch 47 | loss: 0.01324 | val_logits_ll: 0.01755 |  0:01:20s\n",
      "epoch 48 | loss: 0.01314 | val_logits_ll: 0.01766 |  0:01:21s\n",
      "epoch 49 | loss: 0.01301 | val_logits_ll: 0.01783 |  0:01:23s\n",
      "epoch 50 | loss: 0.01293 | val_logits_ll: 0.01795 |  0:01:25s\n",
      "epoch 51 | loss: 0.01278 | val_logits_ll: 0.01807 |  0:01:26s\n",
      "epoch 52 | loss: 0.01248 | val_logits_ll: 0.01807 |  0:01:28s\n",
      "epoch 53 | loss: 0.01239 | val_logits_ll: 0.01809 |  0:01:30s\n",
      "epoch 54 | loss: 0.01234 | val_logits_ll: 0.01811 |  0:01:32s\n",
      "epoch 55 | loss: 0.0123  | val_logits_ll: 0.01813 |  0:01:33s\n",
      "epoch 56 | loss: 0.01226 | val_logits_ll: 0.01817 |  0:01:35s\n",
      "epoch 57 | loss: 0.01221 | val_logits_ll: 0.01821 |  0:01:37s\n",
      "epoch 58 | loss: 0.01221 | val_logits_ll: 0.01823 |  0:01:38s\n",
      "epoch 59 | loss: 0.01217 | val_logits_ll: 0.01826 |  0:01:40s\n",
      "epoch 60 | loss: 0.01211 | val_logits_ll: 0.01826 |  0:01:41s\n",
      "epoch 61 | loss: 0.0121  | val_logits_ll: 0.01829 |  0:01:43s\n",
      "epoch 62 | loss: 0.01207 | val_logits_ll: 0.01832 |  0:01:45s\n",
      "epoch 63 | loss: 0.01204 | val_logits_ll: 0.01834 |  0:01:47s\n",
      "epoch 64 | loss: 0.01203 | val_logits_ll: 0.01833 |  0:01:48s\n",
      "epoch 65 | loss: 0.01204 | val_logits_ll: 0.01833 |  0:01:50s\n",
      "epoch 66 | loss: 0.01202 | val_logits_ll: 0.01833 |  0:01:52s\n",
      "epoch 67 | loss: 0.01204 | val_logits_ll: 0.01834 |  0:01:53s\n",
      "epoch 68 | loss: 0.01204 | val_logits_ll: 0.01835 |  0:01:55s\n",
      "epoch 69 | loss: 0.01201 | val_logits_ll: 0.01835 |  0:01:57s\n",
      "epoch 70 | loss: 0.01202 | val_logits_ll: 0.01834 |  0:01:58s\n",
      "epoch 71 | loss: 0.01199 | val_logits_ll: 0.01836 |  0:02:00s\n",
      "epoch 72 | loss: 0.012   | val_logits_ll: 0.01836 |  0:02:02s\n",
      "epoch 73 | loss: 0.01202 | val_logits_ll: 0.01836 |  0:02:03s\n",
      "epoch 74 | loss: 0.01199 | val_logits_ll: 0.01836 |  0:02:05s\n",
      "epoch 75 | loss: 0.01201 | val_logits_ll: 0.01836 |  0:02:07s\n",
      "epoch 76 | loss: 0.01202 | val_logits_ll: 0.01837 |  0:02:08s\n",
      "epoch 77 | loss: 0.012   | val_logits_ll: 0.01836 |  0:02:10s\n",
      "epoch 78 | loss: 0.01202 | val_logits_ll: 0.01836 |  0:02:11s\n",
      "epoch 79 | loss: 0.012   | val_logits_ll: 0.01838 |  0:02:13s\n",
      "epoch 80 | loss: 0.01201 | val_logits_ll: 0.01835 |  0:02:15s\n",
      "epoch 81 | loss: 0.012   | val_logits_ll: 0.01835 |  0:02:16s\n",
      "epoch 82 | loss: 0.012   | val_logits_ll: 0.01837 |  0:02:18s\n",
      "epoch 83 | loss: 0.01201 | val_logits_ll: 0.01835 |  0:02:20s\n",
      "epoch 84 | loss: 0.01199 | val_logits_ll: 0.01835 |  0:02:21s\n",
      "epoch 85 | loss: 0.01202 | val_logits_ll: 0.01836 |  0:02:23s\n",
      "epoch 86 | loss: 0.01203 | val_logits_ll: 0.01837 |  0:02:24s\n",
      "epoch 87 | loss: 0.01202 | val_logits_ll: 0.01837 |  0:02:26s\n",
      "epoch 88 | loss: 0.012   | val_logits_ll: 0.01835 |  0:02:27s\n",
      "epoch 89 | loss: 0.012   | val_logits_ll: 0.01837 |  0:02:29s\n",
      "epoch 90 | loss: 0.01204 | val_logits_ll: 0.01836 |  0:02:31s\n",
      "\n",
      "Early stopping occured at epoch 90 with best_epoch = 40 and best_val_logits_ll = 0.01691\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold0_0.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40304 | val_logits_ll: 0.10647 |  0:00:01s\n",
      "epoch 1  | loss: 0.03267 | val_logits_ll: 0.02712 |  0:00:03s\n",
      "epoch 2  | loss: 0.02464 | val_logits_ll: 0.02254 |  0:00:04s\n",
      "epoch 3  | loss: 0.02188 | val_logits_ll: 0.02107 |  0:00:06s\n",
      "epoch 4  | loss: 0.02102 | val_logits_ll: 0.02057 |  0:00:08s\n",
      "epoch 5  | loss: 0.02056 | val_logits_ll: 0.02031 |  0:00:09s\n",
      "epoch 6  | loss: 0.0202  | val_logits_ll: 0.02012 |  0:00:11s\n",
      "epoch 7  | loss: 0.01998 | val_logits_ll: 0.02004 |  0:00:13s\n",
      "epoch 8  | loss: 0.01976 | val_logits_ll: 0.01978 |  0:00:14s\n",
      "epoch 9  | loss: 0.01944 | val_logits_ll: 0.01962 |  0:00:16s\n",
      "epoch 10 | loss: 0.01917 | val_logits_ll: 0.02138 |  0:00:18s\n",
      "epoch 11 | loss: 0.01882 | val_logits_ll: 0.01941 |  0:00:20s\n",
      "epoch 12 | loss: 0.01844 | val_logits_ll: 0.01925 |  0:00:21s\n",
      "epoch 13 | loss: 0.01822 | val_logits_ll: 0.01858 |  0:00:23s\n",
      "epoch 14 | loss: 0.01793 | val_logits_ll: 0.02003 |  0:00:24s\n",
      "epoch 15 | loss: 0.01779 | val_logits_ll: 0.01853 |  0:00:26s\n",
      "epoch 16 | loss: 0.0176  | val_logits_ll: 0.01831 |  0:00:28s\n",
      "epoch 17 | loss: 0.01744 | val_logits_ll: 0.0181  |  0:00:30s\n",
      "epoch 18 | loss: 0.0173  | val_logits_ll: 0.01953 |  0:00:32s\n",
      "epoch 19 | loss: 0.0172  | val_logits_ll: 0.01815 |  0:00:33s\n",
      "epoch 20 | loss: 0.01709 | val_logits_ll: 0.01992 |  0:00:35s\n",
      "epoch 21 | loss: 0.01702 | val_logits_ll: 0.02093 |  0:00:36s\n",
      "epoch 22 | loss: 0.01687 | val_logits_ll: 0.01839 |  0:00:38s\n",
      "epoch 23 | loss: 0.01675 | val_logits_ll: 0.01787 |  0:00:40s\n",
      "epoch 24 | loss: 0.01687 | val_logits_ll: 0.01804 |  0:00:41s\n",
      "epoch 25 | loss: 0.01661 | val_logits_ll: 0.01759 |  0:00:43s\n",
      "epoch 26 | loss: 0.01648 | val_logits_ll: 0.02089 |  0:00:44s\n",
      "epoch 27 | loss: 0.01641 | val_logits_ll: 0.01884 |  0:00:46s\n",
      "epoch 28 | loss: 0.01636 | val_logits_ll: 0.0178  |  0:00:47s\n",
      "epoch 29 | loss: 0.01626 | val_logits_ll: 0.01973 |  0:00:49s\n",
      "epoch 30 | loss: 0.01633 | val_logits_ll: 0.01766 |  0:00:51s\n",
      "epoch 31 | loss: 0.01613 | val_logits_ll: 0.01763 |  0:00:52s\n",
      "epoch 32 | loss: 0.01598 | val_logits_ll: 0.02148 |  0:00:54s\n",
      "epoch 33 | loss: 0.01597 | val_logits_ll: 0.01752 |  0:00:56s\n",
      "epoch 34 | loss: 0.016   | val_logits_ll: 0.01811 |  0:00:57s\n",
      "epoch 35 | loss: 0.01585 | val_logits_ll: 0.01786 |  0:00:59s\n",
      "epoch 36 | loss: 0.01591 | val_logits_ll: 0.01743 |  0:01:01s\n",
      "epoch 37 | loss: 0.01588 | val_logits_ll: 0.01801 |  0:01:03s\n",
      "epoch 38 | loss: 0.01576 | val_logits_ll: 0.01838 |  0:01:04s\n",
      "epoch 39 | loss: 0.0158  | val_logits_ll: 0.01757 |  0:01:06s\n",
      "epoch 40 | loss: 0.0157  | val_logits_ll: 0.01759 |  0:01:08s\n",
      "epoch 41 | loss: 0.01566 | val_logits_ll: 0.0175  |  0:01:09s\n",
      "epoch 42 | loss: 0.0157  | val_logits_ll: 0.01756 |  0:01:11s\n",
      "epoch 43 | loss: 0.0156  | val_logits_ll: 0.01728 |  0:01:13s\n",
      "epoch 44 | loss: 0.01551 | val_logits_ll: 0.01758 |  0:01:14s\n",
      "epoch 45 | loss: 0.01559 | val_logits_ll: 0.01772 |  0:01:16s\n",
      "epoch 46 | loss: 0.01564 | val_logits_ll: 0.01768 |  0:01:18s\n",
      "epoch 47 | loss: 0.01557 | val_logits_ll: 0.01751 |  0:01:20s\n",
      "epoch 48 | loss: 0.01546 | val_logits_ll: 0.01765 |  0:01:21s\n",
      "epoch 49 | loss: 0.01539 | val_logits_ll: 0.01755 |  0:01:23s\n",
      "epoch 50 | loss: 0.01547 | val_logits_ll: 0.01761 |  0:01:24s\n",
      "epoch 51 | loss: 0.01544 | val_logits_ll: 0.01748 |  0:01:26s\n",
      "epoch 52 | loss: 0.01541 | val_logits_ll: 0.01751 |  0:01:28s\n",
      "epoch 53 | loss: 0.01539 | val_logits_ll: 0.01782 |  0:01:29s\n",
      "epoch 54 | loss: 0.01531 | val_logits_ll: 0.01755 |  0:01:31s\n",
      "epoch 55 | loss: 0.01475 | val_logits_ll: 0.01732 |  0:01:32s\n",
      "epoch 56 | loss: 0.01428 | val_logits_ll: 0.01735 |  0:01:34s\n",
      "epoch 57 | loss: 0.01404 | val_logits_ll: 0.01744 |  0:01:36s\n",
      "epoch 58 | loss: 0.01388 | val_logits_ll: 0.01751 |  0:01:38s\n",
      "epoch 59 | loss: 0.01371 | val_logits_ll: 0.01756 |  0:01:39s\n",
      "epoch 60 | loss: 0.01358 | val_logits_ll: 0.01774 |  0:01:41s\n",
      "epoch 61 | loss: 0.01345 | val_logits_ll: 0.0178  |  0:01:42s\n",
      "epoch 62 | loss: 0.01332 | val_logits_ll: 0.01794 |  0:01:44s\n",
      "epoch 63 | loss: 0.01317 | val_logits_ll: 0.01806 |  0:01:46s\n",
      "epoch 64 | loss: 0.01307 | val_logits_ll: 0.01814 |  0:01:47s\n",
      "epoch 65 | loss: 0.01291 | val_logits_ll: 0.01827 |  0:01:49s\n",
      "epoch 66 | loss: 0.01266 | val_logits_ll: 0.01831 |  0:01:50s\n",
      "epoch 67 | loss: 0.01255 | val_logits_ll: 0.01835 |  0:01:52s\n",
      "epoch 68 | loss: 0.01251 | val_logits_ll: 0.01834 |  0:01:54s\n",
      "epoch 69 | loss: 0.01247 | val_logits_ll: 0.01842 |  0:01:55s\n",
      "epoch 70 | loss: 0.01244 | val_logits_ll: 0.01842 |  0:01:57s\n",
      "epoch 71 | loss: 0.01241 | val_logits_ll: 0.01843 |  0:01:59s\n",
      "epoch 72 | loss: 0.01238 | val_logits_ll: 0.01846 |  0:02:00s\n",
      "epoch 73 | loss: 0.01235 | val_logits_ll: 0.01849 |  0:02:02s\n",
      "epoch 74 | loss: 0.01232 | val_logits_ll: 0.01851 |  0:02:03s\n",
      "epoch 75 | loss: 0.01229 | val_logits_ll: 0.01856 |  0:02:05s\n",
      "epoch 76 | loss: 0.01227 | val_logits_ll: 0.01858 |  0:02:07s\n",
      "epoch 77 | loss: 0.01224 | val_logits_ll: 0.01857 |  0:02:09s\n",
      "epoch 78 | loss: 0.01222 | val_logits_ll: 0.01859 |  0:02:11s\n",
      "epoch 79 | loss: 0.01224 | val_logits_ll: 0.01859 |  0:02:12s\n",
      "epoch 80 | loss: 0.01221 | val_logits_ll: 0.01859 |  0:02:14s\n",
      "epoch 81 | loss: 0.01221 | val_logits_ll: 0.01859 |  0:02:16s\n",
      "epoch 82 | loss: 0.01223 | val_logits_ll: 0.0186  |  0:02:18s\n",
      "epoch 83 | loss: 0.01221 | val_logits_ll: 0.01859 |  0:02:19s\n",
      "epoch 84 | loss: 0.01221 | val_logits_ll: 0.0186  |  0:02:21s\n",
      "epoch 85 | loss: 0.01219 | val_logits_ll: 0.01862 |  0:02:22s\n",
      "epoch 86 | loss: 0.01222 | val_logits_ll: 0.0186  |  0:02:24s\n",
      "epoch 87 | loss: 0.0122  | val_logits_ll: 0.01861 |  0:02:26s\n",
      "epoch 88 | loss: 0.01219 | val_logits_ll: 0.01861 |  0:02:27s\n",
      "epoch 89 | loss: 0.01219 | val_logits_ll: 0.01862 |  0:02:29s\n",
      "epoch 90 | loss: 0.0122  | val_logits_ll: 0.01862 |  0:02:31s\n",
      "epoch 91 | loss: 0.0122  | val_logits_ll: 0.0186  |  0:02:32s\n",
      "epoch 92 | loss: 0.01219 | val_logits_ll: 0.01862 |  0:02:34s\n",
      "epoch 93 | loss: 0.0122  | val_logits_ll: 0.01861 |  0:02:35s\n",
      "\n",
      "Early stopping occured at epoch 93 with best_epoch = 43 and best_val_logits_ll = 0.01728\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold1_0.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40183 | val_logits_ll: 0.10017 |  0:00:01s\n",
      "epoch 1  | loss: 0.0327  | val_logits_ll: 0.02705 |  0:00:03s\n",
      "epoch 2  | loss: 0.02397 | val_logits_ll: 0.02208 |  0:00:05s\n",
      "epoch 3  | loss: 0.0216  | val_logits_ll: 0.0212  |  0:00:06s\n",
      "epoch 4  | loss: 0.02084 | val_logits_ll: 0.0207  |  0:00:08s\n",
      "epoch 5  | loss: 0.02042 | val_logits_ll: 0.02039 |  0:00:09s\n",
      "epoch 6  | loss: 0.02002 | val_logits_ll: 0.02    |  0:00:11s\n",
      "epoch 7  | loss: 0.01962 | val_logits_ll: 0.02001 |  0:00:13s\n",
      "epoch 8  | loss: 0.0193  | val_logits_ll: 0.02075 |  0:00:14s\n",
      "epoch 9  | loss: 0.01897 | val_logits_ll: 0.01921 |  0:00:16s\n",
      "epoch 10 | loss: 0.01863 | val_logits_ll: 0.01909 |  0:00:17s\n",
      "epoch 11 | loss: 0.01832 | val_logits_ll: 0.01918 |  0:00:19s\n",
      "epoch 12 | loss: 0.018   | val_logits_ll: 0.01882 |  0:00:21s\n",
      "epoch 13 | loss: 0.01785 | val_logits_ll: 0.01873 |  0:00:22s\n",
      "epoch 14 | loss: 0.01757 | val_logits_ll: 0.01854 |  0:00:24s\n",
      "epoch 15 | loss: 0.01736 | val_logits_ll: 0.01839 |  0:00:26s\n",
      "epoch 16 | loss: 0.01731 | val_logits_ll: 0.0197  |  0:00:27s\n",
      "epoch 17 | loss: 0.01714 | val_logits_ll: 0.01897 |  0:00:29s\n",
      "epoch 18 | loss: 0.01697 | val_logits_ll: 0.01871 |  0:00:30s\n",
      "epoch 19 | loss: 0.01688 | val_logits_ll: 0.01832 |  0:00:32s\n",
      "epoch 20 | loss: 0.01679 | val_logits_ll: 0.01831 |  0:00:34s\n",
      "epoch 21 | loss: 0.01677 | val_logits_ll: 0.02037 |  0:00:36s\n",
      "epoch 22 | loss: 0.01674 | val_logits_ll: 0.01893 |  0:00:38s\n",
      "epoch 23 | loss: 0.01664 | val_logits_ll: 0.01843 |  0:00:40s\n",
      "epoch 24 | loss: 0.01653 | val_logits_ll: 0.02104 |  0:00:41s\n",
      "epoch 25 | loss: 0.01649 | val_logits_ll: 0.01936 |  0:00:43s\n",
      "epoch 26 | loss: 0.01647 | val_logits_ll: 0.01894 |  0:00:44s\n",
      "epoch 27 | loss: 0.01636 | val_logits_ll: 0.01826 |  0:00:46s\n",
      "epoch 28 | loss: 0.01623 | val_logits_ll: 0.01845 |  0:00:48s\n",
      "epoch 29 | loss: 0.01619 | val_logits_ll: 0.01845 |  0:00:49s\n",
      "epoch 30 | loss: 0.01618 | val_logits_ll: 0.01835 |  0:00:51s\n",
      "epoch 31 | loss: 0.01596 | val_logits_ll: 0.01905 |  0:00:53s\n",
      "epoch 32 | loss: 0.01598 | val_logits_ll: 0.01881 |  0:00:54s\n",
      "epoch 33 | loss: 0.01596 | val_logits_ll: 0.01858 |  0:00:56s\n",
      "epoch 34 | loss: 0.0158  | val_logits_ll: 0.01838 |  0:00:57s\n",
      "epoch 35 | loss: 0.01578 | val_logits_ll: 0.01836 |  0:00:59s\n",
      "epoch 36 | loss: 0.01578 | val_logits_ll: 0.02132 |  0:01:01s\n",
      "epoch 37 | loss: 0.01566 | val_logits_ll: 0.0183  |  0:01:02s\n",
      "epoch 38 | loss: 0.01553 | val_logits_ll: 0.01842 |  0:01:04s\n",
      "epoch 39 | loss: 0.01497 | val_logits_ll: 0.01806 |  0:01:06s\n",
      "epoch 40 | loss: 0.01451 | val_logits_ll: 0.0181  |  0:01:08s\n",
      "epoch 41 | loss: 0.01427 | val_logits_ll: 0.0182  |  0:01:09s\n",
      "epoch 42 | loss: 0.01407 | val_logits_ll: 0.01825 |  0:01:11s\n",
      "epoch 43 | loss: 0.01393 | val_logits_ll: 0.01834 |  0:01:12s\n",
      "epoch 44 | loss: 0.01377 | val_logits_ll: 0.01839 |  0:01:14s\n",
      "epoch 45 | loss: 0.01364 | val_logits_ll: 0.01852 |  0:01:16s\n",
      "epoch 46 | loss: 0.01349 | val_logits_ll: 0.01859 |  0:01:17s\n",
      "epoch 47 | loss: 0.01338 | val_logits_ll: 0.01868 |  0:01:19s\n",
      "epoch 48 | loss: 0.01325 | val_logits_ll: 0.01881 |  0:01:20s\n",
      "epoch 49 | loss: 0.01308 | val_logits_ll: 0.01888 |  0:01:22s\n",
      "epoch 50 | loss: 0.01297 | val_logits_ll: 0.01908 |  0:01:24s\n",
      "epoch 51 | loss: 0.01269 | val_logits_ll: 0.01906 |  0:01:25s\n",
      "epoch 52 | loss: 0.01256 | val_logits_ll: 0.0191  |  0:01:27s\n",
      "epoch 53 | loss: 0.0125  | val_logits_ll: 0.01909 |  0:01:28s\n",
      "epoch 54 | loss: 0.01246 | val_logits_ll: 0.01914 |  0:01:30s\n",
      "epoch 55 | loss: 0.01245 | val_logits_ll: 0.01916 |  0:01:32s\n",
      "epoch 56 | loss: 0.01238 | val_logits_ll: 0.0192  |  0:01:33s\n",
      "epoch 57 | loss: 0.01238 | val_logits_ll: 0.01921 |  0:01:35s\n",
      "epoch 58 | loss: 0.01233 | val_logits_ll: 0.01921 |  0:01:37s\n",
      "epoch 59 | loss: 0.0123  | val_logits_ll: 0.01927 |  0:01:39s\n",
      "epoch 60 | loss: 0.01228 | val_logits_ll: 0.0193  |  0:01:41s\n",
      "epoch 61 | loss: 0.01226 | val_logits_ll: 0.01933 |  0:01:43s\n",
      "epoch 62 | loss: 0.01219 | val_logits_ll: 0.01933 |  0:01:44s\n",
      "epoch 63 | loss: 0.01222 | val_logits_ll: 0.01933 |  0:01:46s\n",
      "epoch 64 | loss: 0.0122  | val_logits_ll: 0.01933 |  0:01:48s\n",
      "epoch 65 | loss: 0.01219 | val_logits_ll: 0.01935 |  0:01:49s\n",
      "epoch 66 | loss: 0.0122  | val_logits_ll: 0.01933 |  0:01:51s\n",
      "epoch 67 | loss: 0.01221 | val_logits_ll: 0.01934 |  0:01:52s\n",
      "epoch 68 | loss: 0.01217 | val_logits_ll: 0.01935 |  0:01:54s\n",
      "epoch 69 | loss: 0.01218 | val_logits_ll: 0.01935 |  0:01:56s\n",
      "epoch 70 | loss: 0.0122  | val_logits_ll: 0.01935 |  0:01:57s\n",
      "epoch 71 | loss: 0.01218 | val_logits_ll: 0.01936 |  0:01:59s\n",
      "epoch 72 | loss: 0.01217 | val_logits_ll: 0.01935 |  0:02:00s\n",
      "epoch 73 | loss: 0.01217 | val_logits_ll: 0.01936 |  0:02:02s\n",
      "epoch 74 | loss: 0.01217 | val_logits_ll: 0.01935 |  0:02:04s\n",
      "epoch 75 | loss: 0.01216 | val_logits_ll: 0.01937 |  0:02:05s\n",
      "epoch 76 | loss: 0.01217 | val_logits_ll: 0.01935 |  0:02:07s\n",
      "epoch 77 | loss: 0.01217 | val_logits_ll: 0.01936 |  0:02:09s\n",
      "epoch 78 | loss: 0.01217 | val_logits_ll: 0.01936 |  0:02:10s\n",
      "epoch 79 | loss: 0.01218 | val_logits_ll: 0.01936 |  0:02:12s\n",
      "epoch 80 | loss: 0.01219 | val_logits_ll: 0.01936 |  0:02:14s\n",
      "epoch 81 | loss: 0.01218 | val_logits_ll: 0.01937 |  0:02:15s\n",
      "epoch 82 | loss: 0.01217 | val_logits_ll: 0.01937 |  0:02:17s\n",
      "epoch 83 | loss: 0.01217 | val_logits_ll: 0.01936 |  0:02:18s\n",
      "epoch 84 | loss: 0.01217 | val_logits_ll: 0.01937 |  0:02:20s\n",
      "epoch 85 | loss: 0.01217 | val_logits_ll: 0.01936 |  0:02:21s\n",
      "epoch 86 | loss: 0.01218 | val_logits_ll: 0.01936 |  0:02:23s\n",
      "epoch 87 | loss: 0.01216 | val_logits_ll: 0.01936 |  0:02:25s\n",
      "epoch 88 | loss: 0.01217 | val_logits_ll: 0.01936 |  0:02:27s\n",
      "epoch 89 | loss: 0.01215 | val_logits_ll: 0.01937 |  0:02:28s\n",
      "\n",
      "Early stopping occured at epoch 89 with best_epoch = 39 and best_val_logits_ll = 0.01806\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold2_0.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.405   | val_logits_ll: 0.10002 |  0:00:01s\n",
      "epoch 1  | loss: 0.03243 | val_logits_ll: 0.02806 |  0:00:03s\n",
      "epoch 2  | loss: 0.02423 | val_logits_ll: 0.02302 |  0:00:05s\n",
      "epoch 3  | loss: 0.02141 | val_logits_ll: 0.02187 |  0:00:07s\n",
      "epoch 4  | loss: 0.02063 | val_logits_ll: 0.0215  |  0:00:08s\n",
      "epoch 5  | loss: 0.02022 | val_logits_ll: 0.02115 |  0:00:10s\n",
      "epoch 6  | loss: 0.01984 | val_logits_ll: 0.02082 |  0:00:12s\n",
      "epoch 7  | loss: 0.01948 | val_logits_ll: 0.02043 |  0:00:14s\n",
      "epoch 8  | loss: 0.01912 | val_logits_ll: 0.02017 |  0:00:15s\n",
      "epoch 9  | loss: 0.01878 | val_logits_ll: 0.01988 |  0:00:17s\n",
      "epoch 10 | loss: 0.01842 | val_logits_ll: 0.0197  |  0:00:19s\n",
      "epoch 11 | loss: 0.01804 | val_logits_ll: 0.01933 |  0:00:20s\n",
      "epoch 12 | loss: 0.0178  | val_logits_ll: 0.01933 |  0:00:22s\n",
      "epoch 13 | loss: 0.01761 | val_logits_ll: 0.01937 |  0:00:23s\n",
      "epoch 14 | loss: 0.01742 | val_logits_ll: 0.01907 |  0:00:25s\n",
      "epoch 15 | loss: 0.01728 | val_logits_ll: 0.01891 |  0:00:26s\n",
      "epoch 16 | loss: 0.01718 | val_logits_ll: 0.02072 |  0:00:28s\n",
      "epoch 17 | loss: 0.01708 | val_logits_ll: 0.01901 |  0:00:30s\n",
      "epoch 18 | loss: 0.01696 | val_logits_ll: 0.01925 |  0:00:31s\n",
      "epoch 19 | loss: 0.01673 | val_logits_ll: 0.01897 |  0:00:33s\n",
      "epoch 20 | loss: 0.01662 | val_logits_ll: 0.01915 |  0:00:35s\n",
      "epoch 21 | loss: 0.01651 | val_logits_ll: 0.02035 |  0:00:36s\n",
      "epoch 22 | loss: 0.0164  | val_logits_ll: 0.01877 |  0:00:38s\n",
      "epoch 23 | loss: 0.01634 | val_logits_ll: 0.01885 |  0:00:40s\n",
      "epoch 24 | loss: 0.01629 | val_logits_ll: 0.01994 |  0:00:42s\n",
      "epoch 25 | loss: 0.01615 | val_logits_ll: 0.01884 |  0:00:43s\n",
      "epoch 26 | loss: 0.01604 | val_logits_ll: 0.0188  |  0:00:45s\n",
      "epoch 27 | loss: 0.01602 | val_logits_ll: 0.0186  |  0:00:47s\n",
      "epoch 28 | loss: 0.01592 | val_logits_ll: 0.01931 |  0:00:48s\n",
      "epoch 29 | loss: 0.01584 | val_logits_ll: 0.01848 |  0:00:50s\n",
      "epoch 30 | loss: 0.0158  | val_logits_ll: 0.02151 |  0:00:51s\n",
      "epoch 31 | loss: 0.01579 | val_logits_ll: 0.01881 |  0:00:53s\n",
      "epoch 32 | loss: 0.0158  | val_logits_ll: 0.01849 |  0:00:55s\n",
      "epoch 33 | loss: 0.01563 | val_logits_ll: 0.01853 |  0:00:56s\n",
      "epoch 34 | loss: 0.01565 | val_logits_ll: 0.01874 |  0:00:58s\n",
      "epoch 35 | loss: 0.01545 | val_logits_ll: 0.01876 |  0:00:59s\n",
      "epoch 36 | loss: 0.01539 | val_logits_ll: 0.01864 |  0:01:01s\n",
      "epoch 37 | loss: 0.01546 | val_logits_ll: 0.02007 |  0:01:03s\n",
      "epoch 38 | loss: 0.0154  | val_logits_ll: 0.01871 |  0:01:04s\n",
      "epoch 39 | loss: 0.01536 | val_logits_ll: 0.01865 |  0:01:06s\n",
      "epoch 40 | loss: 0.01529 | val_logits_ll: 0.01882 |  0:01:08s\n",
      "epoch 41 | loss: 0.01469 | val_logits_ll: 0.0183  |  0:01:10s\n",
      "epoch 42 | loss: 0.01422 | val_logits_ll: 0.01836 |  0:01:11s\n",
      "epoch 43 | loss: 0.01397 | val_logits_ll: 0.01843 |  0:01:13s\n",
      "epoch 44 | loss: 0.01378 | val_logits_ll: 0.01857 |  0:01:15s\n",
      "epoch 45 | loss: 0.01366 | val_logits_ll: 0.01856 |  0:01:16s\n",
      "epoch 46 | loss: 0.01351 | val_logits_ll: 0.01871 |  0:01:18s\n",
      "epoch 47 | loss: 0.01337 | val_logits_ll: 0.01891 |  0:01:20s\n",
      "epoch 48 | loss: 0.01327 | val_logits_ll: 0.01891 |  0:01:21s\n",
      "epoch 49 | loss: 0.01315 | val_logits_ll: 0.01913 |  0:01:23s\n",
      "epoch 50 | loss: 0.01302 | val_logits_ll: 0.01919 |  0:01:25s\n",
      "epoch 51 | loss: 0.01288 | val_logits_ll: 0.01928 |  0:01:26s\n",
      "epoch 52 | loss: 0.0128  | val_logits_ll: 0.01948 |  0:01:28s\n",
      "epoch 53 | loss: 0.0125  | val_logits_ll: 0.01947 |  0:01:29s\n",
      "epoch 54 | loss: 0.01238 | val_logits_ll: 0.01949 |  0:01:31s\n",
      "epoch 55 | loss: 0.01235 | val_logits_ll: 0.01951 |  0:01:33s\n",
      "epoch 56 | loss: 0.01229 | val_logits_ll: 0.01955 |  0:01:34s\n",
      "epoch 57 | loss: 0.01226 | val_logits_ll: 0.01957 |  0:01:36s\n",
      "epoch 58 | loss: 0.01222 | val_logits_ll: 0.01963 |  0:01:38s\n",
      "epoch 59 | loss: 0.01222 | val_logits_ll: 0.01963 |  0:01:39s\n",
      "epoch 60 | loss: 0.01217 | val_logits_ll: 0.01967 |  0:01:41s\n",
      "epoch 61 | loss: 0.01214 | val_logits_ll: 0.0197  |  0:01:43s\n",
      "epoch 62 | loss: 0.0121  | val_logits_ll: 0.01973 |  0:01:44s\n",
      "epoch 63 | loss: 0.01209 | val_logits_ll: 0.01975 |  0:01:46s\n",
      "epoch 64 | loss: 0.01206 | val_logits_ll: 0.01975 |  0:01:48s\n",
      "epoch 65 | loss: 0.01204 | val_logits_ll: 0.01975 |  0:01:49s\n",
      "epoch 66 | loss: 0.01205 | val_logits_ll: 0.01977 |  0:01:51s\n",
      "epoch 67 | loss: 0.01204 | val_logits_ll: 0.01978 |  0:01:53s\n",
      "epoch 68 | loss: 0.01202 | val_logits_ll: 0.01976 |  0:01:54s\n",
      "epoch 69 | loss: 0.01202 | val_logits_ll: 0.01977 |  0:01:56s\n",
      "epoch 70 | loss: 0.01202 | val_logits_ll: 0.01977 |  0:01:58s\n",
      "epoch 71 | loss: 0.01202 | val_logits_ll: 0.01977 |  0:01:59s\n",
      "epoch 72 | loss: 0.01202 | val_logits_ll: 0.01979 |  0:02:01s\n",
      "epoch 73 | loss: 0.01202 | val_logits_ll: 0.01979 |  0:02:02s\n",
      "epoch 74 | loss: 0.01202 | val_logits_ll: 0.0198  |  0:02:04s\n",
      "epoch 75 | loss: 0.01201 | val_logits_ll: 0.01979 |  0:02:06s\n",
      "epoch 76 | loss: 0.012   | val_logits_ll: 0.01979 |  0:02:08s\n",
      "epoch 77 | loss: 0.01198 | val_logits_ll: 0.01979 |  0:02:09s\n",
      "epoch 78 | loss: 0.012   | val_logits_ll: 0.0198  |  0:02:11s\n",
      "epoch 79 | loss: 0.01201 | val_logits_ll: 0.01978 |  0:02:13s\n",
      "epoch 80 | loss: 0.01203 | val_logits_ll: 0.01979 |  0:02:14s\n",
      "epoch 81 | loss: 0.01199 | val_logits_ll: 0.01978 |  0:02:16s\n",
      "epoch 82 | loss: 0.012   | val_logits_ll: 0.01978 |  0:02:18s\n",
      "epoch 83 | loss: 0.01201 | val_logits_ll: 0.0198  |  0:02:19s\n",
      "epoch 84 | loss: 0.01199 | val_logits_ll: 0.0198  |  0:02:21s\n",
      "epoch 85 | loss: 0.01202 | val_logits_ll: 0.0198  |  0:02:22s\n",
      "epoch 86 | loss: 0.01202 | val_logits_ll: 0.0198  |  0:02:24s\n",
      "epoch 87 | loss: 0.012   | val_logits_ll: 0.0198  |  0:02:26s\n",
      "epoch 88 | loss: 0.01201 | val_logits_ll: 0.0198  |  0:02:27s\n",
      "epoch 89 | loss: 0.01201 | val_logits_ll: 0.0198  |  0:02:29s\n",
      "epoch 90 | loss: 0.01201 | val_logits_ll: 0.0198  |  0:02:31s\n",
      "epoch 91 | loss: 0.01201 | val_logits_ll: 0.0198  |  0:02:32s\n",
      "\n",
      "Early stopping occured at epoch 91 with best_epoch = 41 and best_val_logits_ll = 0.0183\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold3_0.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4002  | val_logits_ll: 0.09966 |  0:00:01s\n",
      "epoch 1  | loss: 0.03266 | val_logits_ll: 0.02723 |  0:00:03s\n",
      "epoch 2  | loss: 0.02448 | val_logits_ll: 0.02192 |  0:00:04s\n",
      "epoch 3  | loss: 0.02153 | val_logits_ll: 0.02086 |  0:00:06s\n",
      "epoch 4  | loss: 0.02074 | val_logits_ll: 0.02034 |  0:00:08s\n",
      "epoch 5  | loss: 0.02037 | val_logits_ll: 0.0201  |  0:00:09s\n",
      "epoch 6  | loss: 0.02008 | val_logits_ll: 0.0199  |  0:00:11s\n",
      "epoch 7  | loss: 0.01976 | val_logits_ll: 0.0206  |  0:00:13s\n",
      "epoch 8  | loss: 0.01947 | val_logits_ll: 0.01949 |  0:00:14s\n",
      "epoch 9  | loss: 0.0192  | val_logits_ll: 0.01926 |  0:00:16s\n",
      "epoch 10 | loss: 0.01895 | val_logits_ll: 0.0192  |  0:00:17s\n",
      "epoch 11 | loss: 0.01863 | val_logits_ll: 0.02039 |  0:00:19s\n",
      "epoch 12 | loss: 0.01839 | val_logits_ll: 0.02066 |  0:00:21s\n",
      "epoch 13 | loss: 0.01816 | val_logits_ll: 0.01869 |  0:00:22s\n",
      "epoch 14 | loss: 0.01788 | val_logits_ll: 0.01853 |  0:00:24s\n",
      "epoch 15 | loss: 0.01771 | val_logits_ll: 0.01938 |  0:00:26s\n",
      "epoch 16 | loss: 0.01745 | val_logits_ll: 0.01855 |  0:00:27s\n",
      "epoch 17 | loss: 0.01731 | val_logits_ll: 0.01821 |  0:00:29s\n",
      "epoch 18 | loss: 0.01726 | val_logits_ll: 0.01805 |  0:00:31s\n",
      "epoch 19 | loss: 0.01708 | val_logits_ll: 0.01796 |  0:00:33s\n",
      "epoch 20 | loss: 0.0169  | val_logits_ll: 0.01794 |  0:00:34s\n",
      "epoch 21 | loss: 0.01689 | val_logits_ll: 0.01783 |  0:00:36s\n",
      "epoch 22 | loss: 0.01681 | val_logits_ll: 0.02045 |  0:00:37s\n",
      "epoch 23 | loss: 0.01673 | val_logits_ll: 0.02024 |  0:00:40s\n",
      "epoch 24 | loss: 0.01654 | val_logits_ll: 0.01773 |  0:00:41s\n",
      "epoch 25 | loss: 0.01651 | val_logits_ll: 0.01981 |  0:00:43s\n",
      "epoch 26 | loss: 0.01639 | val_logits_ll: 0.01755 |  0:00:44s\n",
      "epoch 27 | loss: 0.01624 | val_logits_ll: 0.01908 |  0:00:46s\n",
      "epoch 28 | loss: 0.01619 | val_logits_ll: 0.01961 |  0:00:48s\n",
      "epoch 29 | loss: 0.01612 | val_logits_ll: 0.01809 |  0:00:49s\n",
      "epoch 30 | loss: 0.01611 | val_logits_ll: 0.01756 |  0:00:51s\n",
      "epoch 31 | loss: 0.01605 | val_logits_ll: 0.0177  |  0:00:53s\n",
      "epoch 32 | loss: 0.01592 | val_logits_ll: 0.01749 |  0:00:54s\n",
      "epoch 33 | loss: 0.01582 | val_logits_ll: 0.01751 |  0:00:56s\n",
      "epoch 34 | loss: 0.01584 | val_logits_ll: 0.01766 |  0:00:57s\n",
      "epoch 35 | loss: 0.01577 | val_logits_ll: 0.01752 |  0:00:59s\n",
      "epoch 36 | loss: 0.01575 | val_logits_ll: 0.01777 |  0:01:01s\n",
      "epoch 37 | loss: 0.01577 | val_logits_ll: 0.01743 |  0:01:02s\n",
      "epoch 38 | loss: 0.01566 | val_logits_ll: 0.01769 |  0:01:04s\n",
      "epoch 39 | loss: 0.01561 | val_logits_ll: 0.01763 |  0:01:06s\n",
      "epoch 40 | loss: 0.01558 | val_logits_ll: 0.01838 |  0:01:07s\n",
      "epoch 41 | loss: 0.01553 | val_logits_ll: 0.01741 |  0:01:09s\n",
      "epoch 42 | loss: 0.01541 | val_logits_ll: 0.01753 |  0:01:11s\n",
      "epoch 43 | loss: 0.01534 | val_logits_ll: 0.01732 |  0:01:12s\n",
      "epoch 44 | loss: 0.01538 | val_logits_ll: 0.01735 |  0:01:14s\n",
      "epoch 45 | loss: 0.01535 | val_logits_ll: 0.01766 |  0:01:15s\n",
      "epoch 46 | loss: 0.01534 | val_logits_ll: 0.01738 |  0:01:17s\n",
      "epoch 47 | loss: 0.01524 | val_logits_ll: 0.01782 |  0:01:19s\n",
      "epoch 48 | loss: 0.01534 | val_logits_ll: 0.01753 |  0:01:20s\n",
      "epoch 49 | loss: 0.01534 | val_logits_ll: 0.01759 |  0:01:22s\n",
      "epoch 50 | loss: 0.01513 | val_logits_ll: 0.0175  |  0:01:24s\n",
      "epoch 51 | loss: 0.0151  | val_logits_ll: 0.01779 |  0:01:25s\n",
      "epoch 52 | loss: 0.01521 | val_logits_ll: 0.01838 |  0:01:27s\n",
      "epoch 53 | loss: 0.01554 | val_logits_ll: 0.01758 |  0:01:29s\n",
      "epoch 54 | loss: 0.01525 | val_logits_ll: 0.01768 |  0:01:30s\n",
      "epoch 55 | loss: 0.01462 | val_logits_ll: 0.01727 |  0:01:32s\n",
      "epoch 56 | loss: 0.01411 | val_logits_ll: 0.01734 |  0:01:34s\n",
      "epoch 57 | loss: 0.01384 | val_logits_ll: 0.01741 |  0:01:36s\n",
      "epoch 58 | loss: 0.01367 | val_logits_ll: 0.0175  |  0:01:37s\n",
      "epoch 59 | loss: 0.01351 | val_logits_ll: 0.01761 |  0:01:39s\n",
      "epoch 60 | loss: 0.01336 | val_logits_ll: 0.01771 |  0:01:40s\n",
      "epoch 61 | loss: 0.01321 | val_logits_ll: 0.01782 |  0:01:42s\n",
      "epoch 62 | loss: 0.01309 | val_logits_ll: 0.01792 |  0:01:44s\n",
      "epoch 63 | loss: 0.01296 | val_logits_ll: 0.01806 |  0:01:46s\n",
      "epoch 64 | loss: 0.01283 | val_logits_ll: 0.01819 |  0:01:47s\n",
      "epoch 65 | loss: 0.01273 | val_logits_ll: 0.01837 |  0:01:49s\n",
      "epoch 66 | loss: 0.0126  | val_logits_ll: 0.01842 |  0:01:50s\n",
      "epoch 67 | loss: 0.0123  | val_logits_ll: 0.01837 |  0:01:52s\n",
      "epoch 68 | loss: 0.01217 | val_logits_ll: 0.01842 |  0:01:53s\n",
      "epoch 69 | loss: 0.01212 | val_logits_ll: 0.01848 |  0:01:55s\n",
      "epoch 70 | loss: 0.01208 | val_logits_ll: 0.0185  |  0:01:57s\n",
      "epoch 71 | loss: 0.01203 | val_logits_ll: 0.01853 |  0:01:58s\n",
      "epoch 72 | loss: 0.01201 | val_logits_ll: 0.01857 |  0:02:00s\n",
      "epoch 73 | loss: 0.012   | val_logits_ll: 0.01858 |  0:02:02s\n",
      "epoch 74 | loss: 0.01192 | val_logits_ll: 0.0186  |  0:02:03s\n",
      "epoch 75 | loss: 0.01191 | val_logits_ll: 0.01866 |  0:02:05s\n",
      "epoch 76 | loss: 0.01189 | val_logits_ll: 0.01865 |  0:02:07s\n",
      "epoch 77 | loss: 0.01184 | val_logits_ll: 0.01871 |  0:02:08s\n",
      "epoch 78 | loss: 0.01182 | val_logits_ll: 0.01871 |  0:02:10s\n",
      "epoch 79 | loss: 0.01183 | val_logits_ll: 0.01872 |  0:02:12s\n",
      "epoch 80 | loss: 0.01182 | val_logits_ll: 0.01871 |  0:02:13s\n",
      "epoch 81 | loss: 0.01181 | val_logits_ll: 0.01871 |  0:02:15s\n",
      "epoch 82 | loss: 0.01181 | val_logits_ll: 0.01871 |  0:02:17s\n",
      "epoch 83 | loss: 0.01179 | val_logits_ll: 0.01874 |  0:02:18s\n",
      "epoch 84 | loss: 0.0118  | val_logits_ll: 0.01872 |  0:02:20s\n",
      "epoch 85 | loss: 0.01178 | val_logits_ll: 0.01872 |  0:02:21s\n",
      "epoch 86 | loss: 0.01179 | val_logits_ll: 0.01872 |  0:02:23s\n",
      "epoch 87 | loss: 0.01178 | val_logits_ll: 0.01873 |  0:02:24s\n",
      "epoch 88 | loss: 0.01179 | val_logits_ll: 0.01874 |  0:02:26s\n",
      "epoch 89 | loss: 0.01179 | val_logits_ll: 0.01872 |  0:02:28s\n",
      "epoch 90 | loss: 0.01178 | val_logits_ll: 0.01873 |  0:02:30s\n",
      "epoch 91 | loss: 0.01177 | val_logits_ll: 0.01871 |  0:02:31s\n",
      "epoch 92 | loss: 0.01176 | val_logits_ll: 0.01871 |  0:02:33s\n",
      "epoch 93 | loss: 0.01176 | val_logits_ll: 0.01872 |  0:02:35s\n",
      "epoch 94 | loss: 0.01177 | val_logits_ll: 0.01873 |  0:02:37s\n",
      "epoch 95 | loss: 0.01178 | val_logits_ll: 0.01873 |  0:02:38s\n",
      "epoch 96 | loss: 0.01176 | val_logits_ll: 0.01872 |  0:02:40s\n",
      "epoch 97 | loss: 0.01177 | val_logits_ll: 0.01872 |  0:02:42s\n",
      "epoch 98 | loss: 0.01177 | val_logits_ll: 0.01873 |  0:02:43s\n",
      "epoch 99 | loss: 0.01178 | val_logits_ll: 0.01875 |  0:02:45s\n",
      "epoch 100| loss: 0.01176 | val_logits_ll: 0.01872 |  0:02:46s\n",
      "epoch 101| loss: 0.01177 | val_logits_ll: 0.01874 |  0:02:48s\n",
      "epoch 102| loss: 0.01178 | val_logits_ll: 0.01874 |  0:02:50s\n",
      "epoch 103| loss: 0.01176 | val_logits_ll: 0.01873 |  0:02:51s\n",
      "epoch 104| loss: 0.01176 | val_logits_ll: 0.01873 |  0:02:53s\n",
      "epoch 105| loss: 0.01177 | val_logits_ll: 0.01872 |  0:02:54s\n",
      "\n",
      "Early stopping occured at epoch 105 with best_epoch = 55 and best_val_logits_ll = 0.01727\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold4_0.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40129 | val_logits_ll: 0.09402 |  0:00:01s\n",
      "epoch 1  | loss: 0.03256 | val_logits_ll: 0.02759 |  0:00:03s\n",
      "epoch 2  | loss: 0.02436 | val_logits_ll: 0.02255 |  0:00:04s\n",
      "epoch 3  | loss: 0.02154 | val_logits_ll: 0.0214  |  0:00:06s\n",
      "epoch 4  | loss: 0.02072 | val_logits_ll: 0.02082 |  0:00:07s\n",
      "epoch 5  | loss: 0.02023 | val_logits_ll: 0.02052 |  0:00:09s\n",
      "epoch 6  | loss: 0.01981 | val_logits_ll: 0.02021 |  0:00:11s\n",
      "epoch 7  | loss: 0.01948 | val_logits_ll: 0.02026 |  0:00:13s\n",
      "epoch 8  | loss: 0.0191  | val_logits_ll: 0.01972 |  0:00:14s\n",
      "epoch 9  | loss: 0.01872 | val_logits_ll: 0.01953 |  0:00:16s\n",
      "epoch 10 | loss: 0.01836 | val_logits_ll: 0.02053 |  0:00:18s\n",
      "epoch 11 | loss: 0.01803 | val_logits_ll: 0.01919 |  0:00:19s\n",
      "epoch 12 | loss: 0.01799 | val_logits_ll: 0.01912 |  0:00:21s\n",
      "epoch 13 | loss: 0.01769 | val_logits_ll: 0.01893 |  0:00:22s\n",
      "epoch 14 | loss: 0.01749 | val_logits_ll: 0.01879 |  0:00:24s\n",
      "epoch 15 | loss: 0.01745 | val_logits_ll: 0.01881 |  0:00:26s\n",
      "epoch 16 | loss: 0.01735 | val_logits_ll: 0.01929 |  0:00:27s\n",
      "epoch 17 | loss: 0.01729 | val_logits_ll: 0.01866 |  0:00:29s\n",
      "epoch 18 | loss: 0.01718 | val_logits_ll: 0.01857 |  0:00:30s\n",
      "epoch 19 | loss: 0.01703 | val_logits_ll: 0.01847 |  0:00:32s\n",
      "epoch 20 | loss: 0.01697 | val_logits_ll: 0.01861 |  0:00:34s\n",
      "epoch 21 | loss: 0.01685 | val_logits_ll: 0.01873 |  0:00:35s\n",
      "epoch 22 | loss: 0.01669 | val_logits_ll: 0.01854 |  0:00:37s\n",
      "epoch 23 | loss: 0.01659 | val_logits_ll: 0.01857 |  0:00:39s\n",
      "epoch 24 | loss: 0.01657 | val_logits_ll: 0.01868 |  0:00:41s\n",
      "epoch 25 | loss: 0.01657 | val_logits_ll: 0.0187  |  0:00:42s\n",
      "epoch 26 | loss: 0.0165  | val_logits_ll: 0.01868 |  0:00:44s\n",
      "epoch 27 | loss: 0.0164  | val_logits_ll: 0.01887 |  0:00:46s\n",
      "epoch 28 | loss: 0.01623 | val_logits_ll: 0.01824 |  0:00:47s\n",
      "epoch 29 | loss: 0.01624 | val_logits_ll: 0.01854 |  0:00:49s\n",
      "epoch 30 | loss: 0.0161  | val_logits_ll: 0.01808 |  0:00:51s\n",
      "epoch 31 | loss: 0.01594 | val_logits_ll: 0.01865 |  0:00:52s\n",
      "epoch 32 | loss: 0.01591 | val_logits_ll: 0.01832 |  0:00:54s\n",
      "epoch 33 | loss: 0.01587 | val_logits_ll: 0.01927 |  0:00:55s\n",
      "epoch 34 | loss: 0.01589 | val_logits_ll: 0.01833 |  0:00:57s\n",
      "epoch 35 | loss: 0.01577 | val_logits_ll: 0.01865 |  0:00:59s\n",
      "epoch 36 | loss: 0.01564 | val_logits_ll: 0.01806 |  0:01:00s\n",
      "epoch 37 | loss: 0.01562 | val_logits_ll: 0.01818 |  0:01:02s\n",
      "epoch 38 | loss: 0.01556 | val_logits_ll: 0.01826 |  0:01:03s\n",
      "epoch 39 | loss: 0.01557 | val_logits_ll: 0.01815 |  0:01:05s\n",
      "epoch 40 | loss: 0.01545 | val_logits_ll: 0.01809 |  0:01:06s\n",
      "epoch 41 | loss: 0.01541 | val_logits_ll: 0.01839 |  0:01:08s\n",
      "epoch 42 | loss: 0.01551 | val_logits_ll: 0.01826 |  0:01:10s\n",
      "epoch 43 | loss: 0.01536 | val_logits_ll: 0.0182  |  0:01:11s\n",
      "epoch 44 | loss: 0.01538 | val_logits_ll: 0.0182  |  0:01:13s\n",
      "epoch 45 | loss: 0.01538 | val_logits_ll: 0.01851 |  0:01:15s\n",
      "epoch 46 | loss: 0.01531 | val_logits_ll: 0.01834 |  0:01:16s\n",
      "epoch 47 | loss: 0.01526 | val_logits_ll: 0.01835 |  0:01:18s\n",
      "epoch 48 | loss: 0.01459 | val_logits_ll: 0.01797 |  0:01:19s\n",
      "epoch 49 | loss: 0.01413 | val_logits_ll: 0.018   |  0:01:21s\n",
      "epoch 50 | loss: 0.01388 | val_logits_ll: 0.01812 |  0:01:23s\n",
      "epoch 51 | loss: 0.01371 | val_logits_ll: 0.01823 |  0:01:25s\n",
      "epoch 52 | loss: 0.01352 | val_logits_ll: 0.01829 |  0:01:26s\n",
      "epoch 53 | loss: 0.01337 | val_logits_ll: 0.01842 |  0:01:28s\n",
      "epoch 54 | loss: 0.01321 | val_logits_ll: 0.01858 |  0:01:29s\n",
      "epoch 55 | loss: 0.01314 | val_logits_ll: 0.01862 |  0:01:31s\n",
      "epoch 56 | loss: 0.01298 | val_logits_ll: 0.0188  |  0:01:33s\n",
      "epoch 57 | loss: 0.01287 | val_logits_ll: 0.01889 |  0:01:35s\n",
      "epoch 58 | loss: 0.01272 | val_logits_ll: 0.01913 |  0:01:36s\n",
      "epoch 59 | loss: 0.01263 | val_logits_ll: 0.01911 |  0:01:38s\n",
      "epoch 60 | loss: 0.01233 | val_logits_ll: 0.01915 |  0:01:39s\n",
      "epoch 61 | loss: 0.01222 | val_logits_ll: 0.0192  |  0:01:41s\n",
      "epoch 62 | loss: 0.01216 | val_logits_ll: 0.01924 |  0:01:43s\n",
      "epoch 63 | loss: 0.01212 | val_logits_ll: 0.01926 |  0:01:45s\n",
      "epoch 64 | loss: 0.01209 | val_logits_ll: 0.01931 |  0:01:46s\n",
      "epoch 65 | loss: 0.01208 | val_logits_ll: 0.01935 |  0:01:48s\n",
      "epoch 66 | loss: 0.01204 | val_logits_ll: 0.01936 |  0:01:50s\n",
      "epoch 67 | loss: 0.01198 | val_logits_ll: 0.01937 |  0:01:51s\n",
      "epoch 68 | loss: 0.01199 | val_logits_ll: 0.01941 |  0:01:53s\n",
      "epoch 69 | loss: 0.01194 | val_logits_ll: 0.01942 |  0:01:55s\n",
      "epoch 70 | loss: 0.01191 | val_logits_ll: 0.01948 |  0:01:57s\n",
      "epoch 71 | loss: 0.01187 | val_logits_ll: 0.01946 |  0:01:58s\n",
      "epoch 72 | loss: 0.01187 | val_logits_ll: 0.01945 |  0:02:00s\n",
      "epoch 73 | loss: 0.01188 | val_logits_ll: 0.01948 |  0:02:01s\n",
      "epoch 74 | loss: 0.01186 | val_logits_ll: 0.01947 |  0:02:03s\n",
      "epoch 75 | loss: 0.01187 | val_logits_ll: 0.0195  |  0:02:05s\n",
      "epoch 76 | loss: 0.01185 | val_logits_ll: 0.01949 |  0:02:06s\n",
      "epoch 77 | loss: 0.01185 | val_logits_ll: 0.01947 |  0:02:08s\n",
      "epoch 78 | loss: 0.01185 | val_logits_ll: 0.01948 |  0:02:09s\n",
      "epoch 79 | loss: 0.01185 | val_logits_ll: 0.01949 |  0:02:11s\n",
      "epoch 80 | loss: 0.01184 | val_logits_ll: 0.0195  |  0:02:12s\n",
      "epoch 81 | loss: 0.01184 | val_logits_ll: 0.0195  |  0:02:14s\n",
      "epoch 82 | loss: 0.01182 | val_logits_ll: 0.01949 |  0:02:16s\n",
      "epoch 83 | loss: 0.01185 | val_logits_ll: 0.01948 |  0:02:17s\n",
      "epoch 84 | loss: 0.01182 | val_logits_ll: 0.01951 |  0:02:19s\n",
      "epoch 85 | loss: 0.01183 | val_logits_ll: 0.01949 |  0:02:20s\n",
      "epoch 86 | loss: 0.01185 | val_logits_ll: 0.01949 |  0:02:22s\n",
      "epoch 87 | loss: 0.01184 | val_logits_ll: 0.0195  |  0:02:24s\n",
      "epoch 88 | loss: 0.01183 | val_logits_ll: 0.0195  |  0:02:26s\n",
      "epoch 89 | loss: 0.01183 | val_logits_ll: 0.0195  |  0:02:28s\n",
      "epoch 90 | loss: 0.01182 | val_logits_ll: 0.01949 |  0:02:29s\n",
      "epoch 91 | loss: 0.01184 | val_logits_ll: 0.01949 |  0:02:31s\n",
      "epoch 92 | loss: 0.01183 | val_logits_ll: 0.0195  |  0:02:33s\n",
      "epoch 93 | loss: 0.01184 | val_logits_ll: 0.01949 |  0:02:34s\n",
      "epoch 94 | loss: 0.01183 | val_logits_ll: 0.0195  |  0:02:36s\n",
      "epoch 95 | loss: 0.01183 | val_logits_ll: 0.0195  |  0:02:38s\n",
      "epoch 96 | loss: 0.01184 | val_logits_ll: 0.0195  |  0:02:39s\n",
      "epoch 97 | loss: 0.01182 | val_logits_ll: 0.01949 |  0:02:41s\n",
      "epoch 98 | loss: 0.01184 | val_logits_ll: 0.0195  |  0:02:43s\n",
      "\n",
      "Early stopping occured at epoch 98 with best_epoch = 48 and best_val_logits_ll = 0.01797\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold5_0.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40405 | val_logits_ll: 0.10761 |  0:00:01s\n",
      "epoch 1  | loss: 0.03333 | val_logits_ll: 0.02702 |  0:00:03s\n",
      "epoch 2  | loss: 0.02437 | val_logits_ll: 0.02329 |  0:00:04s\n",
      "epoch 3  | loss: 0.02194 | val_logits_ll: 0.0218  |  0:00:06s\n",
      "epoch 4  | loss: 0.02097 | val_logits_ll: 0.02113 |  0:00:08s\n",
      "epoch 5  | loss: 0.02052 | val_logits_ll: 0.02081 |  0:00:09s\n",
      "epoch 6  | loss: 0.02007 | val_logits_ll: 0.0205  |  0:00:11s\n",
      "epoch 7  | loss: 0.01964 | val_logits_ll: 0.02028 |  0:00:13s\n",
      "epoch 8  | loss: 0.01927 | val_logits_ll: 0.02007 |  0:00:15s\n",
      "epoch 9  | loss: 0.01893 | val_logits_ll: 0.01965 |  0:00:17s\n",
      "epoch 10 | loss: 0.01849 | val_logits_ll: 0.01934 |  0:00:18s\n",
      "epoch 11 | loss: 0.01813 | val_logits_ll: 0.01983 |  0:00:20s\n",
      "epoch 12 | loss: 0.01786 | val_logits_ll: 0.01977 |  0:00:21s\n",
      "epoch 13 | loss: 0.01775 | val_logits_ll: 0.01996 |  0:00:23s\n",
      "epoch 14 | loss: 0.01749 | val_logits_ll: 0.02076 |  0:00:24s\n",
      "epoch 15 | loss: 0.01734 | val_logits_ll: 0.0189  |  0:00:26s\n",
      "epoch 16 | loss: 0.0172  | val_logits_ll: 0.0189  |  0:00:28s\n",
      "epoch 17 | loss: 0.01705 | val_logits_ll: 0.01857 |  0:00:29s\n",
      "epoch 18 | loss: 0.017   | val_logits_ll: 0.01936 |  0:00:31s\n",
      "epoch 19 | loss: 0.01688 | val_logits_ll: 0.01895 |  0:00:32s\n",
      "epoch 20 | loss: 0.01675 | val_logits_ll: 0.01872 |  0:00:34s\n",
      "epoch 21 | loss: 0.01676 | val_logits_ll: 0.01857 |  0:00:35s\n",
      "epoch 22 | loss: 0.01661 | val_logits_ll: 0.01909 |  0:00:37s\n",
      "epoch 23 | loss: 0.01652 | val_logits_ll: 0.02057 |  0:00:39s\n",
      "epoch 24 | loss: 0.01638 | val_logits_ll: 0.01842 |  0:00:40s\n",
      "epoch 25 | loss: 0.01627 | val_logits_ll: 0.01822 |  0:00:42s\n",
      "epoch 26 | loss: 0.01622 | val_logits_ll: 0.0188  |  0:00:44s\n",
      "epoch 27 | loss: 0.01626 | val_logits_ll: 0.01829 |  0:00:46s\n",
      "epoch 28 | loss: 0.0161  | val_logits_ll: 0.01833 |  0:00:48s\n",
      "epoch 29 | loss: 0.01613 | val_logits_ll: 0.01827 |  0:00:49s\n",
      "epoch 30 | loss: 0.01598 | val_logits_ll: 0.01812 |  0:00:51s\n",
      "epoch 31 | loss: 0.01584 | val_logits_ll: 0.01827 |  0:00:53s\n",
      "epoch 32 | loss: 0.01578 | val_logits_ll: 0.01889 |  0:00:54s\n",
      "epoch 33 | loss: 0.01595 | val_logits_ll: 0.01822 |  0:00:56s\n",
      "epoch 34 | loss: 0.01574 | val_logits_ll: 0.01804 |  0:00:58s\n",
      "epoch 35 | loss: 0.01565 | val_logits_ll: 0.01796 |  0:00:59s\n",
      "epoch 36 | loss: 0.01559 | val_logits_ll: 0.01806 |  0:01:01s\n",
      "epoch 37 | loss: 0.01559 | val_logits_ll: 0.01809 |  0:01:03s\n",
      "epoch 38 | loss: 0.01557 | val_logits_ll: 0.01801 |  0:01:04s\n",
      "epoch 39 | loss: 0.01544 | val_logits_ll: 0.01786 |  0:01:06s\n",
      "epoch 40 | loss: 0.01536 | val_logits_ll: 0.01865 |  0:01:07s\n",
      "epoch 41 | loss: 0.01538 | val_logits_ll: 0.01798 |  0:01:09s\n",
      "epoch 42 | loss: 0.01541 | val_logits_ll: 0.01825 |  0:01:11s\n",
      "epoch 43 | loss: 0.01533 | val_logits_ll: 0.01815 |  0:01:12s\n",
      "epoch 44 | loss: 0.01531 | val_logits_ll: 0.01818 |  0:01:14s\n",
      "epoch 45 | loss: 0.0153  | val_logits_ll: 0.01822 |  0:01:16s\n",
      "epoch 46 | loss: 0.01522 | val_logits_ll: 0.01906 |  0:01:18s\n",
      "epoch 47 | loss: 0.01535 | val_logits_ll: 0.01834 |  0:01:19s\n",
      "epoch 48 | loss: 0.01517 | val_logits_ll: 0.0187  |  0:01:21s\n",
      "epoch 49 | loss: 0.01516 | val_logits_ll: 0.01813 |  0:01:23s\n",
      "epoch 50 | loss: 0.01511 | val_logits_ll: 0.01827 |  0:01:24s\n",
      "epoch 51 | loss: 0.01449 | val_logits_ll: 0.01787 |  0:01:26s\n",
      "epoch 52 | loss: 0.01399 | val_logits_ll: 0.0179  |  0:01:27s\n",
      "epoch 53 | loss: 0.01374 | val_logits_ll: 0.01795 |  0:01:29s\n",
      "epoch 54 | loss: 0.01356 | val_logits_ll: 0.01805 |  0:01:31s\n",
      "epoch 55 | loss: 0.01338 | val_logits_ll: 0.01815 |  0:01:32s\n",
      "epoch 56 | loss: 0.01325 | val_logits_ll: 0.01824 |  0:01:34s\n",
      "epoch 57 | loss: 0.01311 | val_logits_ll: 0.01835 |  0:01:36s\n",
      "epoch 58 | loss: 0.01296 | val_logits_ll: 0.01847 |  0:01:37s\n",
      "epoch 59 | loss: 0.01285 | val_logits_ll: 0.01854 |  0:01:39s\n",
      "epoch 60 | loss: 0.01272 | val_logits_ll: 0.01867 |  0:01:40s\n",
      "epoch 61 | loss: 0.01257 | val_logits_ll: 0.01883 |  0:01:42s\n",
      "epoch 62 | loss: 0.0123  | val_logits_ll: 0.01878 |  0:01:44s\n",
      "epoch 63 | loss: 0.0122  | val_logits_ll: 0.01884 |  0:01:45s\n",
      "epoch 64 | loss: 0.01215 | val_logits_ll: 0.01886 |  0:01:47s\n",
      "epoch 65 | loss: 0.01211 | val_logits_ll: 0.01886 |  0:01:49s\n",
      "epoch 66 | loss: 0.01207 | val_logits_ll: 0.01889 |  0:01:51s\n",
      "epoch 67 | loss: 0.01205 | val_logits_ll: 0.01892 |  0:01:53s\n",
      "epoch 68 | loss: 0.01202 | val_logits_ll: 0.01896 |  0:01:55s\n",
      "epoch 69 | loss: 0.01197 | val_logits_ll: 0.01899 |  0:01:56s\n",
      "epoch 70 | loss: 0.01197 | val_logits_ll: 0.019   |  0:01:58s\n",
      "epoch 71 | loss: 0.01193 | val_logits_ll: 0.01905 |  0:02:00s\n",
      "epoch 72 | loss: 0.01192 | val_logits_ll: 0.01906 |  0:02:01s\n",
      "epoch 73 | loss: 0.01188 | val_logits_ll: 0.01906 |  0:02:03s\n",
      "epoch 74 | loss: 0.01187 | val_logits_ll: 0.01906 |  0:02:05s\n",
      "epoch 75 | loss: 0.01187 | val_logits_ll: 0.01906 |  0:02:06s\n",
      "epoch 76 | loss: 0.01187 | val_logits_ll: 0.01907 |  0:02:08s\n",
      "epoch 77 | loss: 0.01187 | val_logits_ll: 0.01908 |  0:02:09s\n",
      "epoch 78 | loss: 0.01185 | val_logits_ll: 0.01907 |  0:02:11s\n",
      "epoch 79 | loss: 0.01185 | val_logits_ll: 0.01907 |  0:02:13s\n",
      "epoch 80 | loss: 0.01184 | val_logits_ll: 0.01908 |  0:02:14s\n",
      "epoch 81 | loss: 0.01185 | val_logits_ll: 0.01908 |  0:02:16s\n",
      "epoch 82 | loss: 0.01185 | val_logits_ll: 0.01909 |  0:02:17s\n",
      "epoch 83 | loss: 0.01184 | val_logits_ll: 0.01909 |  0:02:19s\n",
      "epoch 84 | loss: 0.01185 | val_logits_ll: 0.01909 |  0:02:21s\n",
      "epoch 85 | loss: 0.01185 | val_logits_ll: 0.01909 |  0:02:23s\n",
      "epoch 86 | loss: 0.01185 | val_logits_ll: 0.01909 |  0:02:24s\n",
      "epoch 87 | loss: 0.01183 | val_logits_ll: 0.01909 |  0:02:26s\n",
      "epoch 88 | loss: 0.01183 | val_logits_ll: 0.01908 |  0:02:28s\n",
      "epoch 89 | loss: 0.01184 | val_logits_ll: 0.01909 |  0:02:29s\n",
      "\n",
      "Early stopping occured at epoch 89 with best_epoch = 39 and best_val_logits_ll = 0.01786\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold6_0.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4023  | val_logits_ll: 0.09708 |  0:00:01s\n",
      "epoch 1  | loss: 0.03248 | val_logits_ll: 0.02743 |  0:00:03s\n",
      "epoch 2  | loss: 0.02401 | val_logits_ll: 0.02212 |  0:00:04s\n",
      "epoch 3  | loss: 0.02155 | val_logits_ll: 0.02122 |  0:00:06s\n",
      "epoch 4  | loss: 0.02079 | val_logits_ll: 0.0208  |  0:00:08s\n",
      "epoch 5  | loss: 0.02035 | val_logits_ll: 0.0205  |  0:00:09s\n",
      "epoch 6  | loss: 0.02002 | val_logits_ll: 0.02121 |  0:00:11s\n",
      "epoch 7  | loss: 0.01978 | val_logits_ll: 0.02012 |  0:00:12s\n",
      "epoch 8  | loss: 0.01952 | val_logits_ll: 0.01995 |  0:00:14s\n",
      "epoch 9  | loss: 0.01921 | val_logits_ll: 0.01993 |  0:00:16s\n",
      "epoch 10 | loss: 0.01878 | val_logits_ll: 0.01957 |  0:00:18s\n",
      "epoch 11 | loss: 0.01844 | val_logits_ll: 0.01924 |  0:00:20s\n",
      "epoch 12 | loss: 0.01819 | val_logits_ll: 0.01892 |  0:00:22s\n",
      "epoch 13 | loss: 0.01794 | val_logits_ll: 0.01879 |  0:00:23s\n",
      "epoch 14 | loss: 0.01772 | val_logits_ll: 0.01898 |  0:00:25s\n",
      "epoch 15 | loss: 0.01752 | val_logits_ll: 0.01894 |  0:00:27s\n",
      "epoch 16 | loss: 0.01744 | val_logits_ll: 0.01914 |  0:00:28s\n",
      "epoch 17 | loss: 0.01738 | val_logits_ll: 0.01884 |  0:00:30s\n",
      "epoch 18 | loss: 0.01718 | val_logits_ll: 0.01842 |  0:00:32s\n",
      "epoch 19 | loss: 0.01705 | val_logits_ll: 0.01967 |  0:00:33s\n",
      "epoch 20 | loss: 0.01699 | val_logits_ll: 0.01836 |  0:00:35s\n",
      "epoch 21 | loss: 0.01684 | val_logits_ll: 0.01843 |  0:00:36s\n",
      "epoch 22 | loss: 0.01685 | val_logits_ll: 0.01892 |  0:00:38s\n",
      "epoch 23 | loss: 0.01668 | val_logits_ll: 0.02059 |  0:00:40s\n",
      "epoch 24 | loss: 0.01652 | val_logits_ll: 0.01858 |  0:00:41s\n",
      "epoch 25 | loss: 0.01638 | val_logits_ll: 0.01861 |  0:00:43s\n",
      "epoch 26 | loss: 0.01643 | val_logits_ll: 0.01886 |  0:00:44s\n",
      "epoch 27 | loss: 0.01639 | val_logits_ll: 0.01866 |  0:00:46s\n",
      "epoch 28 | loss: 0.01619 | val_logits_ll: 0.01826 |  0:00:47s\n",
      "epoch 29 | loss: 0.01619 | val_logits_ll: 0.01851 |  0:00:49s\n",
      "epoch 30 | loss: 0.01617 | val_logits_ll: 0.01814 |  0:00:51s\n",
      "epoch 31 | loss: 0.01606 | val_logits_ll: 0.01799 |  0:00:53s\n",
      "epoch 32 | loss: 0.01597 | val_logits_ll: 0.01798 |  0:00:54s\n",
      "epoch 33 | loss: 0.01588 | val_logits_ll: 0.01775 |  0:00:56s\n",
      "epoch 34 | loss: 0.01594 | val_logits_ll: 0.01783 |  0:00:58s\n",
      "epoch 35 | loss: 0.01583 | val_logits_ll: 0.01812 |  0:00:59s\n",
      "epoch 36 | loss: 0.01574 | val_logits_ll: 0.01775 |  0:01:01s\n",
      "epoch 37 | loss: 0.01572 | val_logits_ll: 0.01786 |  0:01:03s\n",
      "epoch 38 | loss: 0.01568 | val_logits_ll: 0.01784 |  0:01:04s\n",
      "epoch 39 | loss: 0.01556 | val_logits_ll: 0.01815 |  0:01:06s\n",
      "epoch 40 | loss: 0.01563 | val_logits_ll: 0.01785 |  0:01:08s\n",
      "epoch 41 | loss: 0.01562 | val_logits_ll: 0.01786 |  0:01:09s\n",
      "epoch 42 | loss: 0.01541 | val_logits_ll: 0.01785 |  0:01:11s\n",
      "epoch 43 | loss: 0.01559 | val_logits_ll: 0.01796 |  0:01:12s\n",
      "epoch 44 | loss: 0.01565 | val_logits_ll: 0.01782 |  0:01:14s\n",
      "epoch 45 | loss: 0.01497 | val_logits_ll: 0.01748 |  0:01:16s\n",
      "epoch 46 | loss: 0.01451 | val_logits_ll: 0.01747 |  0:01:18s\n",
      "epoch 47 | loss: 0.01425 | val_logits_ll: 0.01758 |  0:01:19s\n",
      "epoch 48 | loss: 0.01406 | val_logits_ll: 0.01766 |  0:01:21s\n",
      "epoch 49 | loss: 0.0139  | val_logits_ll: 0.01775 |  0:01:23s\n",
      "epoch 50 | loss: 0.01377 | val_logits_ll: 0.01782 |  0:01:25s\n",
      "epoch 51 | loss: 0.01361 | val_logits_ll: 0.01798 |  0:01:27s\n",
      "epoch 52 | loss: 0.01346 | val_logits_ll: 0.01799 |  0:01:28s\n",
      "epoch 53 | loss: 0.01334 | val_logits_ll: 0.01812 |  0:01:30s\n",
      "epoch 54 | loss: 0.01322 | val_logits_ll: 0.01821 |  0:01:32s\n",
      "epoch 55 | loss: 0.01313 | val_logits_ll: 0.01839 |  0:01:34s\n",
      "epoch 56 | loss: 0.01301 | val_logits_ll: 0.01845 |  0:01:35s\n",
      "epoch 57 | loss: 0.01289 | val_logits_ll: 0.0186  |  0:01:37s\n",
      "epoch 58 | loss: 0.01258 | val_logits_ll: 0.01863 |  0:01:38s\n",
      "epoch 59 | loss: 0.01243 | val_logits_ll: 0.01862 |  0:01:40s\n",
      "epoch 60 | loss: 0.01238 | val_logits_ll: 0.01869 |  0:01:42s\n",
      "epoch 61 | loss: 0.01236 | val_logits_ll: 0.0187  |  0:01:43s\n",
      "epoch 62 | loss: 0.01231 | val_logits_ll: 0.01876 |  0:01:45s\n",
      "epoch 63 | loss: 0.01229 | val_logits_ll: 0.01877 |  0:01:47s\n",
      "epoch 64 | loss: 0.01226 | val_logits_ll: 0.0188  |  0:01:48s\n",
      "epoch 65 | loss: 0.01224 | val_logits_ll: 0.01882 |  0:01:50s\n",
      "epoch 66 | loss: 0.0122  | val_logits_ll: 0.01883 |  0:01:51s\n",
      "epoch 67 | loss: 0.01217 | val_logits_ll: 0.01888 |  0:01:53s\n",
      "epoch 68 | loss: 0.01214 | val_logits_ll: 0.0189  |  0:01:54s\n",
      "epoch 69 | loss: 0.01211 | val_logits_ll: 0.01893 |  0:01:57s\n",
      "epoch 70 | loss: 0.0121  | val_logits_ll: 0.01892 |  0:01:58s\n",
      "epoch 71 | loss: 0.01208 | val_logits_ll: 0.01894 |  0:02:00s\n",
      "epoch 72 | loss: 0.01209 | val_logits_ll: 0.01894 |  0:02:01s\n",
      "epoch 73 | loss: 0.01207 | val_logits_ll: 0.01893 |  0:02:03s\n",
      "epoch 74 | loss: 0.01209 | val_logits_ll: 0.01894 |  0:02:05s\n",
      "epoch 75 | loss: 0.0121  | val_logits_ll: 0.01894 |  0:02:06s\n",
      "epoch 76 | loss: 0.01207 | val_logits_ll: 0.01893 |  0:02:08s\n",
      "epoch 77 | loss: 0.01207 | val_logits_ll: 0.01894 |  0:02:10s\n",
      "epoch 78 | loss: 0.01206 | val_logits_ll: 0.01894 |  0:02:11s\n",
      "epoch 79 | loss: 0.01205 | val_logits_ll: 0.01895 |  0:02:13s\n",
      "epoch 80 | loss: 0.01208 | val_logits_ll: 0.01896 |  0:02:14s\n",
      "epoch 81 | loss: 0.01207 | val_logits_ll: 0.01896 |  0:02:16s\n",
      "epoch 82 | loss: 0.01207 | val_logits_ll: 0.01896 |  0:02:18s\n",
      "epoch 83 | loss: 0.01206 | val_logits_ll: 0.01896 |  0:02:20s\n",
      "epoch 84 | loss: 0.01205 | val_logits_ll: 0.01894 |  0:02:21s\n",
      "epoch 85 | loss: 0.01206 | val_logits_ll: 0.01896 |  0:02:23s\n",
      "epoch 86 | loss: 0.01205 | val_logits_ll: 0.01896 |  0:02:25s\n",
      "epoch 87 | loss: 0.01206 | val_logits_ll: 0.01895 |  0:02:26s\n",
      "epoch 88 | loss: 0.01206 | val_logits_ll: 0.01896 |  0:02:28s\n",
      "epoch 89 | loss: 0.01208 | val_logits_ll: 0.01896 |  0:02:30s\n",
      "epoch 90 | loss: 0.01205 | val_logits_ll: 0.01895 |  0:02:32s\n",
      "epoch 91 | loss: 0.01207 | val_logits_ll: 0.01897 |  0:02:33s\n",
      "epoch 92 | loss: 0.01207 | val_logits_ll: 0.01896 |  0:02:35s\n",
      "epoch 93 | loss: 0.01206 | val_logits_ll: 0.01896 |  0:02:37s\n",
      "epoch 94 | loss: 0.01206 | val_logits_ll: 0.01896 |  0:02:38s\n",
      "epoch 95 | loss: 0.01206 | val_logits_ll: 0.01894 |  0:02:40s\n",
      "epoch 96 | loss: 0.01206 | val_logits_ll: 0.01894 |  0:02:42s\n",
      "\n",
      "Early stopping occured at epoch 96 with best_epoch = 46 and best_val_logits_ll = 0.01747\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold0_1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4006  | val_logits_ll: 0.10393 |  0:00:01s\n",
      "epoch 1  | loss: 0.0327  | val_logits_ll: 0.0276  |  0:00:03s\n",
      "epoch 2  | loss: 0.02437 | val_logits_ll: 0.02259 |  0:00:04s\n",
      "epoch 3  | loss: 0.02192 | val_logits_ll: 0.02146 |  0:00:06s\n",
      "epoch 4  | loss: 0.02109 | val_logits_ll: 0.02098 |  0:00:08s\n",
      "epoch 5  | loss: 0.02058 | val_logits_ll: 0.0208  |  0:00:09s\n",
      "epoch 6  | loss: 0.02013 | val_logits_ll: 0.02018 |  0:00:11s\n",
      "epoch 7  | loss: 0.01973 | val_logits_ll: 0.02005 |  0:00:12s\n",
      "epoch 8  | loss: 0.01934 | val_logits_ll: 0.01952 |  0:00:14s\n",
      "epoch 9  | loss: 0.01902 | val_logits_ll: 0.02001 |  0:00:16s\n",
      "epoch 10 | loss: 0.01862 | val_logits_ll: 0.01914 |  0:00:17s\n",
      "epoch 11 | loss: 0.01829 | val_logits_ll: 0.01886 |  0:00:19s\n",
      "epoch 12 | loss: 0.01808 | val_logits_ll: 0.01893 |  0:00:21s\n",
      "epoch 13 | loss: 0.0178  | val_logits_ll: 0.01848 |  0:00:22s\n",
      "epoch 14 | loss: 0.0175  | val_logits_ll: 0.0185  |  0:00:24s\n",
      "epoch 15 | loss: 0.01727 | val_logits_ll: 0.01866 |  0:00:26s\n",
      "epoch 16 | loss: 0.01719 | val_logits_ll: 0.01838 |  0:00:27s\n",
      "epoch 17 | loss: 0.01703 | val_logits_ll: 0.01875 |  0:00:29s\n",
      "epoch 18 | loss: 0.01695 | val_logits_ll: 0.01858 |  0:00:31s\n",
      "epoch 19 | loss: 0.01684 | val_logits_ll: 0.01843 |  0:00:33s\n",
      "epoch 20 | loss: 0.01672 | val_logits_ll: 0.01811 |  0:00:34s\n",
      "epoch 21 | loss: 0.01661 | val_logits_ll: 0.0182  |  0:00:36s\n",
      "epoch 22 | loss: 0.01651 | val_logits_ll: 0.0182  |  0:00:38s\n",
      "epoch 23 | loss: 0.01645 | val_logits_ll: 0.01811 |  0:00:39s\n",
      "epoch 24 | loss: 0.01623 | val_logits_ll: 0.0192  |  0:00:41s\n",
      "epoch 25 | loss: 0.01611 | val_logits_ll: 0.01803 |  0:00:43s\n",
      "epoch 26 | loss: 0.01612 | val_logits_ll: 0.01812 |  0:00:44s\n",
      "epoch 27 | loss: 0.01605 | val_logits_ll: 0.01799 |  0:00:46s\n",
      "epoch 28 | loss: 0.01602 | val_logits_ll: 0.01931 |  0:00:48s\n",
      "epoch 29 | loss: 0.01592 | val_logits_ll: 0.01858 |  0:00:49s\n",
      "epoch 30 | loss: 0.01586 | val_logits_ll: 0.01797 |  0:00:51s\n",
      "epoch 31 | loss: 0.01582 | val_logits_ll: 0.01803 |  0:00:53s\n",
      "epoch 32 | loss: 0.01566 | val_logits_ll: 0.01799 |  0:00:54s\n",
      "epoch 33 | loss: 0.01567 | val_logits_ll: 0.01823 |  0:00:56s\n",
      "epoch 34 | loss: 0.01566 | val_logits_ll: 0.01807 |  0:00:58s\n",
      "epoch 35 | loss: 0.01563 | val_logits_ll: 0.0181  |  0:00:59s\n",
      "epoch 36 | loss: 0.01558 | val_logits_ll: 0.01801 |  0:01:01s\n",
      "epoch 37 | loss: 0.01547 | val_logits_ll: 0.01816 |  0:01:03s\n",
      "epoch 38 | loss: 0.01551 | val_logits_ll: 0.01818 |  0:01:04s\n",
      "epoch 39 | loss: 0.01532 | val_logits_ll: 0.01831 |  0:01:06s\n",
      "epoch 40 | loss: 0.01535 | val_logits_ll: 0.01796 |  0:01:07s\n",
      "epoch 41 | loss: 0.01539 | val_logits_ll: 0.01808 |  0:01:09s\n",
      "epoch 42 | loss: 0.01534 | val_logits_ll: 0.01835 |  0:01:10s\n",
      "epoch 43 | loss: 0.0153  | val_logits_ll: 0.01811 |  0:01:12s\n",
      "epoch 44 | loss: 0.0152  | val_logits_ll: 0.01836 |  0:01:14s\n",
      "epoch 45 | loss: 0.01511 | val_logits_ll: 0.01835 |  0:01:15s\n",
      "epoch 46 | loss: 0.01518 | val_logits_ll: 0.01857 |  0:01:17s\n",
      "epoch 47 | loss: 0.01518 | val_logits_ll: 0.01818 |  0:01:19s\n",
      "epoch 48 | loss: 0.01521 | val_logits_ll: 0.01814 |  0:01:20s\n",
      "epoch 49 | loss: 0.01505 | val_logits_ll: 0.01811 |  0:01:22s\n",
      "epoch 50 | loss: 0.01499 | val_logits_ll: 0.01816 |  0:01:23s\n",
      "epoch 51 | loss: 0.01509 | val_logits_ll: 0.01849 |  0:01:25s\n",
      "epoch 52 | loss: 0.01448 | val_logits_ll: 0.0181  |  0:01:27s\n",
      "epoch 53 | loss: 0.01394 | val_logits_ll: 0.01809 |  0:01:28s\n",
      "epoch 54 | loss: 0.01366 | val_logits_ll: 0.01812 |  0:01:30s\n",
      "epoch 55 | loss: 0.01345 | val_logits_ll: 0.01813 |  0:01:32s\n",
      "epoch 56 | loss: 0.01328 | val_logits_ll: 0.01824 |  0:01:34s\n",
      "epoch 57 | loss: 0.01312 | val_logits_ll: 0.01832 |  0:01:35s\n",
      "epoch 58 | loss: 0.01299 | val_logits_ll: 0.0184  |  0:01:37s\n",
      "epoch 59 | loss: 0.01289 | val_logits_ll: 0.01854 |  0:01:39s\n",
      "epoch 60 | loss: 0.01274 | val_logits_ll: 0.01864 |  0:01:41s\n",
      "epoch 61 | loss: 0.0126  | val_logits_ll: 0.01873 |  0:01:42s\n",
      "epoch 62 | loss: 0.01247 | val_logits_ll: 0.01882 |  0:01:44s\n",
      "epoch 63 | loss: 0.01218 | val_logits_ll: 0.0189  |  0:01:45s\n",
      "epoch 64 | loss: 0.01209 | val_logits_ll: 0.01897 |  0:01:47s\n",
      "epoch 65 | loss: 0.01202 | val_logits_ll: 0.01902 |  0:01:49s\n",
      "epoch 66 | loss: 0.01199 | val_logits_ll: 0.01903 |  0:01:50s\n",
      "epoch 67 | loss: 0.01196 | val_logits_ll: 0.01903 |  0:01:52s\n",
      "epoch 68 | loss: 0.01193 | val_logits_ll: 0.01906 |  0:01:54s\n",
      "epoch 69 | loss: 0.01191 | val_logits_ll: 0.01908 |  0:01:55s\n",
      "epoch 70 | loss: 0.01187 | val_logits_ll: 0.01911 |  0:01:57s\n",
      "epoch 71 | loss: 0.01184 | val_logits_ll: 0.01913 |  0:01:59s\n",
      "epoch 72 | loss: 0.01182 | val_logits_ll: 0.01916 |  0:02:00s\n",
      "epoch 73 | loss: 0.01178 | val_logits_ll: 0.0192  |  0:02:02s\n",
      "epoch 74 | loss: 0.01175 | val_logits_ll: 0.01921 |  0:02:04s\n",
      "epoch 75 | loss: 0.01174 | val_logits_ll: 0.01921 |  0:02:05s\n",
      "epoch 76 | loss: 0.01174 | val_logits_ll: 0.01921 |  0:02:07s\n",
      "epoch 77 | loss: 0.01173 | val_logits_ll: 0.01923 |  0:02:08s\n",
      "epoch 78 | loss: 0.01172 | val_logits_ll: 0.01924 |  0:02:10s\n",
      "epoch 79 | loss: 0.01173 | val_logits_ll: 0.01923 |  0:02:12s\n",
      "epoch 80 | loss: 0.01172 | val_logits_ll: 0.01922 |  0:02:13s\n",
      "epoch 81 | loss: 0.01174 | val_logits_ll: 0.01923 |  0:02:15s\n",
      "epoch 82 | loss: 0.01171 | val_logits_ll: 0.01922 |  0:02:16s\n",
      "epoch 83 | loss: 0.01172 | val_logits_ll: 0.01924 |  0:02:18s\n",
      "epoch 84 | loss: 0.01172 | val_logits_ll: 0.01924 |  0:02:20s\n",
      "epoch 85 | loss: 0.01171 | val_logits_ll: 0.01925 |  0:02:21s\n",
      "epoch 86 | loss: 0.0117  | val_logits_ll: 0.01925 |  0:02:23s\n",
      "epoch 87 | loss: 0.0117  | val_logits_ll: 0.01924 |  0:02:25s\n",
      "epoch 88 | loss: 0.0117  | val_logits_ll: 0.01924 |  0:02:26s\n",
      "epoch 89 | loss: 0.01172 | val_logits_ll: 0.01925 |  0:02:28s\n",
      "epoch 90 | loss: 0.01171 | val_logits_ll: 0.01924 |  0:02:29s\n",
      "\n",
      "Early stopping occured at epoch 90 with best_epoch = 40 and best_val_logits_ll = 0.01796\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold1_1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40195 | val_logits_ll: 0.10443 |  0:00:01s\n",
      "epoch 1  | loss: 0.03231 | val_logits_ll: 0.02833 |  0:00:03s\n",
      "epoch 2  | loss: 0.02433 | val_logits_ll: 0.02326 |  0:00:05s\n",
      "epoch 3  | loss: 0.02163 | val_logits_ll: 0.02229 |  0:00:07s\n",
      "epoch 4  | loss: 0.02077 | val_logits_ll: 0.02126 |  0:00:08s\n",
      "epoch 5  | loss: 0.02031 | val_logits_ll: 0.02111 |  0:00:10s\n",
      "epoch 6  | loss: 0.02    | val_logits_ll: 0.02068 |  0:00:12s\n",
      "epoch 7  | loss: 0.01976 | val_logits_ll: 0.02054 |  0:00:13s\n",
      "epoch 8  | loss: 0.01951 | val_logits_ll: 0.02084 |  0:00:15s\n",
      "epoch 9  | loss: 0.01917 | val_logits_ll: 0.02026 |  0:00:16s\n",
      "epoch 10 | loss: 0.01884 | val_logits_ll: 0.01974 |  0:00:18s\n",
      "epoch 11 | loss: 0.01841 | val_logits_ll: 0.02014 |  0:00:20s\n",
      "epoch 12 | loss: 0.01815 | val_logits_ll: 0.01944 |  0:00:22s\n",
      "epoch 13 | loss: 0.01787 | val_logits_ll: 0.01932 |  0:00:23s\n",
      "epoch 14 | loss: 0.01773 | val_logits_ll: 0.01903 |  0:00:25s\n",
      "epoch 15 | loss: 0.01753 | val_logits_ll: 0.01971 |  0:00:26s\n",
      "epoch 16 | loss: 0.01726 | val_logits_ll: 0.02022 |  0:00:28s\n",
      "epoch 17 | loss: 0.0172  | val_logits_ll: 0.02019 |  0:00:30s\n",
      "epoch 18 | loss: 0.01706 | val_logits_ll: 0.01893 |  0:00:31s\n",
      "epoch 19 | loss: 0.0169  | val_logits_ll: 0.01889 |  0:00:33s\n",
      "epoch 20 | loss: 0.01683 | val_logits_ll: 0.01866 |  0:00:34s\n",
      "epoch 21 | loss: 0.0169  | val_logits_ll: 0.01867 |  0:00:36s\n",
      "epoch 22 | loss: 0.01661 | val_logits_ll: 0.01947 |  0:00:38s\n",
      "epoch 23 | loss: 0.01649 | val_logits_ll: 0.01832 |  0:00:39s\n",
      "epoch 24 | loss: 0.01639 | val_logits_ll: 0.01851 |  0:00:41s\n",
      "epoch 25 | loss: 0.01639 | val_logits_ll: 0.01907 |  0:00:43s\n",
      "epoch 26 | loss: 0.01627 | val_logits_ll: 0.01855 |  0:00:44s\n",
      "epoch 27 | loss: 0.01611 | val_logits_ll: 0.019   |  0:00:46s\n",
      "epoch 28 | loss: 0.01607 | val_logits_ll: 0.01841 |  0:00:48s\n",
      "epoch 29 | loss: 0.016   | val_logits_ll: 0.01844 |  0:00:49s\n",
      "epoch 30 | loss: 0.01609 | val_logits_ll: 0.01837 |  0:00:51s\n",
      "epoch 31 | loss: 0.01597 | val_logits_ll: 0.01838 |  0:00:52s\n",
      "epoch 32 | loss: 0.01589 | val_logits_ll: 0.01859 |  0:00:54s\n",
      "epoch 33 | loss: 0.01588 | val_logits_ll: 0.01951 |  0:00:56s\n",
      "epoch 34 | loss: 0.01582 | val_logits_ll: 0.01861 |  0:00:57s\n",
      "epoch 35 | loss: 0.01533 | val_logits_ll: 0.01786 |  0:00:59s\n",
      "epoch 36 | loss: 0.01492 | val_logits_ll: 0.01788 |  0:01:01s\n",
      "epoch 37 | loss: 0.0147  | val_logits_ll: 0.01798 |  0:01:03s\n",
      "epoch 38 | loss: 0.01452 | val_logits_ll: 0.01806 |  0:01:04s\n",
      "epoch 39 | loss: 0.0144  | val_logits_ll: 0.01815 |  0:01:06s\n",
      "epoch 40 | loss: 0.01427 | val_logits_ll: 0.01821 |  0:01:08s\n",
      "epoch 41 | loss: 0.01416 | val_logits_ll: 0.01837 |  0:01:10s\n",
      "epoch 42 | loss: 0.01403 | val_logits_ll: 0.01841 |  0:01:11s\n",
      "epoch 43 | loss: 0.01392 | val_logits_ll: 0.01852 |  0:01:13s\n",
      "epoch 44 | loss: 0.01379 | val_logits_ll: 0.01862 |  0:01:15s\n",
      "epoch 45 | loss: 0.01369 | val_logits_ll: 0.01867 |  0:01:16s\n",
      "epoch 46 | loss: 0.01358 | val_logits_ll: 0.01876 |  0:01:18s\n",
      "epoch 47 | loss: 0.01333 | val_logits_ll: 0.01883 |  0:01:19s\n",
      "epoch 48 | loss: 0.01322 | val_logits_ll: 0.01885 |  0:01:21s\n",
      "epoch 49 | loss: 0.01317 | val_logits_ll: 0.01889 |  0:01:23s\n",
      "epoch 50 | loss: 0.01313 | val_logits_ll: 0.01893 |  0:01:24s\n",
      "epoch 51 | loss: 0.0131  | val_logits_ll: 0.01894 |  0:01:26s\n",
      "epoch 52 | loss: 0.01307 | val_logits_ll: 0.01896 |  0:01:28s\n",
      "epoch 53 | loss: 0.01305 | val_logits_ll: 0.01898 |  0:01:29s\n",
      "epoch 54 | loss: 0.013   | val_logits_ll: 0.01899 |  0:01:31s\n",
      "epoch 55 | loss: 0.01299 | val_logits_ll: 0.01905 |  0:01:33s\n",
      "epoch 56 | loss: 0.01294 | val_logits_ll: 0.01905 |  0:01:34s\n",
      "epoch 57 | loss: 0.01293 | val_logits_ll: 0.01908 |  0:01:36s\n",
      "epoch 58 | loss: 0.0129  | val_logits_ll: 0.01908 |  0:01:38s\n",
      "epoch 59 | loss: 0.0129  | val_logits_ll: 0.01908 |  0:01:39s\n",
      "epoch 60 | loss: 0.01288 | val_logits_ll: 0.01908 |  0:01:41s\n",
      "epoch 61 | loss: 0.01287 | val_logits_ll: 0.01909 |  0:01:43s\n",
      "epoch 62 | loss: 0.01287 | val_logits_ll: 0.01909 |  0:01:44s\n",
      "epoch 63 | loss: 0.01286 | val_logits_ll: 0.01909 |  0:01:46s\n",
      "epoch 64 | loss: 0.01287 | val_logits_ll: 0.01909 |  0:01:47s\n",
      "epoch 65 | loss: 0.01287 | val_logits_ll: 0.01911 |  0:01:49s\n",
      "epoch 66 | loss: 0.01285 | val_logits_ll: 0.0191  |  0:01:51s\n",
      "epoch 67 | loss: 0.01286 | val_logits_ll: 0.0191  |  0:01:52s\n",
      "epoch 68 | loss: 0.01285 | val_logits_ll: 0.01911 |  0:01:54s\n",
      "epoch 69 | loss: 0.01287 | val_logits_ll: 0.0191  |  0:01:55s\n",
      "epoch 70 | loss: 0.01284 | val_logits_ll: 0.01911 |  0:01:57s\n",
      "epoch 71 | loss: 0.01285 | val_logits_ll: 0.01911 |  0:01:59s\n",
      "epoch 72 | loss: 0.01285 | val_logits_ll: 0.01911 |  0:02:01s\n",
      "epoch 73 | loss: 0.01285 | val_logits_ll: 0.01911 |  0:02:03s\n",
      "epoch 74 | loss: 0.01285 | val_logits_ll: 0.01912 |  0:02:04s\n",
      "epoch 75 | loss: 0.01285 | val_logits_ll: 0.01912 |  0:02:06s\n",
      "epoch 76 | loss: 0.01285 | val_logits_ll: 0.01912 |  0:02:07s\n",
      "epoch 77 | loss: 0.01283 | val_logits_ll: 0.01911 |  0:02:09s\n",
      "epoch 78 | loss: 0.01285 | val_logits_ll: 0.0191  |  0:02:11s\n",
      "epoch 79 | loss: 0.01285 | val_logits_ll: 0.01912 |  0:02:13s\n",
      "epoch 80 | loss: 0.01284 | val_logits_ll: 0.01912 |  0:02:14s\n",
      "epoch 81 | loss: 0.01285 | val_logits_ll: 0.01911 |  0:02:16s\n",
      "epoch 82 | loss: 0.01283 | val_logits_ll: 0.01912 |  0:02:17s\n",
      "epoch 83 | loss: 0.01283 | val_logits_ll: 0.0191  |  0:02:19s\n",
      "epoch 84 | loss: 0.01284 | val_logits_ll: 0.01911 |  0:02:20s\n",
      "epoch 85 | loss: 0.01282 | val_logits_ll: 0.01911 |  0:02:22s\n",
      "\n",
      "Early stopping occured at epoch 85 with best_epoch = 35 and best_val_logits_ll = 0.01786\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold2_1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40266 | val_logits_ll: 0.10242 |  0:00:01s\n",
      "epoch 1  | loss: 0.03312 | val_logits_ll: 0.02676 |  0:00:03s\n",
      "epoch 2  | loss: 0.02451 | val_logits_ll: 0.02201 |  0:00:04s\n",
      "epoch 3  | loss: 0.02173 | val_logits_ll: 0.0211  |  0:00:06s\n",
      "epoch 4  | loss: 0.02087 | val_logits_ll: 0.02042 |  0:00:08s\n",
      "epoch 5  | loss: 0.02047 | val_logits_ll: 0.02026 |  0:00:09s\n",
      "epoch 6  | loss: 0.02025 | val_logits_ll: 0.02002 |  0:00:11s\n",
      "epoch 7  | loss: 0.01992 | val_logits_ll: 0.01989 |  0:00:13s\n",
      "epoch 8  | loss: 0.01957 | val_logits_ll: 0.01965 |  0:00:14s\n",
      "epoch 9  | loss: 0.01936 | val_logits_ll: 0.01946 |  0:00:16s\n",
      "epoch 10 | loss: 0.01903 | val_logits_ll: 0.01945 |  0:00:17s\n",
      "epoch 11 | loss: 0.01866 | val_logits_ll: 0.01906 |  0:00:19s\n",
      "epoch 12 | loss: 0.01822 | val_logits_ll: 0.01862 |  0:00:21s\n",
      "epoch 13 | loss: 0.01793 | val_logits_ll: 0.01842 |  0:00:22s\n",
      "epoch 14 | loss: 0.01765 | val_logits_ll: 0.01872 |  0:00:24s\n",
      "epoch 15 | loss: 0.01749 | val_logits_ll: 0.01825 |  0:00:26s\n",
      "epoch 16 | loss: 0.01727 | val_logits_ll: 0.01815 |  0:00:27s\n",
      "epoch 17 | loss: 0.01723 | val_logits_ll: 0.01825 |  0:00:29s\n",
      "epoch 18 | loss: 0.01707 | val_logits_ll: 0.01819 |  0:00:30s\n",
      "epoch 19 | loss: 0.01698 | val_logits_ll: 0.01827 |  0:00:32s\n",
      "epoch 20 | loss: 0.01687 | val_logits_ll: 0.01809 |  0:00:34s\n",
      "epoch 21 | loss: 0.01672 | val_logits_ll: 0.0184  |  0:00:36s\n",
      "epoch 22 | loss: 0.0166  | val_logits_ll: 0.01819 |  0:00:38s\n",
      "epoch 23 | loss: 0.01652 | val_logits_ll: 0.02009 |  0:00:39s\n",
      "epoch 24 | loss: 0.01639 | val_logits_ll: 0.01803 |  0:00:41s\n",
      "epoch 25 | loss: 0.01629 | val_logits_ll: 0.02004 |  0:00:43s\n",
      "epoch 26 | loss: 0.01625 | val_logits_ll: 0.01847 |  0:00:44s\n",
      "epoch 27 | loss: 0.01618 | val_logits_ll: 0.01823 |  0:00:46s\n",
      "epoch 28 | loss: 0.01605 | val_logits_ll: 0.01772 |  0:00:47s\n",
      "epoch 29 | loss: 0.01604 | val_logits_ll: 0.01977 |  0:00:49s\n",
      "epoch 30 | loss: 0.01603 | val_logits_ll: 0.01786 |  0:00:51s\n",
      "epoch 31 | loss: 0.01596 | val_logits_ll: 0.01785 |  0:00:53s\n",
      "epoch 32 | loss: 0.01584 | val_logits_ll: 0.01822 |  0:00:54s\n",
      "epoch 33 | loss: 0.01585 | val_logits_ll: 0.01876 |  0:00:56s\n",
      "epoch 34 | loss: 0.01582 | val_logits_ll: 0.0179  |  0:00:57s\n",
      "epoch 35 | loss: 0.01568 | val_logits_ll: 0.01791 |  0:00:59s\n",
      "epoch 36 | loss: 0.01569 | val_logits_ll: 0.01836 |  0:01:00s\n",
      "epoch 37 | loss: 0.01566 | val_logits_ll: 0.01783 |  0:01:02s\n",
      "epoch 38 | loss: 0.01563 | val_logits_ll: 0.01812 |  0:01:04s\n",
      "epoch 39 | loss: 0.01551 | val_logits_ll: 0.01795 |  0:01:05s\n",
      "epoch 40 | loss: 0.01494 | val_logits_ll: 0.01768 |  0:01:07s\n",
      "epoch 41 | loss: 0.01448 | val_logits_ll: 0.01771 |  0:01:09s\n",
      "epoch 42 | loss: 0.01427 | val_logits_ll: 0.01778 |  0:01:10s\n",
      "epoch 43 | loss: 0.01408 | val_logits_ll: 0.01786 |  0:01:12s\n",
      "epoch 44 | loss: 0.01395 | val_logits_ll: 0.01796 |  0:01:14s\n",
      "epoch 45 | loss: 0.0138  | val_logits_ll: 0.01799 |  0:01:15s\n",
      "epoch 46 | loss: 0.01362 | val_logits_ll: 0.01818 |  0:01:17s\n",
      "epoch 47 | loss: 0.0135  | val_logits_ll: 0.0182  |  0:01:18s\n",
      "epoch 48 | loss: 0.01338 | val_logits_ll: 0.01832 |  0:01:20s\n",
      "epoch 49 | loss: 0.01329 | val_logits_ll: 0.01842 |  0:01:21s\n",
      "epoch 50 | loss: 0.01312 | val_logits_ll: 0.01857 |  0:01:23s\n",
      "epoch 51 | loss: 0.01302 | val_logits_ll: 0.01861 |  0:01:25s\n",
      "epoch 52 | loss: 0.01269 | val_logits_ll: 0.01862 |  0:01:27s\n",
      "epoch 53 | loss: 0.01258 | val_logits_ll: 0.01868 |  0:01:28s\n",
      "epoch 54 | loss: 0.01253 | val_logits_ll: 0.01873 |  0:01:30s\n",
      "epoch 55 | loss: 0.01247 | val_logits_ll: 0.01871 |  0:01:31s\n",
      "epoch 56 | loss: 0.01244 | val_logits_ll: 0.01876 |  0:01:33s\n",
      "epoch 57 | loss: 0.01239 | val_logits_ll: 0.01881 |  0:01:35s\n",
      "epoch 58 | loss: 0.01237 | val_logits_ll: 0.01882 |  0:01:37s\n",
      "epoch 59 | loss: 0.01234 | val_logits_ll: 0.01884 |  0:01:39s\n",
      "epoch 60 | loss: 0.01234 | val_logits_ll: 0.01886 |  0:01:41s\n",
      "epoch 61 | loss: 0.01229 | val_logits_ll: 0.01891 |  0:01:42s\n",
      "epoch 62 | loss: 0.01226 | val_logits_ll: 0.01892 |  0:01:44s\n",
      "epoch 63 | loss: 0.01223 | val_logits_ll: 0.01893 |  0:01:45s\n",
      "epoch 64 | loss: 0.0122  | val_logits_ll: 0.01894 |  0:01:47s\n",
      "epoch 65 | loss: 0.0122  | val_logits_ll: 0.01893 |  0:01:49s\n",
      "epoch 66 | loss: 0.01218 | val_logits_ll: 0.01894 |  0:01:50s\n",
      "epoch 67 | loss: 0.0122  | val_logits_ll: 0.01893 |  0:01:52s\n",
      "epoch 68 | loss: 0.01218 | val_logits_ll: 0.01894 |  0:01:54s\n",
      "epoch 69 | loss: 0.01217 | val_logits_ll: 0.01894 |  0:01:55s\n",
      "epoch 70 | loss: 0.01219 | val_logits_ll: 0.01893 |  0:01:57s\n",
      "epoch 71 | loss: 0.01218 | val_logits_ll: 0.01893 |  0:01:59s\n",
      "epoch 72 | loss: 0.01219 | val_logits_ll: 0.01894 |  0:02:00s\n",
      "epoch 73 | loss: 0.01216 | val_logits_ll: 0.01895 |  0:02:02s\n",
      "epoch 74 | loss: 0.01216 | val_logits_ll: 0.01896 |  0:02:03s\n",
      "epoch 75 | loss: 0.01216 | val_logits_ll: 0.01895 |  0:02:05s\n",
      "epoch 76 | loss: 0.0122  | val_logits_ll: 0.01895 |  0:02:07s\n",
      "epoch 77 | loss: 0.0122  | val_logits_ll: 0.01895 |  0:02:08s\n",
      "epoch 78 | loss: 0.01221 | val_logits_ll: 0.01897 |  0:02:10s\n",
      "epoch 79 | loss: 0.01217 | val_logits_ll: 0.01895 |  0:02:12s\n",
      "epoch 80 | loss: 0.01216 | val_logits_ll: 0.01895 |  0:02:14s\n",
      "epoch 81 | loss: 0.01216 | val_logits_ll: 0.01897 |  0:02:15s\n",
      "epoch 82 | loss: 0.01215 | val_logits_ll: 0.01895 |  0:02:17s\n",
      "epoch 83 | loss: 0.01216 | val_logits_ll: 0.01896 |  0:02:18s\n",
      "epoch 84 | loss: 0.01218 | val_logits_ll: 0.01895 |  0:02:20s\n",
      "epoch 85 | loss: 0.01217 | val_logits_ll: 0.01898 |  0:02:22s\n",
      "epoch 86 | loss: 0.01214 | val_logits_ll: 0.01896 |  0:02:23s\n",
      "epoch 87 | loss: 0.01216 | val_logits_ll: 0.01897 |  0:02:25s\n",
      "epoch 88 | loss: 0.01216 | val_logits_ll: 0.01897 |  0:02:26s\n",
      "epoch 89 | loss: 0.01217 | val_logits_ll: 0.01898 |  0:02:28s\n",
      "epoch 90 | loss: 0.01215 | val_logits_ll: 0.01894 |  0:02:30s\n",
      "\n",
      "Early stopping occured at epoch 90 with best_epoch = 40 and best_val_logits_ll = 0.01768\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold3_1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40126 | val_logits_ll: 0.09491 |  0:00:01s\n",
      "epoch 1  | loss: 0.03268 | val_logits_ll: 0.02656 |  0:00:03s\n",
      "epoch 2  | loss: 0.02422 | val_logits_ll: 0.02117 |  0:00:05s\n",
      "epoch 3  | loss: 0.02166 | val_logits_ll: 0.02042 |  0:00:06s\n",
      "epoch 4  | loss: 0.02084 | val_logits_ll: 0.01991 |  0:00:08s\n",
      "epoch 5  | loss: 0.02048 | val_logits_ll: 0.01977 |  0:00:10s\n",
      "epoch 6  | loss: 0.02017 | val_logits_ll: 0.01956 |  0:00:12s\n",
      "epoch 7  | loss: 0.01989 | val_logits_ll: 0.0193  |  0:00:13s\n",
      "epoch 8  | loss: 0.01948 | val_logits_ll: 0.01895 |  0:00:15s\n",
      "epoch 9  | loss: 0.01911 | val_logits_ll: 0.01905 |  0:00:17s\n",
      "epoch 10 | loss: 0.01874 | val_logits_ll: 0.01856 |  0:00:18s\n",
      "epoch 11 | loss: 0.0184  | val_logits_ll: 0.01833 |  0:00:20s\n",
      "epoch 12 | loss: 0.01813 | val_logits_ll: 0.01808 |  0:00:22s\n",
      "epoch 13 | loss: 0.01791 | val_logits_ll: 0.01816 |  0:00:23s\n",
      "epoch 14 | loss: 0.01776 | val_logits_ll: 0.01821 |  0:00:25s\n",
      "epoch 15 | loss: 0.01757 | val_logits_ll: 0.01793 |  0:00:26s\n",
      "epoch 16 | loss: 0.01738 | val_logits_ll: 0.01824 |  0:00:28s\n",
      "epoch 17 | loss: 0.01722 | val_logits_ll: 0.01808 |  0:00:30s\n",
      "epoch 18 | loss: 0.01715 | val_logits_ll: 0.01788 |  0:00:32s\n",
      "epoch 19 | loss: 0.01699 | val_logits_ll: 0.01987 |  0:00:33s\n",
      "epoch 20 | loss: 0.01686 | val_logits_ll: 0.01766 |  0:00:35s\n",
      "epoch 21 | loss: 0.01668 | val_logits_ll: 0.01761 |  0:00:36s\n",
      "epoch 22 | loss: 0.01668 | val_logits_ll: 0.01807 |  0:00:38s\n",
      "epoch 23 | loss: 0.01659 | val_logits_ll: 0.01767 |  0:00:39s\n",
      "epoch 24 | loss: 0.01644 | val_logits_ll: 0.0175  |  0:00:41s\n",
      "epoch 25 | loss: 0.01638 | val_logits_ll: 0.01794 |  0:00:43s\n",
      "epoch 26 | loss: 0.01633 | val_logits_ll: 0.01823 |  0:00:45s\n",
      "epoch 27 | loss: 0.01633 | val_logits_ll: 0.01746 |  0:00:46s\n",
      "epoch 28 | loss: 0.01624 | val_logits_ll: 0.0178  |  0:00:48s\n",
      "epoch 29 | loss: 0.01615 | val_logits_ll: 0.0174  |  0:00:49s\n",
      "epoch 30 | loss: 0.01605 | val_logits_ll: 0.0173  |  0:00:51s\n",
      "epoch 31 | loss: 0.016   | val_logits_ll: 0.01746 |  0:00:53s\n",
      "epoch 32 | loss: 0.01599 | val_logits_ll: 0.01746 |  0:00:54s\n",
      "epoch 33 | loss: 0.01597 | val_logits_ll: 0.01752 |  0:00:56s\n",
      "epoch 34 | loss: 0.01583 | val_logits_ll: 0.01748 |  0:00:57s\n",
      "epoch 35 | loss: 0.0158  | val_logits_ll: 0.01792 |  0:00:59s\n",
      "epoch 36 | loss: 0.01586 | val_logits_ll: 0.01757 |  0:01:01s\n",
      "epoch 37 | loss: 0.01572 | val_logits_ll: 0.01729 |  0:01:03s\n",
      "epoch 38 | loss: 0.01563 | val_logits_ll: 0.01761 |  0:01:05s\n",
      "epoch 39 | loss: 0.01559 | val_logits_ll: 0.01755 |  0:01:06s\n",
      "epoch 40 | loss: 0.01561 | val_logits_ll: 0.01743 |  0:01:08s\n",
      "epoch 41 | loss: 0.01555 | val_logits_ll: 0.01817 |  0:01:10s\n",
      "epoch 42 | loss: 0.01556 | val_logits_ll: 0.01752 |  0:01:11s\n",
      "epoch 43 | loss: 0.0155  | val_logits_ll: 0.01758 |  0:01:13s\n",
      "epoch 44 | loss: 0.0155  | val_logits_ll: 0.01754 |  0:01:15s\n",
      "epoch 45 | loss: 0.01552 | val_logits_ll: 0.01764 |  0:01:17s\n",
      "epoch 46 | loss: 0.0155  | val_logits_ll: 0.01763 |  0:01:19s\n",
      "epoch 47 | loss: 0.01541 | val_logits_ll: 0.01745 |  0:01:20s\n",
      "epoch 48 | loss: 0.01539 | val_logits_ll: 0.01761 |  0:01:22s\n",
      "epoch 49 | loss: 0.01481 | val_logits_ll: 0.01727 |  0:01:23s\n",
      "epoch 50 | loss: 0.01434 | val_logits_ll: 0.01733 |  0:01:25s\n",
      "epoch 51 | loss: 0.0141  | val_logits_ll: 0.01743 |  0:01:27s\n",
      "epoch 52 | loss: 0.01395 | val_logits_ll: 0.01747 |  0:01:28s\n",
      "epoch 53 | loss: 0.01379 | val_logits_ll: 0.01758 |  0:01:30s\n",
      "epoch 54 | loss: 0.01365 | val_logits_ll: 0.01771 |  0:01:32s\n",
      "epoch 55 | loss: 0.01355 | val_logits_ll: 0.01776 |  0:01:33s\n",
      "epoch 56 | loss: 0.01344 | val_logits_ll: 0.01781 |  0:01:35s\n",
      "epoch 57 | loss: 0.01331 | val_logits_ll: 0.01792 |  0:01:37s\n",
      "epoch 58 | loss: 0.01321 | val_logits_ll: 0.01804 |  0:01:38s\n",
      "epoch 59 | loss: 0.01311 | val_logits_ll: 0.01811 |  0:01:40s\n",
      "epoch 60 | loss: 0.01301 | val_logits_ll: 0.01822 |  0:01:41s\n",
      "epoch 61 | loss: 0.01269 | val_logits_ll: 0.01823 |  0:01:43s\n",
      "epoch 62 | loss: 0.01258 | val_logits_ll: 0.01829 |  0:01:45s\n",
      "epoch 63 | loss: 0.01253 | val_logits_ll: 0.01832 |  0:01:47s\n",
      "epoch 64 | loss: 0.0125  | val_logits_ll: 0.01835 |  0:01:48s\n",
      "epoch 65 | loss: 0.01244 | val_logits_ll: 0.01834 |  0:01:50s\n",
      "epoch 66 | loss: 0.01244 | val_logits_ll: 0.01839 |  0:01:51s\n",
      "epoch 67 | loss: 0.01239 | val_logits_ll: 0.0184  |  0:01:53s\n",
      "epoch 68 | loss: 0.01238 | val_logits_ll: 0.01842 |  0:01:55s\n",
      "epoch 69 | loss: 0.01233 | val_logits_ll: 0.01846 |  0:01:56s\n",
      "epoch 70 | loss: 0.01232 | val_logits_ll: 0.01848 |  0:01:58s\n",
      "epoch 71 | loss: 0.01228 | val_logits_ll: 0.01848 |  0:02:00s\n",
      "epoch 72 | loss: 0.01222 | val_logits_ll: 0.0185  |  0:02:01s\n",
      "epoch 73 | loss: 0.01223 | val_logits_ll: 0.01852 |  0:02:03s\n",
      "epoch 74 | loss: 0.01225 | val_logits_ll: 0.01851 |  0:02:05s\n",
      "epoch 75 | loss: 0.01224 | val_logits_ll: 0.0185  |  0:02:07s\n",
      "epoch 76 | loss: 0.01223 | val_logits_ll: 0.01853 |  0:02:09s\n",
      "epoch 77 | loss: 0.01223 | val_logits_ll: 0.01853 |  0:02:10s\n",
      "epoch 78 | loss: 0.01222 | val_logits_ll: 0.01851 |  0:02:12s\n",
      "epoch 79 | loss: 0.01221 | val_logits_ll: 0.01853 |  0:02:14s\n",
      "epoch 80 | loss: 0.01223 | val_logits_ll: 0.01851 |  0:02:15s\n",
      "epoch 81 | loss: 0.0122  | val_logits_ll: 0.01852 |  0:02:17s\n",
      "epoch 82 | loss: 0.01222 | val_logits_ll: 0.01855 |  0:02:19s\n",
      "epoch 83 | loss: 0.01222 | val_logits_ll: 0.01852 |  0:02:20s\n",
      "epoch 84 | loss: 0.0122  | val_logits_ll: 0.01854 |  0:02:22s\n",
      "epoch 85 | loss: 0.0122  | val_logits_ll: 0.01853 |  0:02:24s\n",
      "epoch 86 | loss: 0.0122  | val_logits_ll: 0.01853 |  0:02:25s\n",
      "epoch 87 | loss: 0.01224 | val_logits_ll: 0.01852 |  0:02:27s\n",
      "epoch 88 | loss: 0.01219 | val_logits_ll: 0.01853 |  0:02:29s\n",
      "epoch 89 | loss: 0.0122  | val_logits_ll: 0.01853 |  0:02:30s\n",
      "epoch 90 | loss: 0.01222 | val_logits_ll: 0.01854 |  0:02:32s\n",
      "epoch 91 | loss: 0.0122  | val_logits_ll: 0.01853 |  0:02:33s\n",
      "epoch 92 | loss: 0.01221 | val_logits_ll: 0.01853 |  0:02:35s\n",
      "epoch 93 | loss: 0.01219 | val_logits_ll: 0.01853 |  0:02:37s\n",
      "epoch 94 | loss: 0.01219 | val_logits_ll: 0.01852 |  0:02:38s\n",
      "epoch 95 | loss: 0.01219 | val_logits_ll: 0.01854 |  0:02:40s\n",
      "epoch 96 | loss: 0.0122  | val_logits_ll: 0.01852 |  0:02:42s\n",
      "epoch 97 | loss: 0.0122  | val_logits_ll: 0.01853 |  0:02:43s\n",
      "epoch 98 | loss: 0.01219 | val_logits_ll: 0.01854 |  0:02:45s\n",
      "epoch 99 | loss: 0.0122  | val_logits_ll: 0.01855 |  0:02:47s\n",
      "\n",
      "Early stopping occured at epoch 99 with best_epoch = 49 and best_val_logits_ll = 0.01727\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold4_1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40211 | val_logits_ll: 0.10545 |  0:00:01s\n",
      "epoch 1  | loss: 0.03279 | val_logits_ll: 0.02665 |  0:00:03s\n",
      "epoch 2  | loss: 0.0238  | val_logits_ll: 0.02225 |  0:00:04s\n",
      "epoch 3  | loss: 0.02145 | val_logits_ll: 0.02122 |  0:00:06s\n",
      "epoch 4  | loss: 0.02082 | val_logits_ll: 0.02078 |  0:00:08s\n",
      "epoch 5  | loss: 0.02039 | val_logits_ll: 0.02048 |  0:00:09s\n",
      "epoch 6  | loss: 0.02008 | val_logits_ll: 0.02098 |  0:00:11s\n",
      "epoch 7  | loss: 0.01978 | val_logits_ll: 0.02015 |  0:00:12s\n",
      "epoch 8  | loss: 0.01948 | val_logits_ll: 0.01992 |  0:00:14s\n",
      "epoch 9  | loss: 0.01904 | val_logits_ll: 0.01978 |  0:00:16s\n",
      "epoch 10 | loss: 0.01859 | val_logits_ll: 0.01905 |  0:00:18s\n",
      "epoch 11 | loss: 0.01824 | val_logits_ll: 0.01898 |  0:00:20s\n",
      "epoch 12 | loss: 0.01787 | val_logits_ll: 0.01888 |  0:00:21s\n",
      "epoch 13 | loss: 0.01761 | val_logits_ll: 0.01953 |  0:00:23s\n",
      "epoch 14 | loss: 0.01747 | val_logits_ll: 0.01862 |  0:00:25s\n",
      "epoch 15 | loss: 0.01721 | val_logits_ll: 0.01998 |  0:00:26s\n",
      "epoch 16 | loss: 0.01702 | val_logits_ll: 0.01891 |  0:00:28s\n",
      "epoch 17 | loss: 0.01696 | val_logits_ll: 0.01882 |  0:00:29s\n",
      "epoch 18 | loss: 0.01691 | val_logits_ll: 0.01848 |  0:00:31s\n",
      "epoch 19 | loss: 0.01682 | val_logits_ll: 0.01817 |  0:00:33s\n",
      "epoch 20 | loss: 0.01674 | val_logits_ll: 0.01836 |  0:00:34s\n",
      "epoch 21 | loss: 0.01656 | val_logits_ll: 0.01812 |  0:00:36s\n",
      "epoch 22 | loss: 0.01643 | val_logits_ll: 0.01844 |  0:00:38s\n",
      "epoch 23 | loss: 0.01633 | val_logits_ll: 0.01817 |  0:00:39s\n",
      "epoch 24 | loss: 0.01624 | val_logits_ll: 0.01822 |  0:00:41s\n",
      "epoch 25 | loss: 0.01619 | val_logits_ll: 0.01803 |  0:00:42s\n",
      "epoch 26 | loss: 0.01604 | val_logits_ll: 0.01804 |  0:00:44s\n",
      "epoch 27 | loss: 0.01596 | val_logits_ll: 0.01816 |  0:00:45s\n",
      "epoch 28 | loss: 0.01606 | val_logits_ll: 0.01833 |  0:00:47s\n",
      "epoch 29 | loss: 0.01603 | val_logits_ll: 0.0185  |  0:00:49s\n",
      "epoch 30 | loss: 0.01587 | val_logits_ll: 0.01796 |  0:00:50s\n",
      "epoch 31 | loss: 0.01579 | val_logits_ll: 0.01892 |  0:00:52s\n",
      "epoch 32 | loss: 0.01571 | val_logits_ll: 0.01835 |  0:00:54s\n",
      "epoch 33 | loss: 0.01554 | val_logits_ll: 0.01843 |  0:00:55s\n",
      "epoch 34 | loss: 0.01559 | val_logits_ll: 0.01868 |  0:00:57s\n",
      "epoch 35 | loss: 0.0156  | val_logits_ll: 0.01872 |  0:00:58s\n",
      "epoch 36 | loss: 0.0155  | val_logits_ll: 0.01897 |  0:01:00s\n",
      "epoch 37 | loss: 0.01549 | val_logits_ll: 0.01825 |  0:01:02s\n",
      "epoch 38 | loss: 0.01536 | val_logits_ll: 0.01798 |  0:01:03s\n",
      "epoch 39 | loss: 0.01539 | val_logits_ll: 0.01833 |  0:01:05s\n",
      "epoch 40 | loss: 0.01527 | val_logits_ll: 0.01876 |  0:01:07s\n",
      "epoch 41 | loss: 0.0154  | val_logits_ll: 0.01831 |  0:01:08s\n",
      "epoch 42 | loss: 0.01469 | val_logits_ll: 0.01779 |  0:01:10s\n",
      "epoch 43 | loss: 0.01424 | val_logits_ll: 0.0178  |  0:01:11s\n",
      "epoch 44 | loss: 0.01398 | val_logits_ll: 0.0179  |  0:01:13s\n",
      "epoch 45 | loss: 0.01378 | val_logits_ll: 0.01797 |  0:01:15s\n",
      "epoch 46 | loss: 0.01362 | val_logits_ll: 0.01809 |  0:01:17s\n",
      "epoch 47 | loss: 0.0135  | val_logits_ll: 0.01821 |  0:01:18s\n",
      "epoch 48 | loss: 0.01333 | val_logits_ll: 0.01826 |  0:01:20s\n",
      "epoch 49 | loss: 0.0132  | val_logits_ll: 0.01843 |  0:01:22s\n",
      "epoch 50 | loss: 0.0131  | val_logits_ll: 0.01847 |  0:01:24s\n",
      "epoch 51 | loss: 0.01298 | val_logits_ll: 0.01861 |  0:01:26s\n",
      "epoch 52 | loss: 0.01285 | val_logits_ll: 0.01875 |  0:01:27s\n",
      "epoch 53 | loss: 0.01275 | val_logits_ll: 0.01878 |  0:01:29s\n",
      "epoch 54 | loss: 0.01246 | val_logits_ll: 0.01884 |  0:01:30s\n",
      "epoch 55 | loss: 0.01233 | val_logits_ll: 0.01887 |  0:01:32s\n",
      "epoch 56 | loss: 0.0123  | val_logits_ll: 0.01889 |  0:01:34s\n",
      "epoch 57 | loss: 0.01223 | val_logits_ll: 0.01892 |  0:01:36s\n",
      "epoch 58 | loss: 0.01221 | val_logits_ll: 0.01894 |  0:01:37s\n",
      "epoch 59 | loss: 0.01218 | val_logits_ll: 0.01896 |  0:01:39s\n",
      "epoch 60 | loss: 0.01217 | val_logits_ll: 0.01896 |  0:01:40s\n",
      "epoch 61 | loss: 0.01212 | val_logits_ll: 0.01903 |  0:01:42s\n",
      "epoch 62 | loss: 0.01208 | val_logits_ll: 0.01904 |  0:01:43s\n",
      "epoch 63 | loss: 0.01206 | val_logits_ll: 0.01907 |  0:01:45s\n",
      "epoch 64 | loss: 0.01206 | val_logits_ll: 0.01909 |  0:01:47s\n",
      "epoch 65 | loss: 0.012   | val_logits_ll: 0.01909 |  0:01:48s\n",
      "epoch 66 | loss: 0.012   | val_logits_ll: 0.01909 |  0:01:50s\n",
      "epoch 67 | loss: 0.01199 | val_logits_ll: 0.01911 |  0:01:51s\n",
      "epoch 68 | loss: 0.012   | val_logits_ll: 0.01912 |  0:01:53s\n",
      "epoch 69 | loss: 0.01198 | val_logits_ll: 0.01912 |  0:01:55s\n",
      "epoch 70 | loss: 0.01197 | val_logits_ll: 0.01912 |  0:01:57s\n",
      "epoch 71 | loss: 0.01199 | val_logits_ll: 0.01913 |  0:01:58s\n",
      "epoch 72 | loss: 0.01195 | val_logits_ll: 0.01912 |  0:02:00s\n",
      "epoch 73 | loss: 0.01197 | val_logits_ll: 0.01913 |  0:02:01s\n",
      "epoch 74 | loss: 0.01198 | val_logits_ll: 0.01912 |  0:02:03s\n",
      "epoch 75 | loss: 0.01197 | val_logits_ll: 0.01913 |  0:02:04s\n",
      "epoch 76 | loss: 0.01198 | val_logits_ll: 0.01914 |  0:02:06s\n",
      "epoch 77 | loss: 0.01196 | val_logits_ll: 0.01913 |  0:02:08s\n",
      "epoch 78 | loss: 0.01195 | val_logits_ll: 0.01912 |  0:02:10s\n",
      "epoch 79 | loss: 0.01196 | val_logits_ll: 0.01912 |  0:02:11s\n",
      "epoch 80 | loss: 0.01196 | val_logits_ll: 0.01912 |  0:02:13s\n",
      "epoch 81 | loss: 0.01195 | val_logits_ll: 0.01913 |  0:02:15s\n",
      "epoch 82 | loss: 0.01196 | val_logits_ll: 0.01913 |  0:02:16s\n",
      "epoch 83 | loss: 0.01196 | val_logits_ll: 0.01914 |  0:02:18s\n",
      "epoch 84 | loss: 0.01196 | val_logits_ll: 0.01913 |  0:02:20s\n",
      "epoch 85 | loss: 0.01195 | val_logits_ll: 0.01912 |  0:02:21s\n",
      "epoch 86 | loss: 0.01195 | val_logits_ll: 0.01913 |  0:02:23s\n",
      "epoch 87 | loss: 0.01194 | val_logits_ll: 0.01912 |  0:02:25s\n",
      "epoch 88 | loss: 0.01196 | val_logits_ll: 0.01913 |  0:02:26s\n",
      "epoch 89 | loss: 0.01195 | val_logits_ll: 0.01913 |  0:02:29s\n",
      "epoch 90 | loss: 0.01196 | val_logits_ll: 0.01913 |  0:02:30s\n",
      "epoch 91 | loss: 0.01195 | val_logits_ll: 0.01913 |  0:02:32s\n",
      "epoch 92 | loss: 0.01195 | val_logits_ll: 0.01914 |  0:02:33s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 42 and best_val_logits_ll = 0.01779\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold5_1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40592 | val_logits_ll: 0.10442 |  0:00:01s\n",
      "epoch 1  | loss: 0.03252 | val_logits_ll: 0.02689 |  0:00:03s\n",
      "epoch 2  | loss: 0.02396 | val_logits_ll: 0.02225 |  0:00:05s\n",
      "epoch 3  | loss: 0.0216  | val_logits_ll: 0.02159 |  0:00:07s\n",
      "epoch 4  | loss: 0.02085 | val_logits_ll: 0.02082 |  0:00:08s\n",
      "epoch 5  | loss: 0.02045 | val_logits_ll: 0.02051 |  0:00:10s\n",
      "epoch 6  | loss: 0.0202  | val_logits_ll: 0.02042 |  0:00:11s\n",
      "epoch 7  | loss: 0.01988 | val_logits_ll: 0.02011 |  0:00:13s\n",
      "epoch 8  | loss: 0.01957 | val_logits_ll: 0.02003 |  0:00:15s\n",
      "epoch 9  | loss: 0.01921 | val_logits_ll: 0.01971 |  0:00:16s\n",
      "epoch 10 | loss: 0.0188  | val_logits_ll: 0.0197  |  0:00:18s\n",
      "epoch 11 | loss: 0.01854 | val_logits_ll: 0.01949 |  0:00:19s\n",
      "epoch 12 | loss: 0.01836 | val_logits_ll: 0.01933 |  0:00:21s\n",
      "epoch 13 | loss: 0.01802 | val_logits_ll: 0.01917 |  0:00:23s\n",
      "epoch 14 | loss: 0.01778 | val_logits_ll: 0.01877 |  0:00:24s\n",
      "epoch 15 | loss: 0.01746 | val_logits_ll: 0.01883 |  0:00:26s\n",
      "epoch 16 | loss: 0.01731 | val_logits_ll: 0.01859 |  0:00:28s\n",
      "epoch 17 | loss: 0.01714 | val_logits_ll: 0.01843 |  0:00:30s\n",
      "epoch 18 | loss: 0.01704 | val_logits_ll: 0.01861 |  0:00:31s\n",
      "epoch 19 | loss: 0.0169  | val_logits_ll: 0.02018 |  0:00:33s\n",
      "epoch 20 | loss: 0.01682 | val_logits_ll: 0.01818 |  0:00:34s\n",
      "epoch 21 | loss: 0.0166  | val_logits_ll: 0.01852 |  0:00:37s\n",
      "epoch 22 | loss: 0.01653 | val_logits_ll: 0.01839 |  0:00:38s\n",
      "epoch 23 | loss: 0.0165  | val_logits_ll: 0.02074 |  0:00:40s\n",
      "epoch 24 | loss: 0.01639 | val_logits_ll: 0.02116 |  0:00:42s\n",
      "epoch 25 | loss: 0.01629 | val_logits_ll: 0.01854 |  0:00:43s\n",
      "epoch 26 | loss: 0.0163  | val_logits_ll: 0.01802 |  0:00:45s\n",
      "epoch 27 | loss: 0.01614 | val_logits_ll: 0.0182  |  0:00:47s\n",
      "epoch 28 | loss: 0.01614 | val_logits_ll: 0.01805 |  0:00:49s\n",
      "epoch 29 | loss: 0.01606 | val_logits_ll: 0.02039 |  0:00:50s\n",
      "epoch 30 | loss: 0.016   | val_logits_ll: 0.01804 |  0:00:52s\n",
      "epoch 31 | loss: 0.01586 | val_logits_ll: 0.01776 |  0:00:53s\n",
      "epoch 32 | loss: 0.01582 | val_logits_ll: 0.01853 |  0:00:55s\n",
      "epoch 33 | loss: 0.01582 | val_logits_ll: 0.01806 |  0:00:56s\n",
      "epoch 34 | loss: 0.01573 | val_logits_ll: 0.01827 |  0:00:59s\n",
      "epoch 35 | loss: 0.0156  | val_logits_ll: 0.01814 |  0:01:00s\n",
      "epoch 36 | loss: 0.01571 | val_logits_ll: 0.01797 |  0:01:02s\n",
      "epoch 37 | loss: 0.01562 | val_logits_ll: 0.01854 |  0:01:03s\n",
      "epoch 38 | loss: 0.01568 | val_logits_ll: 0.0181  |  0:01:05s\n",
      "epoch 39 | loss: 0.01565 | val_logits_ll: 0.01773 |  0:01:06s\n",
      "epoch 40 | loss: 0.01545 | val_logits_ll: 0.01791 |  0:01:08s\n",
      "epoch 41 | loss: 0.01537 | val_logits_ll: 0.01793 |  0:01:10s\n",
      "epoch 42 | loss: 0.01546 | val_logits_ll: 0.0182  |  0:01:12s\n",
      "epoch 43 | loss: 0.01543 | val_logits_ll: 0.01795 |  0:01:13s\n",
      "epoch 44 | loss: 0.01534 | val_logits_ll: 0.01793 |  0:01:15s\n",
      "epoch 45 | loss: 0.01533 | val_logits_ll: 0.01789 |  0:01:16s\n",
      "epoch 46 | loss: 0.01531 | val_logits_ll: 0.01793 |  0:01:18s\n",
      "epoch 47 | loss: 0.01523 | val_logits_ll: 0.01804 |  0:01:20s\n",
      "epoch 48 | loss: 0.01529 | val_logits_ll: 0.01792 |  0:01:21s\n",
      "epoch 49 | loss: 0.01529 | val_logits_ll: 0.01816 |  0:01:23s\n",
      "epoch 50 | loss: 0.0152  | val_logits_ll: 0.01775 |  0:01:25s\n",
      "epoch 51 | loss: 0.01458 | val_logits_ll: 0.01767 |  0:01:26s\n",
      "epoch 52 | loss: 0.01406 | val_logits_ll: 0.0177  |  0:01:28s\n",
      "epoch 53 | loss: 0.0138  | val_logits_ll: 0.0178  |  0:01:29s\n",
      "epoch 54 | loss: 0.01363 | val_logits_ll: 0.0179  |  0:01:31s\n",
      "epoch 55 | loss: 0.01347 | val_logits_ll: 0.01793 |  0:01:33s\n",
      "epoch 56 | loss: 0.01334 | val_logits_ll: 0.01806 |  0:01:35s\n",
      "epoch 57 | loss: 0.01324 | val_logits_ll: 0.01818 |  0:01:36s\n",
      "epoch 58 | loss: 0.01309 | val_logits_ll: 0.01824 |  0:01:38s\n",
      "epoch 59 | loss: 0.01297 | val_logits_ll: 0.01833 |  0:01:40s\n",
      "epoch 60 | loss: 0.01286 | val_logits_ll: 0.01846 |  0:01:42s\n",
      "epoch 61 | loss: 0.01271 | val_logits_ll: 0.01861 |  0:01:44s\n",
      "epoch 62 | loss: 0.01263 | val_logits_ll: 0.01866 |  0:01:45s\n",
      "epoch 63 | loss: 0.01235 | val_logits_ll: 0.01868 |  0:01:47s\n",
      "epoch 64 | loss: 0.01223 | val_logits_ll: 0.01872 |  0:01:49s\n",
      "epoch 65 | loss: 0.01218 | val_logits_ll: 0.01878 |  0:01:50s\n",
      "epoch 66 | loss: 0.01213 | val_logits_ll: 0.01879 |  0:01:52s\n",
      "epoch 67 | loss: 0.01209 | val_logits_ll: 0.01882 |  0:01:54s\n",
      "epoch 68 | loss: 0.01207 | val_logits_ll: 0.01887 |  0:01:55s\n",
      "epoch 69 | loss: 0.01204 | val_logits_ll: 0.01887 |  0:01:57s\n",
      "epoch 70 | loss: 0.012   | val_logits_ll: 0.01891 |  0:01:58s\n",
      "epoch 71 | loss: 0.01199 | val_logits_ll: 0.01892 |  0:02:00s\n",
      "epoch 72 | loss: 0.01196 | val_logits_ll: 0.01896 |  0:02:01s\n",
      "epoch 73 | loss: 0.01194 | val_logits_ll: 0.019   |  0:02:03s\n",
      "epoch 74 | loss: 0.01188 | val_logits_ll: 0.019   |  0:02:05s\n",
      "epoch 75 | loss: 0.01188 | val_logits_ll: 0.019   |  0:02:07s\n",
      "epoch 76 | loss: 0.01187 | val_logits_ll: 0.01901 |  0:02:08s\n",
      "epoch 77 | loss: 0.01186 | val_logits_ll: 0.01901 |  0:02:10s\n",
      "epoch 78 | loss: 0.01188 | val_logits_ll: 0.019   |  0:02:12s\n",
      "epoch 79 | loss: 0.01187 | val_logits_ll: 0.01901 |  0:02:13s\n",
      "epoch 80 | loss: 0.01187 | val_logits_ll: 0.01902 |  0:02:15s\n",
      "epoch 81 | loss: 0.01186 | val_logits_ll: 0.01902 |  0:02:17s\n",
      "epoch 82 | loss: 0.01185 | val_logits_ll: 0.01901 |  0:02:19s\n",
      "epoch 83 | loss: 0.01185 | val_logits_ll: 0.01902 |  0:02:20s\n",
      "epoch 84 | loss: 0.01186 | val_logits_ll: 0.01905 |  0:02:22s\n",
      "epoch 85 | loss: 0.01185 | val_logits_ll: 0.01903 |  0:02:23s\n",
      "epoch 86 | loss: 0.01184 | val_logits_ll: 0.01904 |  0:02:25s\n",
      "epoch 87 | loss: 0.01185 | val_logits_ll: 0.01904 |  0:02:27s\n",
      "epoch 88 | loss: 0.01186 | val_logits_ll: 0.01902 |  0:02:28s\n",
      "epoch 89 | loss: 0.01186 | val_logits_ll: 0.01903 |  0:02:30s\n",
      "epoch 90 | loss: 0.01185 | val_logits_ll: 0.01903 |  0:02:31s\n",
      "epoch 91 | loss: 0.01184 | val_logits_ll: 0.01902 |  0:02:33s\n",
      "epoch 92 | loss: 0.01185 | val_logits_ll: 0.01904 |  0:02:35s\n",
      "epoch 93 | loss: 0.01184 | val_logits_ll: 0.01903 |  0:02:36s\n",
      "epoch 94 | loss: 0.01184 | val_logits_ll: 0.01903 |  0:02:39s\n",
      "epoch 95 | loss: 0.01184 | val_logits_ll: 0.01902 |  0:02:40s\n",
      "epoch 96 | loss: 0.01185 | val_logits_ll: 0.01903 |  0:02:42s\n",
      "epoch 97 | loss: 0.01185 | val_logits_ll: 0.01903 |  0:02:44s\n",
      "epoch 98 | loss: 0.01185 | val_logits_ll: 0.01902 |  0:02:46s\n",
      "epoch 99 | loss: 0.01182 | val_logits_ll: 0.01903 |  0:02:48s\n",
      "epoch 100| loss: 0.01184 | val_logits_ll: 0.01902 |  0:02:49s\n",
      "epoch 101| loss: 0.01186 | val_logits_ll: 0.01904 |  0:02:51s\n",
      "\n",
      "Early stopping occured at epoch 101 with best_epoch = 51 and best_val_logits_ll = 0.01767\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold6_1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40139 | val_logits_ll: 0.09448 |  0:00:01s\n",
      "epoch 1  | loss: 0.03257 | val_logits_ll: 0.02804 |  0:00:03s\n",
      "epoch 2  | loss: 0.02411 | val_logits_ll: 0.0232  |  0:00:04s\n",
      "epoch 3  | loss: 0.0214  | val_logits_ll: 0.02197 |  0:00:06s\n",
      "epoch 4  | loss: 0.0206  | val_logits_ll: 0.02137 |  0:00:08s\n",
      "epoch 5  | loss: 0.02018 | val_logits_ll: 0.02108 |  0:00:09s\n",
      "epoch 6  | loss: 0.01993 | val_logits_ll: 0.02132 |  0:00:11s\n",
      "epoch 7  | loss: 0.0197  | val_logits_ll: 0.02066 |  0:00:12s\n",
      "epoch 8  | loss: 0.01951 | val_logits_ll: 0.02055 |  0:00:14s\n",
      "epoch 9  | loss: 0.01932 | val_logits_ll: 0.02045 |  0:00:16s\n",
      "epoch 10 | loss: 0.01912 | val_logits_ll: 0.0203  |  0:00:18s\n",
      "epoch 11 | loss: 0.01883 | val_logits_ll: 0.02002 |  0:00:19s\n",
      "epoch 12 | loss: 0.0184  | val_logits_ll: 0.01986 |  0:00:21s\n",
      "epoch 13 | loss: 0.01801 | val_logits_ll: 0.01961 |  0:00:23s\n",
      "epoch 14 | loss: 0.01778 | val_logits_ll: 0.01962 |  0:00:24s\n",
      "epoch 15 | loss: 0.01761 | val_logits_ll: 0.01927 |  0:00:26s\n",
      "epoch 16 | loss: 0.01751 | val_logits_ll: 0.01903 |  0:00:28s\n",
      "epoch 17 | loss: 0.01731 | val_logits_ll: 0.02117 |  0:00:29s\n",
      "epoch 18 | loss: 0.01726 | val_logits_ll: 0.02113 |  0:00:31s\n",
      "epoch 19 | loss: 0.01706 | val_logits_ll: 0.01934 |  0:00:32s\n",
      "epoch 20 | loss: 0.01687 | val_logits_ll: 0.01884 |  0:00:34s\n",
      "epoch 21 | loss: 0.01693 | val_logits_ll: 0.01886 |  0:00:36s\n",
      "epoch 22 | loss: 0.01675 | val_logits_ll: 0.01885 |  0:00:37s\n",
      "epoch 23 | loss: 0.01668 | val_logits_ll: 0.0192  |  0:00:39s\n",
      "epoch 24 | loss: 0.01669 | val_logits_ll: 0.02069 |  0:00:40s\n",
      "epoch 25 | loss: 0.01655 | val_logits_ll: 0.01858 |  0:00:42s\n",
      "epoch 26 | loss: 0.0164  | val_logits_ll: 0.01935 |  0:00:44s\n",
      "epoch 27 | loss: 0.01636 | val_logits_ll: 0.02077 |  0:00:46s\n",
      "epoch 28 | loss: 0.01641 | val_logits_ll: 0.02016 |  0:00:47s\n",
      "epoch 29 | loss: 0.01628 | val_logits_ll: 0.01969 |  0:00:49s\n",
      "epoch 30 | loss: 0.01609 | val_logits_ll: 0.01891 |  0:00:51s\n",
      "epoch 31 | loss: 0.01606 | val_logits_ll: 0.01885 |  0:00:53s\n",
      "epoch 32 | loss: 0.01602 | val_logits_ll: 0.01821 |  0:00:55s\n",
      "epoch 33 | loss: 0.01594 | val_logits_ll: 0.01877 |  0:00:56s\n",
      "epoch 34 | loss: 0.01597 | val_logits_ll: 0.01905 |  0:00:58s\n",
      "epoch 35 | loss: 0.01591 | val_logits_ll: 0.01835 |  0:00:59s\n",
      "epoch 36 | loss: 0.0157  | val_logits_ll: 0.01847 |  0:01:01s\n",
      "epoch 37 | loss: 0.01576 | val_logits_ll: 0.01868 |  0:01:03s\n",
      "epoch 38 | loss: 0.01569 | val_logits_ll: 0.0184  |  0:01:04s\n",
      "epoch 39 | loss: 0.01551 | val_logits_ll: 0.01838 |  0:01:06s\n",
      "epoch 40 | loss: 0.01558 | val_logits_ll: 0.01823 |  0:01:08s\n",
      "epoch 41 | loss: 0.01558 | val_logits_ll: 0.01835 |  0:01:09s\n",
      "epoch 42 | loss: 0.01544 | val_logits_ll: 0.01856 |  0:01:11s\n",
      "epoch 43 | loss: 0.01547 | val_logits_ll: 0.01865 |  0:01:13s\n",
      "epoch 44 | loss: 0.01502 | val_logits_ll: 0.01812 |  0:01:14s\n",
      "epoch 45 | loss: 0.01453 | val_logits_ll: 0.01806 |  0:01:16s\n",
      "epoch 46 | loss: 0.01428 | val_logits_ll: 0.01816 |  0:01:17s\n",
      "epoch 47 | loss: 0.0141  | val_logits_ll: 0.01823 |  0:01:19s\n",
      "epoch 48 | loss: 0.01393 | val_logits_ll: 0.01833 |  0:01:20s\n",
      "epoch 49 | loss: 0.01377 | val_logits_ll: 0.01842 |  0:01:22s\n",
      "epoch 50 | loss: 0.01363 | val_logits_ll: 0.01855 |  0:01:24s\n",
      "epoch 51 | loss: 0.01352 | val_logits_ll: 0.01861 |  0:01:26s\n",
      "epoch 52 | loss: 0.01335 | val_logits_ll: 0.01873 |  0:01:28s\n",
      "epoch 53 | loss: 0.01325 | val_logits_ll: 0.01881 |  0:01:29s\n",
      "epoch 54 | loss: 0.01313 | val_logits_ll: 0.01902 |  0:01:31s\n",
      "epoch 55 | loss: 0.013   | val_logits_ll: 0.01903 |  0:01:32s\n",
      "epoch 56 | loss: 0.01288 | val_logits_ll: 0.01949 |  0:01:34s\n",
      "epoch 57 | loss: 0.01259 | val_logits_ll: 0.01928 |  0:01:36s\n",
      "epoch 58 | loss: 0.01244 | val_logits_ll: 0.01929 |  0:01:37s\n",
      "epoch 59 | loss: 0.01238 | val_logits_ll: 0.01933 |  0:01:39s\n",
      "epoch 60 | loss: 0.01234 | val_logits_ll: 0.01935 |  0:01:40s\n",
      "epoch 61 | loss: 0.01232 | val_logits_ll: 0.01941 |  0:01:42s\n",
      "epoch 62 | loss: 0.01226 | val_logits_ll: 0.01942 |  0:01:44s\n",
      "epoch 63 | loss: 0.01223 | val_logits_ll: 0.01945 |  0:01:46s\n",
      "epoch 64 | loss: 0.0122  | val_logits_ll: 0.01946 |  0:01:48s\n",
      "epoch 65 | loss: 0.01217 | val_logits_ll: 0.01952 |  0:01:49s\n",
      "epoch 66 | loss: 0.01215 | val_logits_ll: 0.01953 |  0:01:51s\n",
      "epoch 67 | loss: 0.01211 | val_logits_ll: 0.01956 |  0:01:53s\n",
      "epoch 68 | loss: 0.01207 | val_logits_ll: 0.01957 |  0:01:54s\n",
      "epoch 69 | loss: 0.01207 | val_logits_ll: 0.01958 |  0:01:56s\n",
      "epoch 70 | loss: 0.01205 | val_logits_ll: 0.01957 |  0:01:58s\n",
      "epoch 71 | loss: 0.01207 | val_logits_ll: 0.01958 |  0:02:00s\n",
      "epoch 72 | loss: 0.01207 | val_logits_ll: 0.01958 |  0:02:01s\n",
      "epoch 73 | loss: 0.01207 | val_logits_ll: 0.01959 |  0:02:03s\n",
      "epoch 74 | loss: 0.01205 | val_logits_ll: 0.01959 |  0:02:05s\n",
      "epoch 75 | loss: 0.01206 | val_logits_ll: 0.01959 |  0:02:06s\n",
      "epoch 76 | loss: 0.01205 | val_logits_ll: 0.01959 |  0:02:08s\n",
      "epoch 77 | loss: 0.01205 | val_logits_ll: 0.0196  |  0:02:09s\n",
      "epoch 78 | loss: 0.01203 | val_logits_ll: 0.01961 |  0:02:11s\n",
      "epoch 79 | loss: 0.01205 | val_logits_ll: 0.0196  |  0:02:13s\n",
      "epoch 80 | loss: 0.01205 | val_logits_ll: 0.01959 |  0:02:14s\n",
      "epoch 81 | loss: 0.01203 | val_logits_ll: 0.01958 |  0:02:16s\n",
      "epoch 82 | loss: 0.01203 | val_logits_ll: 0.0196  |  0:02:17s\n",
      "epoch 83 | loss: 0.01203 | val_logits_ll: 0.01961 |  0:02:19s\n",
      "epoch 84 | loss: 0.01205 | val_logits_ll: 0.01959 |  0:02:21s\n",
      "epoch 85 | loss: 0.01204 | val_logits_ll: 0.0196  |  0:02:22s\n",
      "epoch 86 | loss: 0.01204 | val_logits_ll: 0.01961 |  0:02:24s\n",
      "epoch 87 | loss: 0.01203 | val_logits_ll: 0.01961 |  0:02:26s\n",
      "epoch 88 | loss: 0.01201 | val_logits_ll: 0.01961 |  0:02:27s\n",
      "epoch 89 | loss: 0.01206 | val_logits_ll: 0.01959 |  0:02:29s\n",
      "epoch 90 | loss: 0.01204 | val_logits_ll: 0.01962 |  0:02:31s\n",
      "epoch 91 | loss: 0.01203 | val_logits_ll: 0.0196  |  0:02:33s\n",
      "epoch 92 | loss: 0.01204 | val_logits_ll: 0.0196  |  0:02:35s\n",
      "epoch 93 | loss: 0.01205 | val_logits_ll: 0.01959 |  0:02:36s\n",
      "epoch 94 | loss: 0.01203 | val_logits_ll: 0.01961 |  0:02:38s\n",
      "epoch 95 | loss: 0.01205 | val_logits_ll: 0.01961 |  0:02:39s\n",
      "\n",
      "Early stopping occured at epoch 95 with best_epoch = 45 and best_val_logits_ll = 0.01806\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold0_2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40212 | val_logits_ll: 0.10221 |  0:00:01s\n",
      "epoch 1  | loss: 0.03309 | val_logits_ll: 0.02684 |  0:00:03s\n",
      "epoch 2  | loss: 0.02436 | val_logits_ll: 0.02217 |  0:00:05s\n",
      "epoch 3  | loss: 0.02171 | val_logits_ll: 0.0211  |  0:00:07s\n",
      "epoch 4  | loss: 0.02079 | val_logits_ll: 0.02065 |  0:00:08s\n",
      "epoch 5  | loss: 0.0204  | val_logits_ll: 0.02038 |  0:00:10s\n",
      "epoch 6  | loss: 0.02009 | val_logits_ll: 0.02021 |  0:00:12s\n",
      "epoch 7  | loss: 0.01985 | val_logits_ll: 0.02004 |  0:00:13s\n",
      "epoch 8  | loss: 0.01956 | val_logits_ll: 0.01971 |  0:00:15s\n",
      "epoch 9  | loss: 0.01928 | val_logits_ll: 0.01947 |  0:00:17s\n",
      "epoch 10 | loss: 0.01898 | val_logits_ll: 0.01944 |  0:00:19s\n",
      "epoch 11 | loss: 0.01866 | val_logits_ll: 0.01915 |  0:00:20s\n",
      "epoch 12 | loss: 0.01832 | val_logits_ll: 0.01891 |  0:00:22s\n",
      "epoch 13 | loss: 0.01808 | val_logits_ll: 0.01868 |  0:00:24s\n",
      "epoch 14 | loss: 0.01778 | val_logits_ll: 0.01867 |  0:00:25s\n",
      "epoch 15 | loss: 0.01764 | val_logits_ll: 0.01858 |  0:00:27s\n",
      "epoch 16 | loss: 0.01746 | val_logits_ll: 0.01838 |  0:00:29s\n",
      "epoch 17 | loss: 0.01725 | val_logits_ll: 0.02082 |  0:00:30s\n",
      "epoch 18 | loss: 0.01714 | val_logits_ll: 0.01875 |  0:00:32s\n",
      "epoch 19 | loss: 0.01707 | val_logits_ll: 0.01842 |  0:00:34s\n",
      "epoch 20 | loss: 0.01696 | val_logits_ll: 0.01848 |  0:00:35s\n",
      "epoch 21 | loss: 0.01681 | val_logits_ll: 0.01874 |  0:00:37s\n",
      "epoch 22 | loss: 0.01665 | val_logits_ll: 0.01826 |  0:00:38s\n",
      "epoch 23 | loss: 0.0165  | val_logits_ll: 0.01822 |  0:00:40s\n",
      "epoch 24 | loss: 0.01647 | val_logits_ll: 0.0182  |  0:00:42s\n",
      "epoch 25 | loss: 0.01638 | val_logits_ll: 0.01786 |  0:00:43s\n",
      "epoch 26 | loss: 0.01618 | val_logits_ll: 0.01802 |  0:00:45s\n",
      "epoch 27 | loss: 0.01603 | val_logits_ll: 0.01822 |  0:00:46s\n",
      "epoch 28 | loss: 0.01598 | val_logits_ll: 0.01838 |  0:00:48s\n",
      "epoch 29 | loss: 0.016   | val_logits_ll: 0.01808 |  0:00:50s\n",
      "epoch 30 | loss: 0.01586 | val_logits_ll: 0.01788 |  0:00:52s\n",
      "epoch 31 | loss: 0.01588 | val_logits_ll: 0.01851 |  0:00:53s\n",
      "epoch 32 | loss: 0.01582 | val_logits_ll: 0.01793 |  0:00:55s\n",
      "epoch 33 | loss: 0.01573 | val_logits_ll: 0.01875 |  0:00:57s\n",
      "epoch 34 | loss: 0.0157  | val_logits_ll: 0.01793 |  0:00:58s\n",
      "epoch 35 | loss: 0.01563 | val_logits_ll: 0.01785 |  0:01:00s\n",
      "epoch 36 | loss: 0.0156  | val_logits_ll: 0.02018 |  0:01:02s\n",
      "epoch 37 | loss: 0.01552 | val_logits_ll: 0.01801 |  0:01:04s\n",
      "epoch 38 | loss: 0.01555 | val_logits_ll: 0.01771 |  0:01:06s\n",
      "epoch 39 | loss: 0.01537 | val_logits_ll: 0.01781 |  0:01:07s\n",
      "epoch 40 | loss: 0.01538 | val_logits_ll: 0.01805 |  0:01:09s\n",
      "epoch 41 | loss: 0.01539 | val_logits_ll: 0.01768 |  0:01:11s\n",
      "epoch 42 | loss: 0.01529 | val_logits_ll: 0.01781 |  0:01:12s\n",
      "epoch 43 | loss: 0.01535 | val_logits_ll: 0.01798 |  0:01:14s\n",
      "epoch 44 | loss: 0.01524 | val_logits_ll: 0.01776 |  0:01:16s\n",
      "epoch 45 | loss: 0.01519 | val_logits_ll: 0.01785 |  0:01:17s\n",
      "epoch 46 | loss: 0.01519 | val_logits_ll: 0.01787 |  0:01:19s\n",
      "epoch 47 | loss: 0.01522 | val_logits_ll: 0.01785 |  0:01:21s\n",
      "epoch 48 | loss: 0.01514 | val_logits_ll: 0.01785 |  0:01:22s\n",
      "epoch 49 | loss: 0.01518 | val_logits_ll: 0.0179  |  0:01:24s\n",
      "epoch 50 | loss: 0.01516 | val_logits_ll: 0.01763 |  0:01:26s\n",
      "epoch 51 | loss: 0.01501 | val_logits_ll: 0.01812 |  0:01:28s\n",
      "epoch 52 | loss: 0.01513 | val_logits_ll: 0.01868 |  0:01:29s\n",
      "epoch 53 | loss: 0.01519 | val_logits_ll: 0.01785 |  0:01:31s\n",
      "epoch 54 | loss: 0.01503 | val_logits_ll: 0.01776 |  0:01:33s\n",
      "epoch 55 | loss: 0.01499 | val_logits_ll: 0.01788 |  0:01:34s\n",
      "epoch 56 | loss: 0.01488 | val_logits_ll: 0.01791 |  0:01:36s\n",
      "epoch 57 | loss: 0.01503 | val_logits_ll: 0.018   |  0:01:38s\n",
      "epoch 58 | loss: 0.01506 | val_logits_ll: 0.01778 |  0:01:39s\n",
      "epoch 59 | loss: 0.01491 | val_logits_ll: 0.01803 |  0:01:41s\n",
      "epoch 60 | loss: 0.01496 | val_logits_ll: 0.01785 |  0:01:42s\n",
      "epoch 61 | loss: 0.015   | val_logits_ll: 0.01778 |  0:01:44s\n",
      "epoch 62 | loss: 0.01439 | val_logits_ll: 0.01748 |  0:01:45s\n",
      "epoch 63 | loss: 0.01379 | val_logits_ll: 0.01754 |  0:01:47s\n",
      "epoch 64 | loss: 0.01349 | val_logits_ll: 0.01765 |  0:01:49s\n",
      "epoch 65 | loss: 0.01329 | val_logits_ll: 0.01774 |  0:01:51s\n",
      "epoch 66 | loss: 0.01312 | val_logits_ll: 0.01786 |  0:01:52s\n",
      "epoch 67 | loss: 0.01295 | val_logits_ll: 0.01788 |  0:01:54s\n",
      "epoch 68 | loss: 0.01282 | val_logits_ll: 0.01801 |  0:01:56s\n",
      "epoch 69 | loss: 0.01267 | val_logits_ll: 0.01815 |  0:01:57s\n",
      "epoch 70 | loss: 0.01258 | val_logits_ll: 0.01825 |  0:02:00s\n",
      "epoch 71 | loss: 0.01242 | val_logits_ll: 0.0184  |  0:02:01s\n",
      "epoch 72 | loss: 0.01231 | val_logits_ll: 0.01852 |  0:02:03s\n",
      "epoch 73 | loss: 0.01221 | val_logits_ll: 0.0187  |  0:02:05s\n",
      "epoch 74 | loss: 0.01192 | val_logits_ll: 0.01873 |  0:02:07s\n",
      "epoch 75 | loss: 0.01179 | val_logits_ll: 0.01869 |  0:02:08s\n",
      "epoch 76 | loss: 0.01175 | val_logits_ll: 0.01877 |  0:02:10s\n",
      "epoch 77 | loss: 0.0117  | val_logits_ll: 0.01879 |  0:02:12s\n",
      "epoch 78 | loss: 0.01165 | val_logits_ll: 0.01881 |  0:02:14s\n",
      "epoch 79 | loss: 0.01163 | val_logits_ll: 0.01882 |  0:02:15s\n",
      "epoch 80 | loss: 0.01159 | val_logits_ll: 0.01885 |  0:02:17s\n",
      "epoch 81 | loss: 0.01157 | val_logits_ll: 0.01889 |  0:02:18s\n",
      "epoch 82 | loss: 0.01154 | val_logits_ll: 0.01889 |  0:02:20s\n",
      "epoch 83 | loss: 0.01152 | val_logits_ll: 0.01894 |  0:02:22s\n",
      "epoch 84 | loss: 0.01151 | val_logits_ll: 0.01894 |  0:02:23s\n",
      "epoch 85 | loss: 0.01146 | val_logits_ll: 0.01898 |  0:02:25s\n",
      "epoch 86 | loss: 0.01144 | val_logits_ll: 0.01897 |  0:02:27s\n",
      "epoch 87 | loss: 0.01144 | val_logits_ll: 0.01894 |  0:02:28s\n",
      "epoch 88 | loss: 0.01143 | val_logits_ll: 0.01896 |  0:02:30s\n",
      "epoch 89 | loss: 0.01144 | val_logits_ll: 0.01899 |  0:02:32s\n",
      "epoch 90 | loss: 0.01142 | val_logits_ll: 0.01899 |  0:02:34s\n",
      "epoch 91 | loss: 0.01143 | val_logits_ll: 0.01898 |  0:02:35s\n",
      "epoch 92 | loss: 0.01141 | val_logits_ll: 0.01899 |  0:02:37s\n",
      "epoch 93 | loss: 0.01143 | val_logits_ll: 0.01899 |  0:02:39s\n",
      "epoch 94 | loss: 0.01142 | val_logits_ll: 0.019   |  0:02:40s\n",
      "epoch 95 | loss: 0.01143 | val_logits_ll: 0.01899 |  0:02:42s\n",
      "epoch 96 | loss: 0.0114  | val_logits_ll: 0.019   |  0:02:44s\n",
      "epoch 97 | loss: 0.0114  | val_logits_ll: 0.019   |  0:02:45s\n",
      "epoch 98 | loss: 0.01141 | val_logits_ll: 0.01898 |  0:02:47s\n",
      "epoch 99 | loss: 0.01141 | val_logits_ll: 0.019   |  0:02:49s\n",
      "epoch 100| loss: 0.01142 | val_logits_ll: 0.01901 |  0:02:50s\n",
      "epoch 101| loss: 0.0114  | val_logits_ll: 0.01899 |  0:02:52s\n",
      "epoch 102| loss: 0.01142 | val_logits_ll: 0.019   |  0:02:53s\n",
      "epoch 103| loss: 0.01142 | val_logits_ll: 0.019   |  0:02:55s\n",
      "epoch 104| loss: 0.01141 | val_logits_ll: 0.01902 |  0:02:57s\n",
      "epoch 105| loss: 0.0114  | val_logits_ll: 0.01901 |  0:02:59s\n",
      "epoch 106| loss: 0.0114  | val_logits_ll: 0.019   |  0:03:00s\n",
      "epoch 107| loss: 0.01142 | val_logits_ll: 0.01902 |  0:03:02s\n",
      "epoch 108| loss: 0.0114  | val_logits_ll: 0.01901 |  0:03:04s\n",
      "epoch 109| loss: 0.01143 | val_logits_ll: 0.01899 |  0:03:06s\n",
      "epoch 110| loss: 0.01141 | val_logits_ll: 0.01899 |  0:03:08s\n",
      "epoch 111| loss: 0.01141 | val_logits_ll: 0.01901 |  0:03:10s\n",
      "epoch 112| loss: 0.01141 | val_logits_ll: 0.01902 |  0:03:11s\n",
      "\n",
      "Early stopping occured at epoch 112 with best_epoch = 62 and best_val_logits_ll = 0.01748\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold1_2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40478 | val_logits_ll: 0.09617 |  0:00:01s\n",
      "epoch 1  | loss: 0.03246 | val_logits_ll: 0.02693 |  0:00:03s\n",
      "epoch 2  | loss: 0.02484 | val_logits_ll: 0.02201 |  0:00:05s\n",
      "epoch 3  | loss: 0.02198 | val_logits_ll: 0.02091 |  0:00:06s\n",
      "epoch 4  | loss: 0.02099 | val_logits_ll: 0.02033 |  0:00:08s\n",
      "epoch 5  | loss: 0.02051 | val_logits_ll: 0.02004 |  0:00:09s\n",
      "epoch 6  | loss: 0.02012 | val_logits_ll: 0.01978 |  0:00:11s\n",
      "epoch 7  | loss: 0.01982 | val_logits_ll: 0.01962 |  0:00:12s\n",
      "epoch 8  | loss: 0.01952 | val_logits_ll: 0.01927 |  0:00:14s\n",
      "epoch 9  | loss: 0.01905 | val_logits_ll: 0.01918 |  0:00:16s\n",
      "epoch 10 | loss: 0.01865 | val_logits_ll: 0.01885 |  0:00:18s\n",
      "epoch 11 | loss: 0.01842 | val_logits_ll: 0.01881 |  0:00:19s\n",
      "epoch 12 | loss: 0.01824 | val_logits_ll: 0.01863 |  0:00:21s\n",
      "epoch 13 | loss: 0.01802 | val_logits_ll: 0.01849 |  0:00:23s\n",
      "epoch 14 | loss: 0.01791 | val_logits_ll: 0.02033 |  0:00:25s\n",
      "epoch 15 | loss: 0.01774 | val_logits_ll: 0.0183  |  0:00:26s\n",
      "epoch 16 | loss: 0.01748 | val_logits_ll: 0.01807 |  0:00:28s\n",
      "epoch 17 | loss: 0.01734 | val_logits_ll: 0.01802 |  0:00:30s\n",
      "epoch 18 | loss: 0.01716 | val_logits_ll: 0.01971 |  0:00:31s\n",
      "epoch 19 | loss: 0.01708 | val_logits_ll: 0.01831 |  0:00:33s\n",
      "epoch 20 | loss: 0.01695 | val_logits_ll: 0.01822 |  0:00:34s\n",
      "epoch 21 | loss: 0.0168  | val_logits_ll: 0.01785 |  0:00:36s\n",
      "epoch 22 | loss: 0.0166  | val_logits_ll: 0.01785 |  0:00:38s\n",
      "epoch 23 | loss: 0.01661 | val_logits_ll: 0.02059 |  0:00:39s\n",
      "epoch 24 | loss: 0.01657 | val_logits_ll: 0.01824 |  0:00:41s\n",
      "epoch 25 | loss: 0.0165  | val_logits_ll: 0.02007 |  0:00:43s\n",
      "epoch 26 | loss: 0.01622 | val_logits_ll: 0.02016 |  0:00:44s\n",
      "epoch 27 | loss: 0.01612 | val_logits_ll: 0.01788 |  0:00:46s\n",
      "epoch 28 | loss: 0.01603 | val_logits_ll: 0.018   |  0:00:48s\n",
      "epoch 29 | loss: 0.01605 | val_logits_ll: 0.01797 |  0:00:50s\n",
      "epoch 30 | loss: 0.01598 | val_logits_ll: 0.01796 |  0:00:52s\n",
      "epoch 31 | loss: 0.0159  | val_logits_ll: 0.02232 |  0:00:54s\n",
      "epoch 32 | loss: 0.01588 | val_logits_ll: 0.01795 |  0:00:56s\n",
      "epoch 33 | loss: 0.01535 | val_logits_ll: 0.01749 |  0:00:58s\n",
      "epoch 34 | loss: 0.01491 | val_logits_ll: 0.01746 |  0:00:59s\n",
      "epoch 35 | loss: 0.01471 | val_logits_ll: 0.0175  |  0:01:01s\n",
      "epoch 36 | loss: 0.01454 | val_logits_ll: 0.01759 |  0:01:03s\n",
      "epoch 37 | loss: 0.0144  | val_logits_ll: 0.01764 |  0:01:04s\n",
      "epoch 38 | loss: 0.01427 | val_logits_ll: 0.01771 |  0:01:06s\n",
      "epoch 39 | loss: 0.01415 | val_logits_ll: 0.01785 |  0:01:07s\n",
      "epoch 40 | loss: 0.01406 | val_logits_ll: 0.01792 |  0:01:09s\n",
      "epoch 41 | loss: 0.01394 | val_logits_ll: 0.01795 |  0:01:11s\n",
      "epoch 42 | loss: 0.01385 | val_logits_ll: 0.01808 |  0:01:12s\n",
      "epoch 43 | loss: 0.01372 | val_logits_ll: 0.01815 |  0:01:14s\n",
      "epoch 44 | loss: 0.01357 | val_logits_ll: 0.01821 |  0:01:15s\n",
      "epoch 45 | loss: 0.01347 | val_logits_ll: 0.01841 |  0:01:17s\n",
      "epoch 46 | loss: 0.01318 | val_logits_ll: 0.01842 |  0:01:19s\n",
      "epoch 47 | loss: 0.01307 | val_logits_ll: 0.01843 |  0:01:20s\n",
      "epoch 48 | loss: 0.01299 | val_logits_ll: 0.01847 |  0:01:22s\n",
      "epoch 49 | loss: 0.01295 | val_logits_ll: 0.0185  |  0:01:24s\n",
      "epoch 50 | loss: 0.01293 | val_logits_ll: 0.01852 |  0:01:26s\n",
      "epoch 51 | loss: 0.01291 | val_logits_ll: 0.01856 |  0:01:28s\n",
      "epoch 52 | loss: 0.01285 | val_logits_ll: 0.01858 |  0:01:29s\n",
      "epoch 53 | loss: 0.01285 | val_logits_ll: 0.01861 |  0:01:31s\n",
      "epoch 54 | loss: 0.01281 | val_logits_ll: 0.01863 |  0:01:33s\n",
      "epoch 55 | loss: 0.01277 | val_logits_ll: 0.01865 |  0:01:34s\n",
      "epoch 56 | loss: 0.01273 | val_logits_ll: 0.01869 |  0:01:36s\n",
      "epoch 57 | loss: 0.0127  | val_logits_ll: 0.01869 |  0:01:38s\n",
      "epoch 58 | loss: 0.01267 | val_logits_ll: 0.0187  |  0:01:39s\n",
      "epoch 59 | loss: 0.01269 | val_logits_ll: 0.0187  |  0:01:41s\n",
      "epoch 60 | loss: 0.01268 | val_logits_ll: 0.01871 |  0:01:43s\n",
      "epoch 61 | loss: 0.01268 | val_logits_ll: 0.01871 |  0:01:44s\n",
      "epoch 62 | loss: 0.01267 | val_logits_ll: 0.01871 |  0:01:46s\n",
      "epoch 63 | loss: 0.01268 | val_logits_ll: 0.01871 |  0:01:47s\n",
      "epoch 64 | loss: 0.01267 | val_logits_ll: 0.01874 |  0:01:49s\n",
      "epoch 65 | loss: 0.01266 | val_logits_ll: 0.01872 |  0:01:51s\n",
      "epoch 66 | loss: 0.01265 | val_logits_ll: 0.01873 |  0:01:53s\n",
      "epoch 67 | loss: 0.01267 | val_logits_ll: 0.01873 |  0:01:55s\n",
      "epoch 68 | loss: 0.01266 | val_logits_ll: 0.01873 |  0:01:57s\n",
      "epoch 69 | loss: 0.01267 | val_logits_ll: 0.01875 |  0:01:58s\n",
      "epoch 70 | loss: 0.01266 | val_logits_ll: 0.01872 |  0:02:00s\n",
      "epoch 71 | loss: 0.01264 | val_logits_ll: 0.01874 |  0:02:02s\n",
      "epoch 72 | loss: 0.01264 | val_logits_ll: 0.01872 |  0:02:04s\n",
      "epoch 73 | loss: 0.01266 | val_logits_ll: 0.01872 |  0:02:05s\n",
      "epoch 74 | loss: 0.01265 | val_logits_ll: 0.01873 |  0:02:07s\n",
      "epoch 75 | loss: 0.01267 | val_logits_ll: 0.01874 |  0:02:08s\n",
      "epoch 76 | loss: 0.01266 | val_logits_ll: 0.01874 |  0:02:10s\n",
      "epoch 77 | loss: 0.01266 | val_logits_ll: 0.01874 |  0:02:12s\n",
      "epoch 78 | loss: 0.01266 | val_logits_ll: 0.01873 |  0:02:13s\n",
      "epoch 79 | loss: 0.01265 | val_logits_ll: 0.01873 |  0:02:15s\n",
      "epoch 80 | loss: 0.01265 | val_logits_ll: 0.01873 |  0:02:17s\n",
      "epoch 81 | loss: 0.01266 | val_logits_ll: 0.01872 |  0:02:18s\n",
      "epoch 82 | loss: 0.01266 | val_logits_ll: 0.01874 |  0:02:20s\n",
      "epoch 83 | loss: 0.01265 | val_logits_ll: 0.01873 |  0:02:21s\n",
      "epoch 84 | loss: 0.01265 | val_logits_ll: 0.01873 |  0:02:23s\n",
      "\n",
      "Early stopping occured at epoch 84 with best_epoch = 34 and best_val_logits_ll = 0.01746\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold2_2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40478 | val_logits_ll: 0.10491 |  0:00:01s\n",
      "epoch 1  | loss: 0.03401 | val_logits_ll: 0.02795 |  0:00:03s\n",
      "epoch 2  | loss: 0.02462 | val_logits_ll: 0.02276 |  0:00:05s\n",
      "epoch 3  | loss: 0.0218  | val_logits_ll: 0.02145 |  0:00:07s\n",
      "epoch 4  | loss: 0.02086 | val_logits_ll: 0.02078 |  0:00:08s\n",
      "epoch 5  | loss: 0.02037 | val_logits_ll: 0.02045 |  0:00:10s\n",
      "epoch 6  | loss: 0.01998 | val_logits_ll: 0.0202  |  0:00:12s\n",
      "epoch 7  | loss: 0.01977 | val_logits_ll: 0.02014 |  0:00:13s\n",
      "epoch 8  | loss: 0.01952 | val_logits_ll: 0.01988 |  0:00:15s\n",
      "epoch 9  | loss: 0.01927 | val_logits_ll: 0.0197  |  0:00:16s\n",
      "epoch 10 | loss: 0.01887 | val_logits_ll: 0.01966 |  0:00:18s\n",
      "epoch 11 | loss: 0.01854 | val_logits_ll: 0.02037 |  0:00:19s\n",
      "epoch 12 | loss: 0.01824 | val_logits_ll: 0.01914 |  0:00:21s\n",
      "epoch 13 | loss: 0.01807 | val_logits_ll: 0.01892 |  0:00:23s\n",
      "epoch 14 | loss: 0.01786 | val_logits_ll: 0.01897 |  0:00:25s\n",
      "epoch 15 | loss: 0.01758 | val_logits_ll: 0.01858 |  0:00:27s\n",
      "epoch 16 | loss: 0.01742 | val_logits_ll: 0.01855 |  0:00:28s\n",
      "epoch 17 | loss: 0.01727 | val_logits_ll: 0.01851 |  0:00:30s\n",
      "epoch 18 | loss: 0.01713 | val_logits_ll: 0.01836 |  0:00:31s\n",
      "epoch 19 | loss: 0.01697 | val_logits_ll: 0.01862 |  0:00:34s\n",
      "epoch 20 | loss: 0.01691 | val_logits_ll: 0.01856 |  0:00:35s\n",
      "epoch 21 | loss: 0.01692 | val_logits_ll: 0.01836 |  0:00:37s\n",
      "epoch 22 | loss: 0.0168  | val_logits_ll: 0.01832 |  0:00:39s\n",
      "epoch 23 | loss: 0.01666 | val_logits_ll: 0.0182  |  0:00:40s\n",
      "epoch 24 | loss: 0.01656 | val_logits_ll: 0.01897 |  0:00:42s\n",
      "epoch 25 | loss: 0.01651 | val_logits_ll: 0.02015 |  0:00:44s\n",
      "epoch 26 | loss: 0.01641 | val_logits_ll: 0.01876 |  0:00:45s\n",
      "epoch 27 | loss: 0.01632 | val_logits_ll: 0.01885 |  0:00:47s\n",
      "epoch 28 | loss: 0.01632 | val_logits_ll: 0.01828 |  0:00:49s\n",
      "epoch 29 | loss: 0.01643 | val_logits_ll: 0.0183  |  0:00:50s\n",
      "epoch 30 | loss: 0.01626 | val_logits_ll: 0.02071 |  0:00:52s\n",
      "epoch 31 | loss: 0.01611 | val_logits_ll: 0.01836 |  0:00:53s\n",
      "epoch 32 | loss: 0.0161  | val_logits_ll: 0.01813 |  0:00:55s\n",
      "epoch 33 | loss: 0.01596 | val_logits_ll: 0.01786 |  0:00:57s\n",
      "epoch 34 | loss: 0.0158  | val_logits_ll: 0.01793 |  0:00:58s\n",
      "epoch 35 | loss: 0.01584 | val_logits_ll: 0.01794 |  0:01:00s\n",
      "epoch 36 | loss: 0.01592 | val_logits_ll: 0.01797 |  0:01:01s\n",
      "epoch 37 | loss: 0.01574 | val_logits_ll: 0.01799 |  0:01:03s\n",
      "epoch 38 | loss: 0.01573 | val_logits_ll: 0.01812 |  0:01:04s\n",
      "epoch 39 | loss: 0.01566 | val_logits_ll: 0.01803 |  0:01:07s\n",
      "epoch 40 | loss: 0.01555 | val_logits_ll: 0.01774 |  0:01:08s\n",
      "epoch 41 | loss: 0.01555 | val_logits_ll: 0.01799 |  0:01:10s\n",
      "epoch 42 | loss: 0.01548 | val_logits_ll: 0.01805 |  0:01:12s\n",
      "epoch 43 | loss: 0.01551 | val_logits_ll: 0.01912 |  0:01:13s\n",
      "epoch 44 | loss: 0.0155  | val_logits_ll: 0.01803 |  0:01:15s\n",
      "epoch 45 | loss: 0.01534 | val_logits_ll: 0.01796 |  0:01:17s\n",
      "epoch 46 | loss: 0.01541 | val_logits_ll: 0.01831 |  0:01:18s\n",
      "epoch 47 | loss: 0.01543 | val_logits_ll: 0.01799 |  0:01:20s\n",
      "epoch 48 | loss: 0.01534 | val_logits_ll: 0.01802 |  0:01:22s\n",
      "epoch 49 | loss: 0.01531 | val_logits_ll: 0.01799 |  0:01:23s\n",
      "epoch 50 | loss: 0.01527 | val_logits_ll: 0.01807 |  0:01:25s\n",
      "epoch 51 | loss: 0.01523 | val_logits_ll: 0.01819 |  0:01:27s\n",
      "epoch 52 | loss: 0.01457 | val_logits_ll: 0.01775 |  0:01:29s\n",
      "epoch 53 | loss: 0.01409 | val_logits_ll: 0.01771 |  0:01:30s\n",
      "epoch 54 | loss: 0.01384 | val_logits_ll: 0.01778 |  0:01:32s\n",
      "epoch 55 | loss: 0.01365 | val_logits_ll: 0.0179  |  0:01:34s\n",
      "epoch 56 | loss: 0.01348 | val_logits_ll: 0.01804 |  0:01:35s\n",
      "epoch 57 | loss: 0.01337 | val_logits_ll: 0.01814 |  0:01:37s\n",
      "epoch 58 | loss: 0.01322 | val_logits_ll: 0.01824 |  0:01:39s\n",
      "epoch 59 | loss: 0.01308 | val_logits_ll: 0.01832 |  0:01:40s\n",
      "epoch 60 | loss: 0.01301 | val_logits_ll: 0.01847 |  0:01:42s\n",
      "epoch 61 | loss: 0.01285 | val_logits_ll: 0.01865 |  0:01:44s\n",
      "epoch 62 | loss: 0.01273 | val_logits_ll: 0.01872 |  0:01:45s\n",
      "epoch 63 | loss: 0.01262 | val_logits_ll: 0.01887 |  0:01:47s\n",
      "epoch 64 | loss: 0.01252 | val_logits_ll: 0.01895 |  0:01:49s\n",
      "epoch 65 | loss: 0.01223 | val_logits_ll: 0.01897 |  0:01:50s\n",
      "epoch 66 | loss: 0.01211 | val_logits_ll: 0.019   |  0:01:52s\n",
      "epoch 67 | loss: 0.01206 | val_logits_ll: 0.01907 |  0:01:54s\n",
      "epoch 68 | loss: 0.01201 | val_logits_ll: 0.0191  |  0:01:55s\n",
      "epoch 69 | loss: 0.01196 | val_logits_ll: 0.01914 |  0:01:57s\n",
      "epoch 70 | loss: 0.01193 | val_logits_ll: 0.01917 |  0:01:58s\n",
      "epoch 71 | loss: 0.01192 | val_logits_ll: 0.01923 |  0:02:00s\n",
      "epoch 72 | loss: 0.01189 | val_logits_ll: 0.01925 |  0:02:02s\n",
      "epoch 73 | loss: 0.01185 | val_logits_ll: 0.01928 |  0:02:03s\n",
      "epoch 74 | loss: 0.01183 | val_logits_ll: 0.01931 |  0:02:05s\n",
      "epoch 75 | loss: 0.01179 | val_logits_ll: 0.01932 |  0:02:06s\n",
      "epoch 76 | loss: 0.01177 | val_logits_ll: 0.01935 |  0:02:08s\n",
      "epoch 77 | loss: 0.01176 | val_logits_ll: 0.01934 |  0:02:10s\n",
      "epoch 78 | loss: 0.01174 | val_logits_ll: 0.01935 |  0:02:12s\n",
      "epoch 79 | loss: 0.01175 | val_logits_ll: 0.01937 |  0:02:13s\n",
      "epoch 80 | loss: 0.01174 | val_logits_ll: 0.01937 |  0:02:15s\n",
      "epoch 81 | loss: 0.01173 | val_logits_ll: 0.01936 |  0:02:17s\n",
      "epoch 82 | loss: 0.01172 | val_logits_ll: 0.01936 |  0:02:18s\n",
      "epoch 83 | loss: 0.01171 | val_logits_ll: 0.01938 |  0:02:20s\n",
      "epoch 84 | loss: 0.01174 | val_logits_ll: 0.01937 |  0:02:22s\n",
      "epoch 85 | loss: 0.01173 | val_logits_ll: 0.01937 |  0:02:24s\n",
      "epoch 86 | loss: 0.01172 | val_logits_ll: 0.01938 |  0:02:26s\n",
      "epoch 87 | loss: 0.01171 | val_logits_ll: 0.01937 |  0:02:27s\n",
      "epoch 88 | loss: 0.01173 | val_logits_ll: 0.01938 |  0:02:29s\n",
      "epoch 89 | loss: 0.01172 | val_logits_ll: 0.01938 |  0:02:31s\n",
      "epoch 90 | loss: 0.01173 | val_logits_ll: 0.01939 |  0:02:32s\n",
      "epoch 91 | loss: 0.01171 | val_logits_ll: 0.01937 |  0:02:34s\n",
      "epoch 92 | loss: 0.01173 | val_logits_ll: 0.01936 |  0:02:36s\n",
      "epoch 93 | loss: 0.01171 | val_logits_ll: 0.01938 |  0:02:37s\n",
      "epoch 94 | loss: 0.01171 | val_logits_ll: 0.01937 |  0:02:39s\n",
      "epoch 95 | loss: 0.01172 | val_logits_ll: 0.01938 |  0:02:41s\n",
      "epoch 96 | loss: 0.01172 | val_logits_ll: 0.01937 |  0:02:42s\n",
      "epoch 97 | loss: 0.01174 | val_logits_ll: 0.01937 |  0:02:44s\n",
      "epoch 98 | loss: 0.0117  | val_logits_ll: 0.01938 |  0:02:46s\n",
      "epoch 99 | loss: 0.0117  | val_logits_ll: 0.01938 |  0:02:48s\n",
      "epoch 100| loss: 0.01172 | val_logits_ll: 0.01938 |  0:02:49s\n",
      "epoch 101| loss: 0.01172 | val_logits_ll: 0.01938 |  0:02:51s\n",
      "epoch 102| loss: 0.01171 | val_logits_ll: 0.01939 |  0:02:53s\n",
      "epoch 103| loss: 0.0117  | val_logits_ll: 0.01938 |  0:02:54s\n",
      "\n",
      "Early stopping occured at epoch 103 with best_epoch = 53 and best_val_logits_ll = 0.01771\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold3_2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3996  | val_logits_ll: 0.10434 |  0:00:01s\n",
      "epoch 1  | loss: 0.03276 | val_logits_ll: 0.0256  |  0:00:03s\n",
      "epoch 2  | loss: 0.02369 | val_logits_ll: 0.02165 |  0:00:04s\n",
      "epoch 3  | loss: 0.02169 | val_logits_ll: 0.02093 |  0:00:06s\n",
      "epoch 4  | loss: 0.02094 | val_logits_ll: 0.0205  |  0:00:07s\n",
      "epoch 5  | loss: 0.02054 | val_logits_ll: 0.02036 |  0:00:09s\n",
      "epoch 6  | loss: 0.02019 | val_logits_ll: 0.01994 |  0:00:11s\n",
      "epoch 7  | loss: 0.01984 | val_logits_ll: 0.01981 |  0:00:12s\n",
      "epoch 8  | loss: 0.0195  | val_logits_ll: 0.01951 |  0:00:14s\n",
      "epoch 9  | loss: 0.01911 | val_logits_ll: 0.01922 |  0:00:16s\n",
      "epoch 10 | loss: 0.01878 | val_logits_ll: 0.01903 |  0:00:17s\n",
      "epoch 11 | loss: 0.01848 | val_logits_ll: 0.01901 |  0:00:19s\n",
      "epoch 12 | loss: 0.0182  | val_logits_ll: 0.01865 |  0:00:21s\n",
      "epoch 13 | loss: 0.01796 | val_logits_ll: 0.0186  |  0:00:23s\n",
      "epoch 14 | loss: 0.01773 | val_logits_ll: 0.01836 |  0:00:25s\n",
      "epoch 15 | loss: 0.0175  | val_logits_ll: 0.01854 |  0:00:26s\n",
      "epoch 16 | loss: 0.0174  | val_logits_ll: 0.01844 |  0:00:28s\n",
      "epoch 17 | loss: 0.01724 | val_logits_ll: 0.01874 |  0:00:30s\n",
      "epoch 18 | loss: 0.01723 | val_logits_ll: 0.01897 |  0:00:32s\n",
      "epoch 19 | loss: 0.01708 | val_logits_ll: 0.02197 |  0:00:34s\n",
      "epoch 20 | loss: 0.01686 | val_logits_ll: 0.01795 |  0:00:35s\n",
      "epoch 21 | loss: 0.0167  | val_logits_ll: 0.01796 |  0:00:37s\n",
      "epoch 22 | loss: 0.01661 | val_logits_ll: 0.01857 |  0:00:39s\n",
      "epoch 23 | loss: 0.01661 | val_logits_ll: 0.01838 |  0:00:40s\n",
      "epoch 24 | loss: 0.01645 | val_logits_ll: 0.01786 |  0:00:42s\n",
      "epoch 25 | loss: 0.01632 | val_logits_ll: 0.01785 |  0:00:44s\n",
      "epoch 26 | loss: 0.01627 | val_logits_ll: 0.019   |  0:00:45s\n",
      "epoch 27 | loss: 0.01623 | val_logits_ll: 0.01765 |  0:00:47s\n",
      "epoch 28 | loss: 0.01613 | val_logits_ll: 0.01799 |  0:00:49s\n",
      "epoch 29 | loss: 0.01601 | val_logits_ll: 0.01905 |  0:00:51s\n",
      "epoch 30 | loss: 0.01599 | val_logits_ll: 0.01948 |  0:00:52s\n",
      "epoch 31 | loss: 0.01596 | val_logits_ll: 0.01788 |  0:00:55s\n",
      "epoch 32 | loss: 0.01584 | val_logits_ll: 0.0182  |  0:00:56s\n",
      "epoch 33 | loss: 0.01585 | val_logits_ll: 0.0184  |  0:00:58s\n",
      "epoch 34 | loss: 0.01579 | val_logits_ll: 0.01769 |  0:01:00s\n",
      "epoch 35 | loss: 0.01566 | val_logits_ll: 0.01776 |  0:01:01s\n",
      "epoch 36 | loss: 0.01581 | val_logits_ll: 0.01785 |  0:01:03s\n",
      "epoch 37 | loss: 0.01563 | val_logits_ll: 0.01774 |  0:01:05s\n",
      "epoch 38 | loss: 0.01555 | val_logits_ll: 0.01788 |  0:01:06s\n",
      "epoch 39 | loss: 0.01501 | val_logits_ll: 0.01746 |  0:01:08s\n",
      "epoch 40 | loss: 0.01457 | val_logits_ll: 0.01755 |  0:01:10s\n",
      "epoch 41 | loss: 0.01437 | val_logits_ll: 0.01762 |  0:01:11s\n",
      "epoch 42 | loss: 0.0142  | val_logits_ll: 0.01774 |  0:01:13s\n",
      "epoch 43 | loss: 0.01407 | val_logits_ll: 0.0178  |  0:01:15s\n",
      "epoch 44 | loss: 0.01395 | val_logits_ll: 0.01788 |  0:01:16s\n",
      "epoch 45 | loss: 0.01382 | val_logits_ll: 0.018   |  0:01:18s\n",
      "epoch 46 | loss: 0.01371 | val_logits_ll: 0.01813 |  0:01:20s\n",
      "epoch 47 | loss: 0.01359 | val_logits_ll: 0.01817 |  0:01:21s\n",
      "epoch 48 | loss: 0.01351 | val_logits_ll: 0.0183  |  0:01:23s\n",
      "epoch 49 | loss: 0.0134  | val_logits_ll: 0.01832 |  0:01:25s\n",
      "epoch 50 | loss: 0.01328 | val_logits_ll: 0.01842 |  0:01:28s\n",
      "epoch 51 | loss: 0.01306 | val_logits_ll: 0.01849 |  0:01:30s\n",
      "epoch 52 | loss: 0.01292 | val_logits_ll: 0.01853 |  0:01:31s\n",
      "epoch 53 | loss: 0.01287 | val_logits_ll: 0.01859 |  0:01:33s\n",
      "epoch 54 | loss: 0.01283 | val_logits_ll: 0.0186  |  0:01:35s\n",
      "epoch 55 | loss: 0.0128  | val_logits_ll: 0.01862 |  0:01:37s\n",
      "epoch 56 | loss: 0.01277 | val_logits_ll: 0.01865 |  0:01:38s\n",
      "epoch 57 | loss: 0.01274 | val_logits_ll: 0.01868 |  0:01:40s\n",
      "epoch 58 | loss: 0.01273 | val_logits_ll: 0.01871 |  0:01:42s\n",
      "epoch 59 | loss: 0.01269 | val_logits_ll: 0.01875 |  0:01:43s\n",
      "epoch 60 | loss: 0.01267 | val_logits_ll: 0.01877 |  0:01:45s\n",
      "epoch 61 | loss: 0.01265 | val_logits_ll: 0.01876 |  0:01:46s\n",
      "epoch 62 | loss: 0.01262 | val_logits_ll: 0.01878 |  0:01:48s\n",
      "epoch 63 | loss: 0.01259 | val_logits_ll: 0.01877 |  0:01:50s\n",
      "epoch 64 | loss: 0.01261 | val_logits_ll: 0.01879 |  0:01:51s\n",
      "epoch 65 | loss: 0.01258 | val_logits_ll: 0.01879 |  0:01:53s\n",
      "epoch 66 | loss: 0.01259 | val_logits_ll: 0.01881 |  0:01:55s\n",
      "epoch 67 | loss: 0.01259 | val_logits_ll: 0.01881 |  0:01:57s\n",
      "epoch 68 | loss: 0.0126  | val_logits_ll: 0.0188  |  0:01:58s\n",
      "epoch 69 | loss: 0.01259 | val_logits_ll: 0.0188  |  0:02:01s\n",
      "epoch 70 | loss: 0.01256 | val_logits_ll: 0.01881 |  0:02:02s\n",
      "epoch 71 | loss: 0.01257 | val_logits_ll: 0.01881 |  0:02:04s\n",
      "epoch 72 | loss: 0.01257 | val_logits_ll: 0.01881 |  0:02:05s\n",
      "epoch 73 | loss: 0.01258 | val_logits_ll: 0.01881 |  0:02:07s\n",
      "epoch 74 | loss: 0.01256 | val_logits_ll: 0.01881 |  0:02:09s\n",
      "epoch 75 | loss: 0.01256 | val_logits_ll: 0.01881 |  0:02:10s\n",
      "epoch 76 | loss: 0.01257 | val_logits_ll: 0.0188  |  0:02:12s\n",
      "epoch 77 | loss: 0.01258 | val_logits_ll: 0.01881 |  0:02:14s\n",
      "epoch 78 | loss: 0.01256 | val_logits_ll: 0.01882 |  0:02:16s\n",
      "epoch 79 | loss: 0.01255 | val_logits_ll: 0.01883 |  0:02:17s\n",
      "epoch 80 | loss: 0.01256 | val_logits_ll: 0.01881 |  0:02:19s\n",
      "epoch 81 | loss: 0.01258 | val_logits_ll: 0.01881 |  0:02:20s\n",
      "epoch 82 | loss: 0.01257 | val_logits_ll: 0.01882 |  0:02:22s\n",
      "epoch 83 | loss: 0.01257 | val_logits_ll: 0.01881 |  0:02:24s\n",
      "epoch 84 | loss: 0.01256 | val_logits_ll: 0.01881 |  0:02:26s\n",
      "epoch 85 | loss: 0.01256 | val_logits_ll: 0.0188  |  0:02:28s\n",
      "epoch 86 | loss: 0.01259 | val_logits_ll: 0.01883 |  0:02:30s\n",
      "epoch 87 | loss: 0.01256 | val_logits_ll: 0.0188  |  0:02:31s\n",
      "epoch 88 | loss: 0.01255 | val_logits_ll: 0.01881 |  0:02:34s\n",
      "epoch 89 | loss: 0.01258 | val_logits_ll: 0.01881 |  0:02:36s\n",
      "\n",
      "Early stopping occured at epoch 89 with best_epoch = 39 and best_val_logits_ll = 0.01746\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold4_2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40468 | val_logits_ll: 0.10229 |  0:00:01s\n",
      "epoch 1  | loss: 0.03359 | val_logits_ll: 0.02718 |  0:00:03s\n",
      "epoch 2  | loss: 0.02416 | val_logits_ll: 0.02231 |  0:00:04s\n",
      "epoch 3  | loss: 0.02158 | val_logits_ll: 0.02113 |  0:00:06s\n",
      "epoch 4  | loss: 0.02091 | val_logits_ll: 0.02066 |  0:00:08s\n",
      "epoch 5  | loss: 0.02042 | val_logits_ll: 0.02038 |  0:00:09s\n",
      "epoch 6  | loss: 0.02007 | val_logits_ll: 0.02021 |  0:00:11s\n",
      "epoch 7  | loss: 0.01973 | val_logits_ll: 0.01992 |  0:00:13s\n",
      "epoch 8  | loss: 0.0194  | val_logits_ll: 0.01979 |  0:00:14s\n",
      "epoch 9  | loss: 0.01894 | val_logits_ll: 0.01927 |  0:00:16s\n",
      "epoch 10 | loss: 0.01839 | val_logits_ll: 0.01886 |  0:00:18s\n",
      "epoch 11 | loss: 0.01809 | val_logits_ll: 0.01883 |  0:00:19s\n",
      "epoch 12 | loss: 0.01783 | val_logits_ll: 0.01872 |  0:00:21s\n",
      "epoch 13 | loss: 0.01756 | val_logits_ll: 0.01839 |  0:00:23s\n",
      "epoch 14 | loss: 0.01735 | val_logits_ll: 0.01854 |  0:00:24s\n",
      "epoch 15 | loss: 0.01728 | val_logits_ll: 0.01843 |  0:00:26s\n",
      "epoch 16 | loss: 0.01708 | val_logits_ll: 0.01847 |  0:00:28s\n",
      "epoch 17 | loss: 0.01689 | val_logits_ll: 0.01848 |  0:00:30s\n",
      "epoch 18 | loss: 0.01671 | val_logits_ll: 0.01837 |  0:00:31s\n",
      "epoch 19 | loss: 0.0166  | val_logits_ll: 0.01864 |  0:00:33s\n",
      "epoch 20 | loss: 0.0165  | val_logits_ll: 0.01828 |  0:00:35s\n",
      "epoch 21 | loss: 0.01637 | val_logits_ll: 0.02072 |  0:00:36s\n",
      "epoch 22 | loss: 0.01623 | val_logits_ll: 0.01823 |  0:00:38s\n",
      "epoch 23 | loss: 0.01614 | val_logits_ll: 0.01804 |  0:00:40s\n",
      "epoch 24 | loss: 0.01609 | val_logits_ll: 0.01824 |  0:00:41s\n",
      "epoch 25 | loss: 0.01608 | val_logits_ll: 0.0184  |  0:00:43s\n",
      "epoch 26 | loss: 0.016   | val_logits_ll: 0.01824 |  0:00:44s\n",
      "epoch 27 | loss: 0.01586 | val_logits_ll: 0.01816 |  0:00:46s\n",
      "epoch 28 | loss: 0.01581 | val_logits_ll: 0.02069 |  0:00:48s\n",
      "epoch 29 | loss: 0.01576 | val_logits_ll: 0.01823 |  0:00:50s\n",
      "epoch 30 | loss: 0.01573 | val_logits_ll: 0.0187  |  0:00:52s\n",
      "epoch 31 | loss: 0.01568 | val_logits_ll: 0.01827 |  0:00:54s\n",
      "epoch 32 | loss: 0.01562 | val_logits_ll: 0.01839 |  0:00:56s\n",
      "epoch 33 | loss: 0.01557 | val_logits_ll: 0.01955 |  0:00:57s\n",
      "epoch 34 | loss: 0.0155  | val_logits_ll: 0.01998 |  0:00:59s\n",
      "epoch 35 | loss: 0.01488 | val_logits_ll: 0.01803 |  0:01:01s\n",
      "epoch 36 | loss: 0.01443 | val_logits_ll: 0.01806 |  0:01:03s\n",
      "epoch 37 | loss: 0.01418 | val_logits_ll: 0.01814 |  0:01:04s\n",
      "epoch 38 | loss: 0.01399 | val_logits_ll: 0.01824 |  0:01:06s\n",
      "epoch 39 | loss: 0.01385 | val_logits_ll: 0.01835 |  0:01:08s\n",
      "epoch 40 | loss: 0.01367 | val_logits_ll: 0.01841 |  0:01:09s\n",
      "epoch 41 | loss: 0.01355 | val_logits_ll: 0.01851 |  0:01:11s\n",
      "epoch 42 | loss: 0.01341 | val_logits_ll: 0.01864 |  0:01:13s\n",
      "epoch 43 | loss: 0.01327 | val_logits_ll: 0.01872 |  0:01:14s\n",
      "epoch 44 | loss: 0.01315 | val_logits_ll: 0.01884 |  0:01:16s\n",
      "epoch 45 | loss: 0.01303 | val_logits_ll: 0.019   |  0:01:17s\n",
      "epoch 46 | loss: 0.01291 | val_logits_ll: 0.01905 |  0:01:19s\n",
      "epoch 47 | loss: 0.01266 | val_logits_ll: 0.01908 |  0:01:21s\n",
      "epoch 48 | loss: 0.01254 | val_logits_ll: 0.01912 |  0:01:22s\n",
      "epoch 49 | loss: 0.01248 | val_logits_ll: 0.01916 |  0:01:24s\n",
      "epoch 50 | loss: 0.01244 | val_logits_ll: 0.01917 |  0:01:26s\n",
      "epoch 51 | loss: 0.01239 | val_logits_ll: 0.0192  |  0:01:27s\n",
      "epoch 52 | loss: 0.01238 | val_logits_ll: 0.01923 |  0:01:29s\n",
      "epoch 53 | loss: 0.01233 | val_logits_ll: 0.01924 |  0:01:31s\n",
      "epoch 54 | loss: 0.01233 | val_logits_ll: 0.01927 |  0:01:33s\n",
      "epoch 55 | loss: 0.01229 | val_logits_ll: 0.01932 |  0:01:35s\n",
      "epoch 56 | loss: 0.01225 | val_logits_ll: 0.01933 |  0:01:36s\n",
      "epoch 57 | loss: 0.01222 | val_logits_ll: 0.01936 |  0:01:38s\n",
      "epoch 58 | loss: 0.01219 | val_logits_ll: 0.01938 |  0:01:39s\n",
      "epoch 59 | loss: 0.01221 | val_logits_ll: 0.01936 |  0:01:41s\n",
      "epoch 60 | loss: 0.01217 | val_logits_ll: 0.01936 |  0:01:42s\n",
      "epoch 61 | loss: 0.01217 | val_logits_ll: 0.01938 |  0:01:44s\n",
      "epoch 62 | loss: 0.01219 | val_logits_ll: 0.01937 |  0:01:46s\n",
      "epoch 63 | loss: 0.01216 | val_logits_ll: 0.01937 |  0:01:47s\n",
      "epoch 64 | loss: 0.01218 | val_logits_ll: 0.01938 |  0:01:49s\n",
      "epoch 65 | loss: 0.01216 | val_logits_ll: 0.01937 |  0:01:51s\n",
      "epoch 66 | loss: 0.01216 | val_logits_ll: 0.01938 |  0:01:53s\n",
      "epoch 67 | loss: 0.01216 | val_logits_ll: 0.01938 |  0:01:55s\n",
      "epoch 68 | loss: 0.01214 | val_logits_ll: 0.01938 |  0:01:57s\n",
      "epoch 69 | loss: 0.01215 | val_logits_ll: 0.01938 |  0:01:59s\n",
      "epoch 70 | loss: 0.01216 | val_logits_ll: 0.01938 |  0:02:00s\n",
      "epoch 71 | loss: 0.01216 | val_logits_ll: 0.01939 |  0:02:02s\n",
      "epoch 72 | loss: 0.01216 | val_logits_ll: 0.01938 |  0:02:03s\n",
      "epoch 73 | loss: 0.01217 | val_logits_ll: 0.01938 |  0:02:05s\n",
      "epoch 74 | loss: 0.01214 | val_logits_ll: 0.01939 |  0:02:07s\n",
      "epoch 75 | loss: 0.01215 | val_logits_ll: 0.0194  |  0:02:09s\n",
      "epoch 76 | loss: 0.01214 | val_logits_ll: 0.01938 |  0:02:10s\n",
      "epoch 77 | loss: 0.01216 | val_logits_ll: 0.01938 |  0:02:12s\n",
      "epoch 78 | loss: 0.01217 | val_logits_ll: 0.01937 |  0:02:14s\n",
      "epoch 79 | loss: 0.01214 | val_logits_ll: 0.01938 |  0:02:15s\n",
      "epoch 80 | loss: 0.01215 | val_logits_ll: 0.01938 |  0:02:17s\n",
      "epoch 81 | loss: 0.01215 | val_logits_ll: 0.0194  |  0:02:19s\n",
      "epoch 82 | loss: 0.01215 | val_logits_ll: 0.01938 |  0:02:20s\n",
      "epoch 83 | loss: 0.01214 | val_logits_ll: 0.01939 |  0:02:22s\n",
      "epoch 84 | loss: 0.01215 | val_logits_ll: 0.01939 |  0:02:23s\n",
      "epoch 85 | loss: 0.01214 | val_logits_ll: 0.01939 |  0:02:25s\n",
      "\n",
      "Early stopping occured at epoch 85 with best_epoch = 35 and best_val_logits_ll = 0.01803\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold5_2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.39811 | val_logits_ll: 0.08967 |  0:00:01s\n",
      "epoch 1  | loss: 0.03147 | val_logits_ll: 0.02671 |  0:00:03s\n",
      "epoch 2  | loss: 0.02393 | val_logits_ll: 0.02182 |  0:00:05s\n",
      "epoch 3  | loss: 0.02155 | val_logits_ll: 0.02083 |  0:00:07s\n",
      "epoch 4  | loss: 0.0208  | val_logits_ll: 0.02042 |  0:00:08s\n",
      "epoch 5  | loss: 0.02045 | val_logits_ll: 0.02013 |  0:00:10s\n",
      "epoch 6  | loss: 0.0201  | val_logits_ll: 0.02    |  0:00:12s\n",
      "epoch 7  | loss: 0.01982 | val_logits_ll: 0.01966 |  0:00:14s\n",
      "epoch 8  | loss: 0.01953 | val_logits_ll: 0.01948 |  0:00:15s\n",
      "epoch 9  | loss: 0.01923 | val_logits_ll: 0.01924 |  0:00:17s\n",
      "epoch 10 | loss: 0.01886 | val_logits_ll: 0.01907 |  0:00:18s\n",
      "epoch 11 | loss: 0.01843 | val_logits_ll: 0.01853 |  0:00:20s\n",
      "epoch 12 | loss: 0.01812 | val_logits_ll: 0.02027 |  0:00:22s\n",
      "epoch 13 | loss: 0.01778 | val_logits_ll: 0.01879 |  0:00:24s\n",
      "epoch 14 | loss: 0.01763 | val_logits_ll: 0.01815 |  0:00:25s\n",
      "epoch 15 | loss: 0.01739 | val_logits_ll: 0.01826 |  0:00:27s\n",
      "epoch 16 | loss: 0.01732 | val_logits_ll: 0.01862 |  0:00:29s\n",
      "epoch 17 | loss: 0.01714 | val_logits_ll: 0.01819 |  0:00:30s\n",
      "epoch 18 | loss: 0.01698 | val_logits_ll: 0.0179  |  0:00:32s\n",
      "epoch 19 | loss: 0.01702 | val_logits_ll: 0.01803 |  0:00:34s\n",
      "epoch 20 | loss: 0.01685 | val_logits_ll: 0.01807 |  0:00:36s\n",
      "epoch 21 | loss: 0.01671 | val_logits_ll: 0.01866 |  0:00:38s\n",
      "epoch 22 | loss: 0.01668 | val_logits_ll: 0.01985 |  0:00:39s\n",
      "epoch 23 | loss: 0.01673 | val_logits_ll: 0.01816 |  0:00:41s\n",
      "epoch 24 | loss: 0.01646 | val_logits_ll: 0.01794 |  0:00:43s\n",
      "epoch 25 | loss: 0.01638 | val_logits_ll: 0.01778 |  0:00:44s\n",
      "epoch 26 | loss: 0.01655 | val_logits_ll: 0.01793 |  0:00:46s\n",
      "epoch 27 | loss: 0.01634 | val_logits_ll: 0.01793 |  0:00:48s\n",
      "epoch 28 | loss: 0.01626 | val_logits_ll: 0.01776 |  0:00:49s\n",
      "epoch 29 | loss: 0.0162  | val_logits_ll: 0.01767 |  0:00:51s\n",
      "epoch 30 | loss: 0.01612 | val_logits_ll: 0.01776 |  0:00:53s\n",
      "epoch 31 | loss: 0.01621 | val_logits_ll: 0.01903 |  0:00:54s\n",
      "epoch 32 | loss: 0.01601 | val_logits_ll: 0.01783 |  0:00:56s\n",
      "epoch 33 | loss: 0.01603 | val_logits_ll: 0.01774 |  0:00:58s\n",
      "epoch 34 | loss: 0.01602 | val_logits_ll: 0.01773 |  0:00:59s\n",
      "epoch 35 | loss: 0.01599 | val_logits_ll: 0.0178  |  0:01:01s\n",
      "epoch 36 | loss: 0.01585 | val_logits_ll: 0.01789 |  0:01:02s\n",
      "epoch 37 | loss: 0.01586 | val_logits_ll: 0.01773 |  0:01:04s\n",
      "epoch 38 | loss: 0.0159  | val_logits_ll: 0.01759 |  0:01:05s\n",
      "epoch 39 | loss: 0.01583 | val_logits_ll: 0.01765 |  0:01:07s\n",
      "epoch 40 | loss: 0.01574 | val_logits_ll: 0.01773 |  0:01:09s\n",
      "epoch 41 | loss: 0.01563 | val_logits_ll: 0.01783 |  0:01:11s\n",
      "epoch 42 | loss: 0.01565 | val_logits_ll: 0.01766 |  0:01:13s\n",
      "epoch 43 | loss: 0.01565 | val_logits_ll: 0.01785 |  0:01:15s\n",
      "epoch 44 | loss: 0.0156  | val_logits_ll: 0.01821 |  0:01:16s\n",
      "epoch 45 | loss: 0.01565 | val_logits_ll: 0.01795 |  0:01:18s\n",
      "epoch 46 | loss: 0.01557 | val_logits_ll: 0.01802 |  0:01:20s\n",
      "epoch 47 | loss: 0.01552 | val_logits_ll: 0.01799 |  0:01:22s\n",
      "epoch 48 | loss: 0.01543 | val_logits_ll: 0.01787 |  0:01:23s\n",
      "epoch 49 | loss: 0.01546 | val_logits_ll: 0.01783 |  0:01:25s\n",
      "epoch 50 | loss: 0.01496 | val_logits_ll: 0.0176  |  0:01:27s\n",
      "epoch 51 | loss: 0.01447 | val_logits_ll: 0.01756 |  0:01:28s\n",
      "epoch 52 | loss: 0.01422 | val_logits_ll: 0.01766 |  0:01:30s\n",
      "epoch 53 | loss: 0.01405 | val_logits_ll: 0.01778 |  0:01:32s\n",
      "epoch 54 | loss: 0.01393 | val_logits_ll: 0.01781 |  0:01:34s\n",
      "epoch 55 | loss: 0.01376 | val_logits_ll: 0.01795 |  0:01:35s\n",
      "epoch 56 | loss: 0.01364 | val_logits_ll: 0.01801 |  0:01:37s\n",
      "epoch 57 | loss: 0.01353 | val_logits_ll: 0.0181  |  0:01:39s\n",
      "epoch 58 | loss: 0.0134  | val_logits_ll: 0.01824 |  0:01:41s\n",
      "epoch 59 | loss: 0.0133  | val_logits_ll: 0.01835 |  0:01:42s\n",
      "epoch 60 | loss: 0.01319 | val_logits_ll: 0.0186  |  0:01:44s\n",
      "epoch 61 | loss: 0.01306 | val_logits_ll: 0.01858 |  0:01:46s\n",
      "epoch 62 | loss: 0.01299 | val_logits_ll: 0.01877 |  0:01:48s\n",
      "epoch 63 | loss: 0.01269 | val_logits_ll: 0.01873 |  0:01:49s\n",
      "epoch 64 | loss: 0.01256 | val_logits_ll: 0.01875 |  0:01:51s\n",
      "epoch 65 | loss: 0.01251 | val_logits_ll: 0.01877 |  0:01:53s\n",
      "epoch 66 | loss: 0.01247 | val_logits_ll: 0.01882 |  0:01:54s\n",
      "epoch 67 | loss: 0.01243 | val_logits_ll: 0.01886 |  0:01:56s\n",
      "epoch 68 | loss: 0.0124  | val_logits_ll: 0.01888 |  0:01:57s\n",
      "epoch 69 | loss: 0.01239 | val_logits_ll: 0.01891 |  0:01:59s\n",
      "epoch 70 | loss: 0.01234 | val_logits_ll: 0.01893 |  0:02:01s\n",
      "epoch 71 | loss: 0.01232 | val_logits_ll: 0.01898 |  0:02:02s\n",
      "epoch 72 | loss: 0.01229 | val_logits_ll: 0.01897 |  0:02:04s\n",
      "epoch 73 | loss: 0.01228 | val_logits_ll: 0.01902 |  0:02:06s\n",
      "epoch 74 | loss: 0.01223 | val_logits_ll: 0.01902 |  0:02:07s\n",
      "epoch 75 | loss: 0.01223 | val_logits_ll: 0.01903 |  0:02:09s\n",
      "epoch 76 | loss: 0.01221 | val_logits_ll: 0.01903 |  0:02:10s\n",
      "epoch 77 | loss: 0.01223 | val_logits_ll: 0.01903 |  0:02:12s\n",
      "epoch 78 | loss: 0.01221 | val_logits_ll: 0.01904 |  0:02:14s\n",
      "epoch 79 | loss: 0.01221 | val_logits_ll: 0.01904 |  0:02:16s\n",
      "epoch 80 | loss: 0.01221 | val_logits_ll: 0.01905 |  0:02:17s\n",
      "epoch 81 | loss: 0.01222 | val_logits_ll: 0.01905 |  0:02:20s\n",
      "epoch 82 | loss: 0.0122  | val_logits_ll: 0.01905 |  0:02:22s\n",
      "epoch 83 | loss: 0.01221 | val_logits_ll: 0.01906 |  0:02:23s\n",
      "epoch 84 | loss: 0.01219 | val_logits_ll: 0.01906 |  0:02:25s\n",
      "epoch 85 | loss: 0.01218 | val_logits_ll: 0.01905 |  0:02:27s\n",
      "epoch 86 | loss: 0.01218 | val_logits_ll: 0.01906 |  0:02:29s\n",
      "epoch 87 | loss: 0.01217 | val_logits_ll: 0.01906 |  0:02:30s\n",
      "epoch 88 | loss: 0.01217 | val_logits_ll: 0.01905 |  0:02:32s\n",
      "epoch 89 | loss: 0.01218 | val_logits_ll: 0.01906 |  0:02:33s\n",
      "epoch 90 | loss: 0.01217 | val_logits_ll: 0.01904 |  0:02:35s\n",
      "epoch 91 | loss: 0.01219 | val_logits_ll: 0.01906 |  0:02:37s\n",
      "epoch 92 | loss: 0.01218 | val_logits_ll: 0.01906 |  0:02:39s\n",
      "epoch 93 | loss: 0.01218 | val_logits_ll: 0.01907 |  0:02:40s\n",
      "epoch 94 | loss: 0.01217 | val_logits_ll: 0.01907 |  0:02:42s\n",
      "epoch 95 | loss: 0.01218 | val_logits_ll: 0.01907 |  0:02:43s\n",
      "epoch 96 | loss: 0.01219 | val_logits_ll: 0.01906 |  0:02:45s\n",
      "epoch 97 | loss: 0.01217 | val_logits_ll: 0.01906 |  0:02:47s\n",
      "epoch 98 | loss: 0.0122  | val_logits_ll: 0.01906 |  0:02:49s\n",
      "epoch 99 | loss: 0.01219 | val_logits_ll: 0.01905 |  0:02:50s\n",
      "epoch 100| loss: 0.01221 | val_logits_ll: 0.01906 |  0:02:52s\n",
      "epoch 101| loss: 0.01217 | val_logits_ll: 0.01905 |  0:02:54s\n",
      "\n",
      "Early stopping occured at epoch 101 with best_epoch = 51 and best_val_logits_ll = 0.01756\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold6_2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40324 | val_logits_ll: 0.10282 |  0:00:01s\n",
      "epoch 1  | loss: 0.03246 | val_logits_ll: 0.02699 |  0:00:03s\n",
      "epoch 2  | loss: 0.02398 | val_logits_ll: 0.02245 |  0:00:05s\n",
      "epoch 3  | loss: 0.02151 | val_logits_ll: 0.02121 |  0:00:06s\n",
      "epoch 4  | loss: 0.02071 | val_logits_ll: 0.02061 |  0:00:08s\n",
      "epoch 5  | loss: 0.02036 | val_logits_ll: 0.02026 |  0:00:09s\n",
      "epoch 6  | loss: 0.02004 | val_logits_ll: 0.02016 |  0:00:11s\n",
      "epoch 7  | loss: 0.01977 | val_logits_ll: 0.01996 |  0:00:13s\n",
      "epoch 8  | loss: 0.01955 | val_logits_ll: 0.01975 |  0:00:14s\n",
      "epoch 9  | loss: 0.01922 | val_logits_ll: 0.01957 |  0:00:16s\n",
      "epoch 10 | loss: 0.01882 | val_logits_ll: 0.01908 |  0:00:17s\n",
      "epoch 11 | loss: 0.01839 | val_logits_ll: 0.01957 |  0:00:19s\n",
      "epoch 12 | loss: 0.01817 | val_logits_ll: 0.01878 |  0:00:21s\n",
      "epoch 13 | loss: 0.0179  | val_logits_ll: 0.01845 |  0:00:22s\n",
      "epoch 14 | loss: 0.01767 | val_logits_ll: 0.01848 |  0:00:25s\n",
      "epoch 15 | loss: 0.01743 | val_logits_ll: 0.01955 |  0:00:26s\n",
      "epoch 16 | loss: 0.01728 | val_logits_ll: 0.01827 |  0:00:29s\n",
      "epoch 17 | loss: 0.01728 | val_logits_ll: 0.01905 |  0:00:30s\n",
      "epoch 18 | loss: 0.0171  | val_logits_ll: 0.01811 |  0:00:32s\n",
      "epoch 19 | loss: 0.01696 | val_logits_ll: 0.01814 |  0:00:34s\n",
      "epoch 20 | loss: 0.01694 | val_logits_ll: 0.01795 |  0:00:35s\n",
      "epoch 21 | loss: 0.01669 | val_logits_ll: 0.01914 |  0:00:37s\n",
      "epoch 22 | loss: 0.01662 | val_logits_ll: 0.01827 |  0:00:39s\n",
      "epoch 23 | loss: 0.01657 | val_logits_ll: 0.01919 |  0:00:40s\n",
      "epoch 24 | loss: 0.01651 | val_logits_ll: 0.01792 |  0:00:42s\n",
      "epoch 25 | loss: 0.0164  | val_logits_ll: 0.01761 |  0:00:43s\n",
      "epoch 26 | loss: 0.01627 | val_logits_ll: 0.01784 |  0:00:45s\n",
      "epoch 27 | loss: 0.01615 | val_logits_ll: 0.0179  |  0:00:47s\n",
      "epoch 28 | loss: 0.01617 | val_logits_ll: 0.01792 |  0:00:48s\n",
      "epoch 29 | loss: 0.01628 | val_logits_ll: 0.01784 |  0:00:50s\n",
      "epoch 30 | loss: 0.01617 | val_logits_ll: 0.01782 |  0:00:52s\n",
      "epoch 31 | loss: 0.01597 | val_logits_ll: 0.01786 |  0:00:53s\n",
      "epoch 32 | loss: 0.01597 | val_logits_ll: 0.01796 |  0:00:55s\n",
      "epoch 33 | loss: 0.01604 | val_logits_ll: 0.01764 |  0:00:57s\n",
      "epoch 34 | loss: 0.01587 | val_logits_ll: 0.01824 |  0:00:59s\n",
      "epoch 35 | loss: 0.01576 | val_logits_ll: 0.01768 |  0:01:01s\n",
      "epoch 36 | loss: 0.01569 | val_logits_ll: 0.01778 |  0:01:03s\n",
      "epoch 37 | loss: 0.01524 | val_logits_ll: 0.01743 |  0:01:04s\n",
      "epoch 38 | loss: 0.01478 | val_logits_ll: 0.01746 |  0:01:06s\n",
      "epoch 39 | loss: 0.01459 | val_logits_ll: 0.01753 |  0:01:08s\n",
      "epoch 40 | loss: 0.01445 | val_logits_ll: 0.01765 |  0:01:09s\n",
      "epoch 41 | loss: 0.01429 | val_logits_ll: 0.01767 |  0:01:11s\n",
      "epoch 42 | loss: 0.01414 | val_logits_ll: 0.01774 |  0:01:12s\n",
      "epoch 43 | loss: 0.01401 | val_logits_ll: 0.01784 |  0:01:14s\n",
      "epoch 44 | loss: 0.0139  | val_logits_ll: 0.01794 |  0:01:15s\n",
      "epoch 45 | loss: 0.01379 | val_logits_ll: 0.01807 |  0:01:17s\n",
      "epoch 46 | loss: 0.01369 | val_logits_ll: 0.01817 |  0:01:19s\n",
      "epoch 47 | loss: 0.01357 | val_logits_ll: 0.01827 |  0:01:20s\n",
      "epoch 48 | loss: 0.01347 | val_logits_ll: 0.01829 |  0:01:22s\n",
      "epoch 49 | loss: 0.01324 | val_logits_ll: 0.01831 |  0:01:24s\n",
      "epoch 50 | loss: 0.0131  | val_logits_ll: 0.01835 |  0:01:26s\n",
      "epoch 51 | loss: 0.01305 | val_logits_ll: 0.01841 |  0:01:27s\n",
      "epoch 52 | loss: 0.01301 | val_logits_ll: 0.01844 |  0:01:30s\n",
      "epoch 53 | loss: 0.01297 | val_logits_ll: 0.01844 |  0:01:31s\n",
      "epoch 54 | loss: 0.01295 | val_logits_ll: 0.01849 |  0:01:34s\n",
      "epoch 55 | loss: 0.01293 | val_logits_ll: 0.01849 |  0:01:35s\n",
      "epoch 56 | loss: 0.01289 | val_logits_ll: 0.01854 |  0:01:37s\n",
      "epoch 57 | loss: 0.01287 | val_logits_ll: 0.01852 |  0:01:38s\n",
      "epoch 58 | loss: 0.01283 | val_logits_ll: 0.01858 |  0:01:40s\n",
      "epoch 59 | loss: 0.01282 | val_logits_ll: 0.01863 |  0:01:42s\n",
      "epoch 60 | loss: 0.01279 | val_logits_ll: 0.01863 |  0:01:43s\n",
      "epoch 61 | loss: 0.01278 | val_logits_ll: 0.01863 |  0:01:45s\n",
      "epoch 62 | loss: 0.01278 | val_logits_ll: 0.01862 |  0:01:47s\n",
      "epoch 63 | loss: 0.01275 | val_logits_ll: 0.01864 |  0:01:48s\n",
      "epoch 64 | loss: 0.01276 | val_logits_ll: 0.01862 |  0:01:50s\n",
      "epoch 65 | loss: 0.01276 | val_logits_ll: 0.01862 |  0:01:52s\n",
      "epoch 66 | loss: 0.01275 | val_logits_ll: 0.01864 |  0:01:53s\n",
      "epoch 67 | loss: 0.01274 | val_logits_ll: 0.01862 |  0:01:55s\n",
      "epoch 68 | loss: 0.01274 | val_logits_ll: 0.01864 |  0:01:56s\n",
      "epoch 69 | loss: 0.01276 | val_logits_ll: 0.01865 |  0:01:58s\n",
      "epoch 70 | loss: 0.01275 | val_logits_ll: 0.01865 |  0:02:00s\n",
      "epoch 71 | loss: 0.01273 | val_logits_ll: 0.01865 |  0:02:01s\n",
      "epoch 72 | loss: 0.01275 | val_logits_ll: 0.01863 |  0:02:03s\n",
      "epoch 73 | loss: 0.01274 | val_logits_ll: 0.01865 |  0:02:05s\n",
      "epoch 74 | loss: 0.01274 | val_logits_ll: 0.01863 |  0:02:07s\n",
      "epoch 75 | loss: 0.01273 | val_logits_ll: 0.01865 |  0:02:09s\n",
      "epoch 76 | loss: 0.01273 | val_logits_ll: 0.01865 |  0:02:10s\n",
      "epoch 77 | loss: 0.01273 | val_logits_ll: 0.01864 |  0:02:12s\n",
      "epoch 78 | loss: 0.01276 | val_logits_ll: 0.01866 |  0:02:14s\n",
      "epoch 79 | loss: 0.01272 | val_logits_ll: 0.01866 |  0:02:15s\n",
      "epoch 80 | loss: 0.01273 | val_logits_ll: 0.01865 |  0:02:17s\n",
      "epoch 81 | loss: 0.01273 | val_logits_ll: 0.01865 |  0:02:18s\n",
      "epoch 82 | loss: 0.01273 | val_logits_ll: 0.01865 |  0:02:20s\n",
      "epoch 83 | loss: 0.01272 | val_logits_ll: 0.01865 |  0:02:22s\n",
      "epoch 84 | loss: 0.01274 | val_logits_ll: 0.01865 |  0:02:23s\n",
      "epoch 85 | loss: 0.01273 | val_logits_ll: 0.01865 |  0:02:25s\n",
      "epoch 86 | loss: 0.01275 | val_logits_ll: 0.01865 |  0:02:27s\n",
      "epoch 87 | loss: 0.01272 | val_logits_ll: 0.01865 |  0:02:29s\n",
      "\n",
      "Early stopping occured at epoch 87 with best_epoch = 37 and best_val_logits_ll = 0.01743\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold0_3.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40182 | val_logits_ll: 0.10191 |  0:00:01s\n",
      "epoch 1  | loss: 0.03321 | val_logits_ll: 0.02743 |  0:00:03s\n",
      "epoch 2  | loss: 0.02463 | val_logits_ll: 0.02233 |  0:00:05s\n",
      "epoch 3  | loss: 0.02192 | val_logits_ll: 0.02136 |  0:00:07s\n",
      "epoch 4  | loss: 0.02116 | val_logits_ll: 0.02081 |  0:00:09s\n",
      "epoch 5  | loss: 0.02064 | val_logits_ll: 0.02045 |  0:00:10s\n",
      "epoch 6  | loss: 0.0203  | val_logits_ll: 0.02012 |  0:00:12s\n",
      "epoch 7  | loss: 0.02001 | val_logits_ll: 0.0201  |  0:00:14s\n",
      "epoch 8  | loss: 0.01978 | val_logits_ll: 0.01979 |  0:00:15s\n",
      "epoch 9  | loss: 0.01949 | val_logits_ll: 0.01948 |  0:00:17s\n",
      "epoch 10 | loss: 0.0191  | val_logits_ll: 0.0193  |  0:00:19s\n",
      "epoch 11 | loss: 0.01874 | val_logits_ll: 0.01904 |  0:00:20s\n",
      "epoch 12 | loss: 0.01841 | val_logits_ll: 0.02028 |  0:00:22s\n",
      "epoch 13 | loss: 0.01809 | val_logits_ll: 0.01883 |  0:00:23s\n",
      "epoch 14 | loss: 0.01788 | val_logits_ll: 0.02063 |  0:00:25s\n",
      "epoch 15 | loss: 0.01766 | val_logits_ll: 0.0184  |  0:00:27s\n",
      "epoch 16 | loss: 0.01743 | val_logits_ll: 0.01827 |  0:00:29s\n",
      "epoch 17 | loss: 0.01736 | val_logits_ll: 0.01836 |  0:00:30s\n",
      "epoch 18 | loss: 0.01732 | val_logits_ll: 0.01863 |  0:00:32s\n",
      "epoch 19 | loss: 0.01716 | val_logits_ll: 0.01798 |  0:00:33s\n",
      "epoch 20 | loss: 0.017   | val_logits_ll: 0.01807 |  0:00:35s\n",
      "epoch 21 | loss: 0.01693 | val_logits_ll: 0.02036 |  0:00:37s\n",
      "epoch 22 | loss: 0.01694 | val_logits_ll: 0.01831 |  0:00:39s\n",
      "epoch 23 | loss: 0.01671 | val_logits_ll: 0.01768 |  0:00:41s\n",
      "epoch 24 | loss: 0.01662 | val_logits_ll: 0.01808 |  0:00:42s\n",
      "epoch 25 | loss: 0.0167  | val_logits_ll: 0.01816 |  0:00:44s\n",
      "epoch 26 | loss: 0.01656 | val_logits_ll: 0.01863 |  0:00:46s\n",
      "epoch 27 | loss: 0.01642 | val_logits_ll: 0.01932 |  0:00:47s\n",
      "epoch 28 | loss: 0.01637 | val_logits_ll: 0.01776 |  0:00:49s\n",
      "epoch 29 | loss: 0.01625 | val_logits_ll: 0.01962 |  0:00:51s\n",
      "epoch 30 | loss: 0.01615 | val_logits_ll: 0.01767 |  0:00:52s\n",
      "epoch 31 | loss: 0.0161  | val_logits_ll: 0.01777 |  0:00:54s\n",
      "epoch 32 | loss: 0.0161  | val_logits_ll: 0.01779 |  0:00:56s\n",
      "epoch 33 | loss: 0.01601 | val_logits_ll: 0.01778 |  0:00:58s\n",
      "epoch 34 | loss: 0.01607 | val_logits_ll: 0.0183  |  0:01:00s\n",
      "epoch 35 | loss: 0.01602 | val_logits_ll: 0.01783 |  0:01:01s\n",
      "epoch 36 | loss: 0.01592 | val_logits_ll: 0.01769 |  0:01:03s\n",
      "epoch 37 | loss: 0.01581 | val_logits_ll: 0.01789 |  0:01:04s\n",
      "epoch 38 | loss: 0.01586 | val_logits_ll: 0.01765 |  0:01:06s\n",
      "epoch 39 | loss: 0.01562 | val_logits_ll: 0.01765 |  0:01:08s\n",
      "epoch 40 | loss: 0.01563 | val_logits_ll: 0.0175  |  0:01:10s\n",
      "epoch 41 | loss: 0.01565 | val_logits_ll: 0.01772 |  0:01:12s\n",
      "epoch 42 | loss: 0.01558 | val_logits_ll: 0.01774 |  0:01:14s\n",
      "epoch 43 | loss: 0.01556 | val_logits_ll: 0.0178  |  0:01:15s\n",
      "epoch 44 | loss: 0.01549 | val_logits_ll: 0.01757 |  0:01:17s\n",
      "epoch 45 | loss: 0.01558 | val_logits_ll: 0.01768 |  0:01:19s\n",
      "epoch 46 | loss: 0.01545 | val_logits_ll: 0.01764 |  0:01:20s\n",
      "epoch 47 | loss: 0.01539 | val_logits_ll: 0.01748 |  0:01:22s\n",
      "epoch 48 | loss: 0.01545 | val_logits_ll: 0.01785 |  0:01:23s\n",
      "epoch 49 | loss: 0.01536 | val_logits_ll: 0.01783 |  0:01:25s\n",
      "epoch 50 | loss: 0.01525 | val_logits_ll: 0.01775 |  0:01:27s\n",
      "epoch 51 | loss: 0.0153  | val_logits_ll: 0.01763 |  0:01:28s\n",
      "epoch 52 | loss: 0.01536 | val_logits_ll: 0.01761 |  0:01:30s\n",
      "epoch 53 | loss: 0.01527 | val_logits_ll: 0.01759 |  0:01:32s\n",
      "epoch 54 | loss: 0.01527 | val_logits_ll: 0.01773 |  0:01:33s\n",
      "epoch 55 | loss: 0.01517 | val_logits_ll: 0.01765 |  0:01:35s\n",
      "epoch 56 | loss: 0.01514 | val_logits_ll: 0.01797 |  0:01:36s\n",
      "epoch 57 | loss: 0.0151  | val_logits_ll: 0.01787 |  0:01:38s\n",
      "epoch 58 | loss: 0.01504 | val_logits_ll: 0.01781 |  0:01:40s\n",
      "epoch 59 | loss: 0.01437 | val_logits_ll: 0.01746 |  0:01:41s\n",
      "epoch 60 | loss: 0.01383 | val_logits_ll: 0.01746 |  0:01:43s\n",
      "epoch 61 | loss: 0.01357 | val_logits_ll: 0.01759 |  0:01:45s\n",
      "epoch 62 | loss: 0.01337 | val_logits_ll: 0.01766 |  0:01:47s\n",
      "epoch 63 | loss: 0.01321 | val_logits_ll: 0.01779 |  0:01:49s\n",
      "epoch 64 | loss: 0.01305 | val_logits_ll: 0.01789 |  0:01:50s\n",
      "epoch 65 | loss: 0.01293 | val_logits_ll: 0.01805 |  0:01:52s\n",
      "epoch 66 | loss: 0.01281 | val_logits_ll: 0.01812 |  0:01:54s\n",
      "epoch 67 | loss: 0.01267 | val_logits_ll: 0.01824 |  0:01:55s\n",
      "epoch 68 | loss: 0.01255 | val_logits_ll: 0.01832 |  0:01:57s\n",
      "epoch 69 | loss: 0.01244 | val_logits_ll: 0.01854 |  0:01:59s\n",
      "epoch 70 | loss: 0.01236 | val_logits_ll: 0.01859 |  0:02:01s\n",
      "epoch 71 | loss: 0.01226 | val_logits_ll: 0.01872 |  0:02:02s\n",
      "epoch 72 | loss: 0.01192 | val_logits_ll: 0.01872 |  0:02:04s\n",
      "epoch 73 | loss: 0.01178 | val_logits_ll: 0.0188  |  0:02:06s\n",
      "epoch 74 | loss: 0.01172 | val_logits_ll: 0.01881 |  0:02:07s\n",
      "epoch 75 | loss: 0.01167 | val_logits_ll: 0.01888 |  0:02:09s\n",
      "epoch 76 | loss: 0.01164 | val_logits_ll: 0.0189  |  0:02:10s\n",
      "epoch 77 | loss: 0.01161 | val_logits_ll: 0.01893 |  0:02:12s\n",
      "epoch 78 | loss: 0.01159 | val_logits_ll: 0.01893 |  0:02:14s\n",
      "epoch 79 | loss: 0.01157 | val_logits_ll: 0.01899 |  0:02:15s\n",
      "epoch 80 | loss: 0.01154 | val_logits_ll: 0.01903 |  0:02:18s\n",
      "epoch 81 | loss: 0.0115  | val_logits_ll: 0.01904 |  0:02:20s\n",
      "epoch 82 | loss: 0.01148 | val_logits_ll: 0.01906 |  0:02:21s\n",
      "epoch 83 | loss: 0.01143 | val_logits_ll: 0.01908 |  0:02:23s\n",
      "epoch 84 | loss: 0.01144 | val_logits_ll: 0.01907 |  0:02:24s\n",
      "epoch 85 | loss: 0.01142 | val_logits_ll: 0.0191  |  0:02:26s\n",
      "epoch 86 | loss: 0.01142 | val_logits_ll: 0.01908 |  0:02:28s\n",
      "epoch 87 | loss: 0.01141 | val_logits_ll: 0.01912 |  0:02:29s\n",
      "epoch 88 | loss: 0.01142 | val_logits_ll: 0.0191  |  0:02:31s\n",
      "epoch 89 | loss: 0.01141 | val_logits_ll: 0.01911 |  0:02:32s\n",
      "epoch 90 | loss: 0.0114  | val_logits_ll: 0.01911 |  0:02:34s\n",
      "epoch 91 | loss: 0.0114  | val_logits_ll: 0.0191  |  0:02:35s\n",
      "epoch 92 | loss: 0.01141 | val_logits_ll: 0.01911 |  0:02:37s\n",
      "epoch 93 | loss: 0.0114  | val_logits_ll: 0.01911 |  0:02:39s\n",
      "epoch 94 | loss: 0.01141 | val_logits_ll: 0.01911 |  0:02:40s\n",
      "epoch 95 | loss: 0.01139 | val_logits_ll: 0.01911 |  0:02:42s\n",
      "epoch 96 | loss: 0.0114  | val_logits_ll: 0.01913 |  0:02:44s\n",
      "epoch 97 | loss: 0.0114  | val_logits_ll: 0.01912 |  0:02:45s\n",
      "epoch 98 | loss: 0.01138 | val_logits_ll: 0.01911 |  0:02:47s\n",
      "epoch 99 | loss: 0.01138 | val_logits_ll: 0.01913 |  0:02:49s\n",
      "epoch 100| loss: 0.0114  | val_logits_ll: 0.01911 |  0:02:51s\n",
      "epoch 101| loss: 0.0114  | val_logits_ll: 0.01912 |  0:02:53s\n",
      "epoch 102| loss: 0.01141 | val_logits_ll: 0.01911 |  0:02:55s\n",
      "epoch 103| loss: 0.01139 | val_logits_ll: 0.01913 |  0:02:56s\n",
      "epoch 104| loss: 0.01139 | val_logits_ll: 0.01913 |  0:02:58s\n",
      "epoch 105| loss: 0.01139 | val_logits_ll: 0.01913 |  0:03:00s\n",
      "epoch 106| loss: 0.01138 | val_logits_ll: 0.01913 |  0:03:02s\n",
      "epoch 107| loss: 0.01139 | val_logits_ll: 0.01912 |  0:03:03s\n",
      "epoch 108| loss: 0.01139 | val_logits_ll: 0.01912 |  0:03:05s\n",
      "epoch 109| loss: 0.01139 | val_logits_ll: 0.01914 |  0:03:06s\n",
      "epoch 110| loss: 0.0114  | val_logits_ll: 0.01911 |  0:03:08s\n",
      "\n",
      "Early stopping occured at epoch 110 with best_epoch = 60 and best_val_logits_ll = 0.01746\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold1_3.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40491 | val_logits_ll: 0.09785 |  0:00:01s\n",
      "epoch 1  | loss: 0.03319 | val_logits_ll: 0.0267  |  0:00:03s\n",
      "epoch 2  | loss: 0.02438 | val_logits_ll: 0.02194 |  0:00:04s\n",
      "epoch 3  | loss: 0.02158 | val_logits_ll: 0.02085 |  0:00:06s\n",
      "epoch 4  | loss: 0.02082 | val_logits_ll: 0.02039 |  0:00:07s\n",
      "epoch 5  | loss: 0.02035 | val_logits_ll: 0.02007 |  0:00:09s\n",
      "epoch 6  | loss: 0.01999 | val_logits_ll: 0.01982 |  0:00:11s\n",
      "epoch 7  | loss: 0.01966 | val_logits_ll: 0.01958 |  0:00:13s\n",
      "epoch 8  | loss: 0.0193  | val_logits_ll: 0.01928 |  0:00:15s\n",
      "epoch 9  | loss: 0.01906 | val_logits_ll: 0.01917 |  0:00:17s\n",
      "epoch 10 | loss: 0.01873 | val_logits_ll: 0.01896 |  0:00:18s\n",
      "epoch 11 | loss: 0.01842 | val_logits_ll: 0.01879 |  0:00:20s\n",
      "epoch 12 | loss: 0.01817 | val_logits_ll: 0.01861 |  0:00:21s\n",
      "epoch 13 | loss: 0.01788 | val_logits_ll: 0.01852 |  0:00:23s\n",
      "epoch 14 | loss: 0.01768 | val_logits_ll: 0.02001 |  0:00:25s\n",
      "epoch 15 | loss: 0.01752 | val_logits_ll: 0.01837 |  0:00:26s\n",
      "epoch 16 | loss: 0.01729 | val_logits_ll: 0.02024 |  0:00:28s\n",
      "epoch 17 | loss: 0.01714 | val_logits_ll: 0.0191  |  0:00:29s\n",
      "epoch 18 | loss: 0.01709 | val_logits_ll: 0.01817 |  0:00:31s\n",
      "epoch 19 | loss: 0.01689 | val_logits_ll: 0.01791 |  0:00:32s\n",
      "epoch 20 | loss: 0.01672 | val_logits_ll: 0.01865 |  0:00:34s\n",
      "epoch 21 | loss: 0.0167  | val_logits_ll: 0.01822 |  0:00:36s\n",
      "epoch 22 | loss: 0.01656 | val_logits_ll: 0.01812 |  0:00:37s\n",
      "epoch 23 | loss: 0.01647 | val_logits_ll: 0.01822 |  0:00:39s\n",
      "epoch 24 | loss: 0.01641 | val_logits_ll: 0.01803 |  0:00:40s\n",
      "epoch 25 | loss: 0.01632 | val_logits_ll: 0.01867 |  0:00:42s\n",
      "epoch 26 | loss: 0.01624 | val_logits_ll: 0.01953 |  0:00:44s\n",
      "epoch 27 | loss: 0.01619 | val_logits_ll: 0.01848 |  0:00:47s\n",
      "epoch 28 | loss: 0.0161  | val_logits_ll: 0.01821 |  0:00:49s\n",
      "epoch 29 | loss: 0.01613 | val_logits_ll: 0.01832 |  0:00:50s\n",
      "epoch 30 | loss: 0.01625 | val_logits_ll: 0.01845 |  0:00:52s\n",
      "epoch 31 | loss: 0.01571 | val_logits_ll: 0.01767 |  0:00:53s\n",
      "epoch 32 | loss: 0.01533 | val_logits_ll: 0.0177  |  0:00:55s\n",
      "epoch 33 | loss: 0.01508 | val_logits_ll: 0.01771 |  0:00:57s\n",
      "epoch 34 | loss: 0.0149  | val_logits_ll: 0.01777 |  0:00:58s\n",
      "epoch 35 | loss: 0.01475 | val_logits_ll: 0.01781 |  0:01:00s\n",
      "epoch 36 | loss: 0.0146  | val_logits_ll: 0.01788 |  0:01:01s\n",
      "epoch 37 | loss: 0.01447 | val_logits_ll: 0.01794 |  0:01:03s\n",
      "epoch 38 | loss: 0.01433 | val_logits_ll: 0.01804 |  0:01:05s\n",
      "epoch 39 | loss: 0.01419 | val_logits_ll: 0.01813 |  0:01:06s\n",
      "epoch 40 | loss: 0.01407 | val_logits_ll: 0.01815 |  0:01:08s\n",
      "epoch 41 | loss: 0.01398 | val_logits_ll: 0.01834 |  0:01:09s\n",
      "epoch 42 | loss: 0.01382 | val_logits_ll: 0.01843 |  0:01:11s\n",
      "epoch 43 | loss: 0.01356 | val_logits_ll: 0.01844 |  0:01:12s\n",
      "epoch 44 | loss: 0.01352 | val_logits_ll: 0.01845 |  0:01:14s\n",
      "epoch 45 | loss: 0.01342 | val_logits_ll: 0.01846 |  0:01:16s\n",
      "epoch 46 | loss: 0.01341 | val_logits_ll: 0.01849 |  0:01:18s\n",
      "epoch 47 | loss: 0.01336 | val_logits_ll: 0.0185  |  0:01:20s\n",
      "epoch 48 | loss: 0.01335 | val_logits_ll: 0.01851 |  0:01:21s\n",
      "epoch 49 | loss: 0.01332 | val_logits_ll: 0.01854 |  0:01:23s\n",
      "epoch 50 | loss: 0.01328 | val_logits_ll: 0.01853 |  0:01:24s\n",
      "epoch 51 | loss: 0.01327 | val_logits_ll: 0.01856 |  0:01:26s\n",
      "epoch 52 | loss: 0.01325 | val_logits_ll: 0.01856 |  0:01:28s\n",
      "epoch 53 | loss: 0.01318 | val_logits_ll: 0.01858 |  0:01:29s\n",
      "epoch 54 | loss: 0.01317 | val_logits_ll: 0.01859 |  0:01:31s\n",
      "epoch 55 | loss: 0.01316 | val_logits_ll: 0.01859 |  0:01:32s\n",
      "epoch 56 | loss: 0.01317 | val_logits_ll: 0.01861 |  0:01:34s\n",
      "epoch 57 | loss: 0.01316 | val_logits_ll: 0.0186  |  0:01:36s\n",
      "epoch 58 | loss: 0.01314 | val_logits_ll: 0.01859 |  0:01:37s\n",
      "epoch 59 | loss: 0.01315 | val_logits_ll: 0.0186  |  0:01:39s\n",
      "epoch 60 | loss: 0.01315 | val_logits_ll: 0.0186  |  0:01:40s\n",
      "epoch 61 | loss: 0.01316 | val_logits_ll: 0.01862 |  0:01:42s\n",
      "epoch 62 | loss: 0.01314 | val_logits_ll: 0.01861 |  0:01:44s\n",
      "epoch 63 | loss: 0.01314 | val_logits_ll: 0.01861 |  0:01:46s\n",
      "epoch 64 | loss: 0.01315 | val_logits_ll: 0.01862 |  0:01:47s\n",
      "epoch 65 | loss: 0.01314 | val_logits_ll: 0.01862 |  0:01:49s\n",
      "epoch 66 | loss: 0.01315 | val_logits_ll: 0.01862 |  0:01:51s\n",
      "epoch 67 | loss: 0.01314 | val_logits_ll: 0.01862 |  0:01:53s\n",
      "epoch 68 | loss: 0.01312 | val_logits_ll: 0.01861 |  0:01:55s\n",
      "epoch 69 | loss: 0.01315 | val_logits_ll: 0.01862 |  0:01:57s\n",
      "epoch 70 | loss: 0.01315 | val_logits_ll: 0.01861 |  0:01:58s\n",
      "epoch 71 | loss: 0.01312 | val_logits_ll: 0.01861 |  0:02:00s\n",
      "epoch 72 | loss: 0.01315 | val_logits_ll: 0.01862 |  0:02:01s\n",
      "epoch 73 | loss: 0.01313 | val_logits_ll: 0.01862 |  0:02:03s\n",
      "epoch 74 | loss: 0.01314 | val_logits_ll: 0.01862 |  0:02:05s\n",
      "epoch 75 | loss: 0.01312 | val_logits_ll: 0.01862 |  0:02:06s\n",
      "epoch 76 | loss: 0.01315 | val_logits_ll: 0.01863 |  0:02:08s\n",
      "epoch 77 | loss: 0.01312 | val_logits_ll: 0.01861 |  0:02:09s\n",
      "epoch 78 | loss: 0.01313 | val_logits_ll: 0.01862 |  0:02:11s\n",
      "epoch 79 | loss: 0.01312 | val_logits_ll: 0.01862 |  0:02:13s\n",
      "epoch 80 | loss: 0.01314 | val_logits_ll: 0.01862 |  0:02:14s\n",
      "epoch 81 | loss: 0.01313 | val_logits_ll: 0.01861 |  0:02:16s\n",
      "\n",
      "Early stopping occured at epoch 81 with best_epoch = 31 and best_val_logits_ll = 0.01767\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold2_3.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.39837 | val_logits_ll: 0.09363 |  0:00:01s\n",
      "epoch 1  | loss: 0.03222 | val_logits_ll: 0.02811 |  0:00:03s\n",
      "epoch 2  | loss: 0.02432 | val_logits_ll: 0.02289 |  0:00:04s\n",
      "epoch 3  | loss: 0.0217  | val_logits_ll: 0.02166 |  0:00:06s\n",
      "epoch 4  | loss: 0.02093 | val_logits_ll: 0.02099 |  0:00:09s\n",
      "epoch 5  | loss: 0.02039 | val_logits_ll: 0.02077 |  0:00:10s\n",
      "epoch 6  | loss: 0.01995 | val_logits_ll: 0.02045 |  0:00:12s\n",
      "epoch 7  | loss: 0.01954 | val_logits_ll: 0.02002 |  0:00:13s\n",
      "epoch 8  | loss: 0.01908 | val_logits_ll: 0.01963 |  0:00:15s\n",
      "epoch 9  | loss: 0.01863 | val_logits_ll: 0.01931 |  0:00:16s\n",
      "epoch 10 | loss: 0.0183  | val_logits_ll: 0.01916 |  0:00:18s\n",
      "epoch 11 | loss: 0.018   | val_logits_ll: 0.02115 |  0:00:20s\n",
      "epoch 12 | loss: 0.01778 | val_logits_ll: 0.02047 |  0:00:21s\n",
      "epoch 13 | loss: 0.01766 | val_logits_ll: 0.02018 |  0:00:23s\n",
      "epoch 14 | loss: 0.01752 | val_logits_ll: 0.01864 |  0:00:25s\n",
      "epoch 15 | loss: 0.01728 | val_logits_ll: 0.02052 |  0:00:26s\n",
      "epoch 16 | loss: 0.01707 | val_logits_ll: 0.0185  |  0:00:28s\n",
      "epoch 17 | loss: 0.01691 | val_logits_ll: 0.0186  |  0:00:30s\n",
      "epoch 18 | loss: 0.01688 | val_logits_ll: 0.01879 |  0:00:31s\n",
      "epoch 19 | loss: 0.01678 | val_logits_ll: 0.01869 |  0:00:33s\n",
      "epoch 20 | loss: 0.01663 | val_logits_ll: 0.02128 |  0:00:35s\n",
      "epoch 21 | loss: 0.01664 | val_logits_ll: 0.01954 |  0:00:36s\n",
      "epoch 22 | loss: 0.01652 | val_logits_ll: 0.01838 |  0:00:38s\n",
      "epoch 23 | loss: 0.01645 | val_logits_ll: 0.01836 |  0:00:41s\n",
      "epoch 24 | loss: 0.01641 | val_logits_ll: 0.01848 |  0:00:42s\n",
      "epoch 25 | loss: 0.01613 | val_logits_ll: 0.01806 |  0:00:44s\n",
      "epoch 26 | loss: 0.01608 | val_logits_ll: 0.02168 |  0:00:45s\n",
      "epoch 27 | loss: 0.01603 | val_logits_ll: 0.01834 |  0:00:47s\n",
      "epoch 28 | loss: 0.01616 | val_logits_ll: 0.01839 |  0:00:48s\n",
      "epoch 29 | loss: 0.01595 | val_logits_ll: 0.01836 |  0:00:50s\n",
      "epoch 30 | loss: 0.01584 | val_logits_ll: 0.01981 |  0:00:52s\n",
      "epoch 31 | loss: 0.01577 | val_logits_ll: 0.01881 |  0:00:53s\n",
      "epoch 32 | loss: 0.01581 | val_logits_ll: 0.01824 |  0:00:55s\n",
      "epoch 33 | loss: 0.0157  | val_logits_ll: 0.01816 |  0:00:56s\n",
      "epoch 34 | loss: 0.01565 | val_logits_ll: 0.01813 |  0:00:58s\n",
      "epoch 35 | loss: 0.01565 | val_logits_ll: 0.01804 |  0:01:00s\n",
      "epoch 36 | loss: 0.01558 | val_logits_ll: 0.01829 |  0:01:01s\n",
      "epoch 37 | loss: 0.01559 | val_logits_ll: 0.01805 |  0:01:03s\n",
      "epoch 38 | loss: 0.01546 | val_logits_ll: 0.02014 |  0:01:05s\n",
      "epoch 39 | loss: 0.01553 | val_logits_ll: 0.01795 |  0:01:06s\n",
      "epoch 40 | loss: 0.01559 | val_logits_ll: 0.01805 |  0:01:08s\n",
      "epoch 41 | loss: 0.01551 | val_logits_ll: 0.01832 |  0:01:09s\n",
      "epoch 42 | loss: 0.01548 | val_logits_ll: 0.01803 |  0:01:11s\n",
      "epoch 43 | loss: 0.01528 | val_logits_ll: 0.01809 |  0:01:14s\n",
      "epoch 44 | loss: 0.01531 | val_logits_ll: 0.01798 |  0:01:15s\n",
      "epoch 45 | loss: 0.01527 | val_logits_ll: 0.01823 |  0:01:17s\n",
      "epoch 46 | loss: 0.01523 | val_logits_ll: 0.01799 |  0:01:18s\n",
      "epoch 47 | loss: 0.0152  | val_logits_ll: 0.01806 |  0:01:20s\n",
      "epoch 48 | loss: 0.01527 | val_logits_ll: 0.01792 |  0:01:21s\n",
      "epoch 49 | loss: 0.01526 | val_logits_ll: 0.01825 |  0:01:23s\n",
      "epoch 50 | loss: 0.01522 | val_logits_ll: 0.01807 |  0:01:25s\n",
      "epoch 51 | loss: 0.01513 | val_logits_ll: 0.018   |  0:01:27s\n",
      "epoch 52 | loss: 0.01508 | val_logits_ll: 0.01819 |  0:01:29s\n",
      "epoch 53 | loss: 0.01521 | val_logits_ll: 0.01833 |  0:01:30s\n",
      "epoch 54 | loss: 0.01517 | val_logits_ll: 0.01809 |  0:01:32s\n",
      "epoch 55 | loss: 0.01508 | val_logits_ll: 0.0182  |  0:01:33s\n",
      "epoch 56 | loss: 0.0151  | val_logits_ll: 0.01811 |  0:01:35s\n",
      "epoch 57 | loss: 0.01512 | val_logits_ll: 0.01834 |  0:01:37s\n",
      "epoch 58 | loss: 0.01506 | val_logits_ll: 0.01808 |  0:01:38s\n",
      "epoch 59 | loss: 0.01509 | val_logits_ll: 0.01819 |  0:01:40s\n",
      "epoch 60 | loss: 0.0144  | val_logits_ll: 0.01768 |  0:01:41s\n",
      "epoch 61 | loss: 0.01385 | val_logits_ll: 0.01776 |  0:01:43s\n",
      "epoch 62 | loss: 0.01358 | val_logits_ll: 0.01788 |  0:01:45s\n",
      "epoch 63 | loss: 0.01336 | val_logits_ll: 0.01796 |  0:01:47s\n",
      "epoch 64 | loss: 0.01319 | val_logits_ll: 0.01805 |  0:01:49s\n",
      "epoch 65 | loss: 0.01305 | val_logits_ll: 0.01819 |  0:01:50s\n",
      "epoch 66 | loss: 0.01286 | val_logits_ll: 0.0183  |  0:01:52s\n",
      "epoch 67 | loss: 0.01273 | val_logits_ll: 0.01837 |  0:01:53s\n",
      "epoch 68 | loss: 0.01259 | val_logits_ll: 0.01853 |  0:01:55s\n",
      "epoch 69 | loss: 0.01249 | val_logits_ll: 0.01861 |  0:01:57s\n",
      "epoch 70 | loss: 0.01238 | val_logits_ll: 0.01877 |  0:01:58s\n",
      "epoch 71 | loss: 0.01229 | val_logits_ll: 0.01887 |  0:02:00s\n",
      "epoch 72 | loss: 0.01195 | val_logits_ll: 0.01889 |  0:02:01s\n",
      "epoch 73 | loss: 0.01183 | val_logits_ll: 0.01893 |  0:02:03s\n",
      "epoch 74 | loss: 0.01176 | val_logits_ll: 0.01895 |  0:02:04s\n",
      "epoch 75 | loss: 0.0117  | val_logits_ll: 0.01897 |  0:02:06s\n",
      "epoch 76 | loss: 0.01169 | val_logits_ll: 0.01901 |  0:02:08s\n",
      "epoch 77 | loss: 0.01166 | val_logits_ll: 0.01903 |  0:02:09s\n",
      "epoch 78 | loss: 0.01162 | val_logits_ll: 0.01902 |  0:02:11s\n",
      "epoch 79 | loss: 0.01161 | val_logits_ll: 0.01908 |  0:02:12s\n",
      "epoch 80 | loss: 0.01155 | val_logits_ll: 0.0191  |  0:02:14s\n",
      "epoch 81 | loss: 0.01154 | val_logits_ll: 0.01911 |  0:02:16s\n",
      "epoch 82 | loss: 0.01151 | val_logits_ll: 0.01915 |  0:02:18s\n",
      "epoch 83 | loss: 0.01147 | val_logits_ll: 0.01916 |  0:02:20s\n",
      "epoch 84 | loss: 0.01144 | val_logits_ll: 0.01916 |  0:02:22s\n",
      "epoch 85 | loss: 0.01148 | val_logits_ll: 0.01917 |  0:02:23s\n",
      "epoch 86 | loss: 0.01145 | val_logits_ll: 0.01917 |  0:02:25s\n",
      "epoch 87 | loss: 0.01144 | val_logits_ll: 0.01917 |  0:02:27s\n",
      "epoch 88 | loss: 0.01144 | val_logits_ll: 0.01918 |  0:02:29s\n",
      "epoch 89 | loss: 0.01143 | val_logits_ll: 0.01918 |  0:02:30s\n",
      "epoch 90 | loss: 0.01143 | val_logits_ll: 0.01919 |  0:02:32s\n",
      "epoch 91 | loss: 0.01144 | val_logits_ll: 0.01918 |  0:02:33s\n",
      "epoch 92 | loss: 0.01145 | val_logits_ll: 0.01919 |  0:02:35s\n",
      "epoch 93 | loss: 0.01142 | val_logits_ll: 0.0192  |  0:02:37s\n",
      "epoch 94 | loss: 0.01142 | val_logits_ll: 0.01919 |  0:02:38s\n",
      "epoch 95 | loss: 0.01144 | val_logits_ll: 0.01919 |  0:02:40s\n",
      "epoch 96 | loss: 0.01142 | val_logits_ll: 0.0192  |  0:02:42s\n",
      "epoch 97 | loss: 0.01143 | val_logits_ll: 0.01921 |  0:02:43s\n",
      "epoch 98 | loss: 0.01141 | val_logits_ll: 0.01919 |  0:02:45s\n",
      "epoch 99 | loss: 0.01141 | val_logits_ll: 0.01919 |  0:02:46s\n",
      "epoch 100| loss: 0.01144 | val_logits_ll: 0.01919 |  0:02:48s\n",
      "epoch 101| loss: 0.0114  | val_logits_ll: 0.01919 |  0:02:49s\n",
      "epoch 102| loss: 0.01141 | val_logits_ll: 0.0192  |  0:02:52s\n",
      "epoch 103| loss: 0.01141 | val_logits_ll: 0.01919 |  0:02:54s\n",
      "epoch 104| loss: 0.0114  | val_logits_ll: 0.0192  |  0:02:55s\n",
      "epoch 105| loss: 0.01143 | val_logits_ll: 0.01921 |  0:02:57s\n",
      "epoch 106| loss: 0.01143 | val_logits_ll: 0.01919 |  0:02:58s\n",
      "epoch 107| loss: 0.01143 | val_logits_ll: 0.01919 |  0:03:00s\n",
      "epoch 108| loss: 0.0114  | val_logits_ll: 0.0192  |  0:03:02s\n",
      "epoch 109| loss: 0.01143 | val_logits_ll: 0.0192  |  0:03:03s\n",
      "epoch 110| loss: 0.0114  | val_logits_ll: 0.01921 |  0:03:05s\n",
      "\n",
      "Early stopping occured at epoch 110 with best_epoch = 60 and best_val_logits_ll = 0.01768\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold3_3.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40244 | val_logits_ll: 0.10216 |  0:00:01s\n",
      "epoch 1  | loss: 0.03234 | val_logits_ll: 0.02676 |  0:00:03s\n",
      "epoch 2  | loss: 0.02414 | val_logits_ll: 0.02273 |  0:00:04s\n",
      "epoch 3  | loss: 0.02147 | val_logits_ll: 0.02135 |  0:00:06s\n",
      "epoch 4  | loss: 0.02071 | val_logits_ll: 0.02096 |  0:00:08s\n",
      "epoch 5  | loss: 0.02025 | val_logits_ll: 0.02059 |  0:00:09s\n",
      "epoch 6  | loss: 0.01989 | val_logits_ll: 0.02041 |  0:00:11s\n",
      "epoch 7  | loss: 0.01953 | val_logits_ll: 0.0202  |  0:00:12s\n",
      "epoch 8  | loss: 0.01926 | val_logits_ll: 0.02003 |  0:00:14s\n",
      "epoch 9  | loss: 0.01897 | val_logits_ll: 0.01981 |  0:00:16s\n",
      "epoch 10 | loss: 0.01857 | val_logits_ll: 0.01964 |  0:00:18s\n",
      "epoch 11 | loss: 0.01826 | val_logits_ll: 0.01939 |  0:00:20s\n",
      "epoch 12 | loss: 0.01802 | val_logits_ll: 0.01938 |  0:00:22s\n",
      "epoch 13 | loss: 0.01777 | val_logits_ll: 0.01934 |  0:00:24s\n",
      "epoch 14 | loss: 0.0176  | val_logits_ll: 0.0192  |  0:00:25s\n",
      "epoch 15 | loss: 0.0174  | val_logits_ll: 0.01902 |  0:00:27s\n",
      "epoch 16 | loss: 0.01719 | val_logits_ll: 0.01908 |  0:00:29s\n",
      "epoch 17 | loss: 0.01704 | val_logits_ll: 0.01894 |  0:00:30s\n",
      "epoch 18 | loss: 0.01685 | val_logits_ll: 0.01963 |  0:00:32s\n",
      "epoch 19 | loss: 0.01672 | val_logits_ll: 0.0198  |  0:00:33s\n",
      "epoch 20 | loss: 0.01669 | val_logits_ll: 0.01961 |  0:00:35s\n",
      "epoch 21 | loss: 0.01654 | val_logits_ll: 0.0195  |  0:00:36s\n",
      "epoch 22 | loss: 0.01637 | val_logits_ll: 0.01854 |  0:00:38s\n",
      "epoch 23 | loss: 0.01632 | val_logits_ll: 0.01881 |  0:00:40s\n",
      "epoch 24 | loss: 0.01629 | val_logits_ll: 0.01878 |  0:00:41s\n",
      "epoch 25 | loss: 0.01621 | val_logits_ll: 0.01862 |  0:00:43s\n",
      "epoch 26 | loss: 0.01603 | val_logits_ll: 0.0186  |  0:00:44s\n",
      "epoch 27 | loss: 0.01597 | val_logits_ll: 0.01948 |  0:00:46s\n",
      "epoch 28 | loss: 0.01593 | val_logits_ll: 0.01853 |  0:00:48s\n",
      "epoch 29 | loss: 0.01582 | val_logits_ll: 0.01847 |  0:00:50s\n",
      "epoch 30 | loss: 0.0157  | val_logits_ll: 0.01845 |  0:00:51s\n",
      "epoch 31 | loss: 0.01571 | val_logits_ll: 0.01855 |  0:00:53s\n",
      "epoch 32 | loss: 0.01587 | val_logits_ll: 0.01863 |  0:00:55s\n",
      "epoch 33 | loss: 0.01573 | val_logits_ll: 0.0185  |  0:00:56s\n",
      "epoch 34 | loss: 0.01558 | val_logits_ll: 0.01835 |  0:00:58s\n",
      "epoch 35 | loss: 0.01551 | val_logits_ll: 0.01859 |  0:01:00s\n",
      "epoch 36 | loss: 0.01555 | val_logits_ll: 0.01937 |  0:01:01s\n",
      "epoch 37 | loss: 0.01556 | val_logits_ll: 0.01863 |  0:01:03s\n",
      "epoch 38 | loss: 0.01554 | val_logits_ll: 0.01871 |  0:01:04s\n",
      "epoch 39 | loss: 0.01548 | val_logits_ll: 0.01841 |  0:01:06s\n",
      "epoch 40 | loss: 0.01536 | val_logits_ll: 0.01864 |  0:01:08s\n",
      "epoch 41 | loss: 0.01539 | val_logits_ll: 0.01935 |  0:01:09s\n",
      "epoch 42 | loss: 0.01532 | val_logits_ll: 0.01869 |  0:01:11s\n",
      "epoch 43 | loss: 0.01533 | val_logits_ll: 0.01855 |  0:01:12s\n",
      "epoch 44 | loss: 0.01543 | val_logits_ll: 0.01866 |  0:01:14s\n",
      "epoch 45 | loss: 0.01541 | val_logits_ll: 0.01861 |  0:01:15s\n",
      "epoch 46 | loss: 0.01479 | val_logits_ll: 0.01829 |  0:01:17s\n",
      "epoch 47 | loss: 0.0143  | val_logits_ll: 0.01839 |  0:01:19s\n",
      "epoch 48 | loss: 0.01406 | val_logits_ll: 0.0184  |  0:01:21s\n",
      "epoch 49 | loss: 0.01389 | val_logits_ll: 0.01854 |  0:01:23s\n",
      "epoch 50 | loss: 0.01375 | val_logits_ll: 0.01858 |  0:01:24s\n",
      "epoch 51 | loss: 0.01363 | val_logits_ll: 0.01867 |  0:01:26s\n",
      "epoch 52 | loss: 0.01349 | val_logits_ll: 0.0187  |  0:01:28s\n",
      "epoch 53 | loss: 0.01336 | val_logits_ll: 0.01881 |  0:01:30s\n",
      "epoch 54 | loss: 0.01322 | val_logits_ll: 0.01898 |  0:01:31s\n",
      "epoch 55 | loss: 0.01313 | val_logits_ll: 0.01916 |  0:01:33s\n",
      "epoch 56 | loss: 0.01302 | val_logits_ll: 0.01913 |  0:01:34s\n",
      "epoch 57 | loss: 0.01295 | val_logits_ll: 0.01928 |  0:01:36s\n",
      "epoch 58 | loss: 0.01268 | val_logits_ll: 0.01928 |  0:01:38s\n",
      "epoch 59 | loss: 0.01256 | val_logits_ll: 0.01932 |  0:01:39s\n",
      "epoch 60 | loss: 0.01251 | val_logits_ll: 0.01936 |  0:01:41s\n",
      "epoch 61 | loss: 0.01248 | val_logits_ll: 0.01938 |  0:01:42s\n",
      "epoch 62 | loss: 0.01244 | val_logits_ll: 0.0194  |  0:01:44s\n",
      "epoch 63 | loss: 0.01239 | val_logits_ll: 0.01942 |  0:01:45s\n",
      "epoch 64 | loss: 0.01239 | val_logits_ll: 0.01946 |  0:01:47s\n",
      "epoch 65 | loss: 0.01236 | val_logits_ll: 0.01944 |  0:01:48s\n",
      "epoch 66 | loss: 0.01229 | val_logits_ll: 0.01949 |  0:01:50s\n",
      "epoch 67 | loss: 0.01228 | val_logits_ll: 0.01953 |  0:01:52s\n",
      "epoch 68 | loss: 0.01228 | val_logits_ll: 0.01953 |  0:01:54s\n",
      "epoch 69 | loss: 0.01224 | val_logits_ll: 0.01956 |  0:01:55s\n",
      "epoch 70 | loss: 0.01221 | val_logits_ll: 0.01957 |  0:01:57s\n",
      "epoch 71 | loss: 0.01222 | val_logits_ll: 0.01957 |  0:01:59s\n",
      "epoch 72 | loss: 0.01222 | val_logits_ll: 0.01956 |  0:02:00s\n",
      "epoch 73 | loss: 0.01224 | val_logits_ll: 0.01957 |  0:02:02s\n",
      "epoch 74 | loss: 0.01221 | val_logits_ll: 0.01956 |  0:02:04s\n",
      "epoch 75 | loss: 0.0122  | val_logits_ll: 0.01957 |  0:02:05s\n",
      "epoch 76 | loss: 0.01218 | val_logits_ll: 0.01957 |  0:02:07s\n",
      "epoch 77 | loss: 0.01218 | val_logits_ll: 0.01957 |  0:02:09s\n",
      "epoch 78 | loss: 0.01221 | val_logits_ll: 0.01958 |  0:02:10s\n",
      "epoch 79 | loss: 0.0122  | val_logits_ll: 0.01958 |  0:02:12s\n",
      "epoch 80 | loss: 0.01223 | val_logits_ll: 0.01959 |  0:02:13s\n",
      "epoch 81 | loss: 0.01219 | val_logits_ll: 0.01958 |  0:02:15s\n",
      "epoch 82 | loss: 0.01219 | val_logits_ll: 0.01958 |  0:02:16s\n",
      "epoch 83 | loss: 0.01216 | val_logits_ll: 0.01959 |  0:02:18s\n",
      "epoch 84 | loss: 0.01219 | val_logits_ll: 0.01959 |  0:02:20s\n",
      "epoch 85 | loss: 0.01219 | val_logits_ll: 0.01959 |  0:02:22s\n",
      "epoch 86 | loss: 0.01219 | val_logits_ll: 0.01959 |  0:02:23s\n",
      "epoch 87 | loss: 0.01218 | val_logits_ll: 0.01958 |  0:02:25s\n",
      "epoch 88 | loss: 0.01219 | val_logits_ll: 0.01958 |  0:02:27s\n",
      "epoch 89 | loss: 0.01219 | val_logits_ll: 0.01958 |  0:02:28s\n",
      "epoch 90 | loss: 0.01218 | val_logits_ll: 0.01958 |  0:02:30s\n",
      "epoch 91 | loss: 0.01219 | val_logits_ll: 0.01959 |  0:02:32s\n",
      "epoch 92 | loss: 0.01219 | val_logits_ll: 0.0196  |  0:02:34s\n",
      "epoch 93 | loss: 0.01219 | val_logits_ll: 0.01958 |  0:02:35s\n",
      "epoch 94 | loss: 0.01217 | val_logits_ll: 0.01958 |  0:02:37s\n",
      "epoch 95 | loss: 0.01218 | val_logits_ll: 0.01958 |  0:02:39s\n",
      "epoch 96 | loss: 0.0122  | val_logits_ll: 0.01959 |  0:02:40s\n",
      "\n",
      "Early stopping occured at epoch 96 with best_epoch = 46 and best_val_logits_ll = 0.01829\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold4_3.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40307 | val_logits_ll: 0.10001 |  0:00:01s\n",
      "epoch 1  | loss: 0.03294 | val_logits_ll: 0.02825 |  0:00:03s\n",
      "epoch 2  | loss: 0.02447 | val_logits_ll: 0.02249 |  0:00:04s\n",
      "epoch 3  | loss: 0.02151 | val_logits_ll: 0.02133 |  0:00:06s\n",
      "epoch 4  | loss: 0.02071 | val_logits_ll: 0.02095 |  0:00:07s\n",
      "epoch 5  | loss: 0.02037 | val_logits_ll: 0.02061 |  0:00:09s\n",
      "epoch 6  | loss: 0.02008 | val_logits_ll: 0.02034 |  0:00:11s\n",
      "epoch 7  | loss: 0.01984 | val_logits_ll: 0.02013 |  0:00:12s\n",
      "epoch 8  | loss: 0.01961 | val_logits_ll: 0.02004 |  0:00:14s\n",
      "epoch 9  | loss: 0.01938 | val_logits_ll: 0.01995 |  0:00:16s\n",
      "epoch 10 | loss: 0.01912 | val_logits_ll: 0.0197  |  0:00:17s\n",
      "epoch 11 | loss: 0.0187  | val_logits_ll: 0.01924 |  0:00:19s\n",
      "epoch 12 | loss: 0.01824 | val_logits_ll: 0.01967 |  0:00:21s\n",
      "epoch 13 | loss: 0.0179  | val_logits_ll: 0.01876 |  0:00:23s\n",
      "epoch 14 | loss: 0.01758 | val_logits_ll: 0.0187  |  0:00:24s\n",
      "epoch 15 | loss: 0.01743 | val_logits_ll: 0.01866 |  0:00:26s\n",
      "epoch 16 | loss: 0.01721 | val_logits_ll: 0.01847 |  0:00:27s\n",
      "epoch 17 | loss: 0.01706 | val_logits_ll: 0.01862 |  0:00:29s\n",
      "epoch 18 | loss: 0.01681 | val_logits_ll: 0.01837 |  0:00:31s\n",
      "epoch 19 | loss: 0.01679 | val_logits_ll: 0.021   |  0:00:32s\n",
      "epoch 20 | loss: 0.0167  | val_logits_ll: 0.0185  |  0:00:34s\n",
      "epoch 21 | loss: 0.01655 | val_logits_ll: 0.01913 |  0:00:36s\n",
      "epoch 22 | loss: 0.01644 | val_logits_ll: 0.02047 |  0:00:37s\n",
      "epoch 23 | loss: 0.01646 | val_logits_ll: 0.01907 |  0:00:39s\n",
      "epoch 24 | loss: 0.01624 | val_logits_ll: 0.01822 |  0:00:41s\n",
      "epoch 25 | loss: 0.0162  | val_logits_ll: 0.01822 |  0:00:43s\n",
      "epoch 26 | loss: 0.01609 | val_logits_ll: 0.01835 |  0:00:44s\n",
      "epoch 27 | loss: 0.01597 | val_logits_ll: 0.01822 |  0:00:46s\n",
      "epoch 28 | loss: 0.01599 | val_logits_ll: 0.0183  |  0:00:47s\n",
      "epoch 29 | loss: 0.01593 | val_logits_ll: 0.01808 |  0:00:49s\n",
      "epoch 30 | loss: 0.01582 | val_logits_ll: 0.01816 |  0:00:51s\n",
      "epoch 31 | loss: 0.01575 | val_logits_ll: 0.01814 |  0:00:52s\n",
      "epoch 32 | loss: 0.01577 | val_logits_ll: 0.01852 |  0:00:54s\n",
      "epoch 33 | loss: 0.01575 | val_logits_ll: 0.01798 |  0:00:56s\n",
      "epoch 34 | loss: 0.0156  | val_logits_ll: 0.01906 |  0:00:58s\n",
      "epoch 35 | loss: 0.0157  | val_logits_ll: 0.01812 |  0:00:59s\n",
      "epoch 36 | loss: 0.01557 | val_logits_ll: 0.01835 |  0:01:01s\n",
      "epoch 37 | loss: 0.01554 | val_logits_ll: 0.01813 |  0:01:02s\n",
      "epoch 38 | loss: 0.01548 | val_logits_ll: 0.01841 |  0:01:04s\n",
      "epoch 39 | loss: 0.01551 | val_logits_ll: 0.01825 |  0:01:05s\n",
      "epoch 40 | loss: 0.01547 | val_logits_ll: 0.01827 |  0:01:07s\n",
      "epoch 41 | loss: 0.01538 | val_logits_ll: 0.01845 |  0:01:08s\n",
      "epoch 42 | loss: 0.01537 | val_logits_ll: 0.01835 |  0:01:10s\n",
      "epoch 43 | loss: 0.01538 | val_logits_ll: 0.0183  |  0:01:12s\n",
      "epoch 44 | loss: 0.01532 | val_logits_ll: 0.01817 |  0:01:13s\n",
      "epoch 45 | loss: 0.01471 | val_logits_ll: 0.0179  |  0:01:15s\n",
      "epoch 46 | loss: 0.01427 | val_logits_ll: 0.01803 |  0:01:16s\n",
      "epoch 47 | loss: 0.01399 | val_logits_ll: 0.01805 |  0:01:18s\n",
      "epoch 48 | loss: 0.01381 | val_logits_ll: 0.01819 |  0:01:20s\n",
      "epoch 49 | loss: 0.01368 | val_logits_ll: 0.01822 |  0:01:21s\n",
      "epoch 50 | loss: 0.01351 | val_logits_ll: 0.01835 |  0:01:23s\n",
      "epoch 51 | loss: 0.01337 | val_logits_ll: 0.01847 |  0:01:25s\n",
      "epoch 52 | loss: 0.01322 | val_logits_ll: 0.01852 |  0:01:26s\n",
      "epoch 53 | loss: 0.01307 | val_logits_ll: 0.01867 |  0:01:28s\n",
      "epoch 54 | loss: 0.01296 | val_logits_ll: 0.01878 |  0:01:30s\n",
      "epoch 55 | loss: 0.01285 | val_logits_ll: 0.01898 |  0:01:31s\n",
      "epoch 56 | loss: 0.01275 | val_logits_ll: 0.01901 |  0:01:33s\n",
      "epoch 57 | loss: 0.01246 | val_logits_ll: 0.01902 |  0:01:35s\n",
      "epoch 58 | loss: 0.01231 | val_logits_ll: 0.01905 |  0:01:37s\n",
      "epoch 59 | loss: 0.01228 | val_logits_ll: 0.01909 |  0:01:39s\n",
      "epoch 60 | loss: 0.01224 | val_logits_ll: 0.01914 |  0:01:40s\n",
      "epoch 61 | loss: 0.0122  | val_logits_ll: 0.01917 |  0:01:42s\n",
      "epoch 62 | loss: 0.01217 | val_logits_ll: 0.01923 |  0:01:43s\n",
      "epoch 63 | loss: 0.01214 | val_logits_ll: 0.01924 |  0:01:45s\n",
      "epoch 64 | loss: 0.01214 | val_logits_ll: 0.01927 |  0:01:47s\n",
      "epoch 65 | loss: 0.0121  | val_logits_ll: 0.01929 |  0:01:48s\n",
      "epoch 66 | loss: 0.01206 | val_logits_ll: 0.01932 |  0:01:50s\n",
      "epoch 67 | loss: 0.01203 | val_logits_ll: 0.01938 |  0:01:52s\n",
      "epoch 68 | loss: 0.01199 | val_logits_ll: 0.01936 |  0:01:53s\n",
      "epoch 69 | loss: 0.01199 | val_logits_ll: 0.01937 |  0:01:55s\n",
      "epoch 70 | loss: 0.01198 | val_logits_ll: 0.01938 |  0:01:57s\n",
      "epoch 71 | loss: 0.01197 | val_logits_ll: 0.01937 |  0:01:58s\n",
      "epoch 72 | loss: 0.01198 | val_logits_ll: 0.01939 |  0:02:01s\n",
      "epoch 73 | loss: 0.01201 | val_logits_ll: 0.01939 |  0:02:02s\n",
      "epoch 74 | loss: 0.01198 | val_logits_ll: 0.01937 |  0:02:04s\n",
      "epoch 75 | loss: 0.01195 | val_logits_ll: 0.01938 |  0:02:05s\n",
      "epoch 76 | loss: 0.01197 | val_logits_ll: 0.0194  |  0:02:07s\n",
      "epoch 77 | loss: 0.01195 | val_logits_ll: 0.01938 |  0:02:09s\n",
      "epoch 78 | loss: 0.01196 | val_logits_ll: 0.01939 |  0:02:10s\n",
      "epoch 79 | loss: 0.01196 | val_logits_ll: 0.01939 |  0:02:12s\n",
      "epoch 80 | loss: 0.01196 | val_logits_ll: 0.01939 |  0:02:13s\n",
      "epoch 81 | loss: 0.01199 | val_logits_ll: 0.0194  |  0:02:15s\n",
      "epoch 82 | loss: 0.01195 | val_logits_ll: 0.01939 |  0:02:16s\n",
      "epoch 83 | loss: 0.01195 | val_logits_ll: 0.01939 |  0:02:18s\n",
      "epoch 84 | loss: 0.01196 | val_logits_ll: 0.01939 |  0:02:20s\n",
      "epoch 85 | loss: 0.01195 | val_logits_ll: 0.01939 |  0:02:21s\n",
      "epoch 86 | loss: 0.01195 | val_logits_ll: 0.01941 |  0:02:23s\n",
      "epoch 87 | loss: 0.01194 | val_logits_ll: 0.0194  |  0:02:25s\n",
      "epoch 88 | loss: 0.01197 | val_logits_ll: 0.0194  |  0:02:26s\n",
      "epoch 89 | loss: 0.01195 | val_logits_ll: 0.01938 |  0:02:28s\n",
      "epoch 90 | loss: 0.01197 | val_logits_ll: 0.01941 |  0:02:29s\n",
      "epoch 91 | loss: 0.01197 | val_logits_ll: 0.01939 |  0:02:31s\n",
      "epoch 92 | loss: 0.01196 | val_logits_ll: 0.0194  |  0:02:33s\n",
      "epoch 93 | loss: 0.01194 | val_logits_ll: 0.0194  |  0:02:35s\n",
      "epoch 94 | loss: 0.01195 | val_logits_ll: 0.01939 |  0:02:37s\n",
      "epoch 95 | loss: 0.01196 | val_logits_ll: 0.01939 |  0:02:39s\n",
      "\n",
      "Early stopping occured at epoch 95 with best_epoch = 45 and best_val_logits_ll = 0.0179\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold5_3.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4026  | val_logits_ll: 0.10403 |  0:00:01s\n",
      "epoch 1  | loss: 0.03287 | val_logits_ll: 0.02617 |  0:00:03s\n",
      "epoch 2  | loss: 0.02418 | val_logits_ll: 0.02149 |  0:00:05s\n",
      "epoch 3  | loss: 0.02175 | val_logits_ll: 0.02065 |  0:00:06s\n",
      "epoch 4  | loss: 0.02096 | val_logits_ll: 0.02019 |  0:00:08s\n",
      "epoch 5  | loss: 0.02047 | val_logits_ll: 0.01991 |  0:00:09s\n",
      "epoch 6  | loss: 0.02013 | val_logits_ll: 0.01966 |  0:00:11s\n",
      "epoch 7  | loss: 0.01978 | val_logits_ll: 0.01952 |  0:00:13s\n",
      "epoch 8  | loss: 0.01942 | val_logits_ll: 0.0192  |  0:00:15s\n",
      "epoch 9  | loss: 0.0191  | val_logits_ll: 0.01944 |  0:00:16s\n",
      "epoch 10 | loss: 0.01893 | val_logits_ll: 0.01891 |  0:00:18s\n",
      "epoch 11 | loss: 0.01857 | val_logits_ll: 0.01866 |  0:00:19s\n",
      "epoch 12 | loss: 0.01832 | val_logits_ll: 0.01855 |  0:00:21s\n",
      "epoch 13 | loss: 0.0181  | val_logits_ll: 0.01824 |  0:00:23s\n",
      "epoch 14 | loss: 0.01786 | val_logits_ll: 0.01841 |  0:00:25s\n",
      "epoch 15 | loss: 0.01769 | val_logits_ll: 0.01813 |  0:00:26s\n",
      "epoch 16 | loss: 0.01754 | val_logits_ll: 0.01856 |  0:00:28s\n",
      "epoch 17 | loss: 0.01743 | val_logits_ll: 0.01847 |  0:00:29s\n",
      "epoch 18 | loss: 0.01724 | val_logits_ll: 0.01811 |  0:00:31s\n",
      "epoch 19 | loss: 0.01709 | val_logits_ll: 0.0181  |  0:00:32s\n",
      "epoch 20 | loss: 0.01694 | val_logits_ll: 0.01818 |  0:00:34s\n",
      "epoch 21 | loss: 0.01685 | val_logits_ll: 0.01912 |  0:00:36s\n",
      "epoch 22 | loss: 0.01677 | val_logits_ll: 0.01828 |  0:00:37s\n",
      "epoch 23 | loss: 0.01671 | val_logits_ll: 0.01786 |  0:00:39s\n",
      "epoch 24 | loss: 0.01656 | val_logits_ll: 0.01792 |  0:00:40s\n",
      "epoch 25 | loss: 0.01651 | val_logits_ll: 0.01944 |  0:00:42s\n",
      "epoch 26 | loss: 0.0164  | val_logits_ll: 0.02087 |  0:00:44s\n",
      "epoch 27 | loss: 0.01631 | val_logits_ll: 0.01803 |  0:00:45s\n",
      "epoch 28 | loss: 0.01631 | val_logits_ll: 0.018   |  0:00:47s\n",
      "epoch 29 | loss: 0.01623 | val_logits_ll: 0.01818 |  0:00:49s\n",
      "epoch 30 | loss: 0.01607 | val_logits_ll: 0.01799 |  0:00:50s\n",
      "epoch 31 | loss: 0.01603 | val_logits_ll: 0.01977 |  0:00:52s\n",
      "epoch 32 | loss: 0.01599 | val_logits_ll: 0.01794 |  0:00:54s\n",
      "epoch 33 | loss: 0.01594 | val_logits_ll: 0.01786 |  0:00:56s\n",
      "epoch 34 | loss: 0.01606 | val_logits_ll: 0.01833 |  0:00:58s\n",
      "epoch 35 | loss: 0.01559 | val_logits_ll: 0.01758 |  0:01:00s\n",
      "epoch 36 | loss: 0.01516 | val_logits_ll: 0.01757 |  0:01:01s\n",
      "epoch 37 | loss: 0.01493 | val_logits_ll: 0.01764 |  0:01:03s\n",
      "epoch 38 | loss: 0.01479 | val_logits_ll: 0.0177  |  0:01:05s\n",
      "epoch 39 | loss: 0.01466 | val_logits_ll: 0.01779 |  0:01:06s\n",
      "epoch 40 | loss: 0.01452 | val_logits_ll: 0.01787 |  0:01:08s\n",
      "epoch 41 | loss: 0.01439 | val_logits_ll: 0.01793 |  0:01:09s\n",
      "epoch 42 | loss: 0.01429 | val_logits_ll: 0.01796 |  0:01:11s\n",
      "epoch 43 | loss: 0.01417 | val_logits_ll: 0.01811 |  0:01:13s\n",
      "epoch 44 | loss: 0.01407 | val_logits_ll: 0.0182  |  0:01:14s\n",
      "epoch 45 | loss: 0.01396 | val_logits_ll: 0.01829 |  0:01:16s\n",
      "epoch 46 | loss: 0.01387 | val_logits_ll: 0.01834 |  0:01:17s\n",
      "epoch 47 | loss: 0.01374 | val_logits_ll: 0.01844 |  0:01:19s\n",
      "epoch 48 | loss: 0.01348 | val_logits_ll: 0.0185  |  0:01:21s\n",
      "epoch 49 | loss: 0.01339 | val_logits_ll: 0.01853 |  0:01:23s\n",
      "epoch 50 | loss: 0.01334 | val_logits_ll: 0.01856 |  0:01:24s\n",
      "epoch 51 | loss: 0.0133  | val_logits_ll: 0.01863 |  0:01:26s\n",
      "epoch 52 | loss: 0.01328 | val_logits_ll: 0.01861 |  0:01:27s\n",
      "epoch 53 | loss: 0.01324 | val_logits_ll: 0.01865 |  0:01:29s\n",
      "epoch 54 | loss: 0.01319 | val_logits_ll: 0.01865 |  0:01:31s\n",
      "epoch 55 | loss: 0.01319 | val_logits_ll: 0.0187  |  0:01:33s\n",
      "epoch 56 | loss: 0.01316 | val_logits_ll: 0.01869 |  0:01:34s\n",
      "epoch 57 | loss: 0.01316 | val_logits_ll: 0.01873 |  0:01:36s\n",
      "epoch 58 | loss: 0.01309 | val_logits_ll: 0.01875 |  0:01:37s\n",
      "epoch 59 | loss: 0.01308 | val_logits_ll: 0.01876 |  0:01:39s\n",
      "epoch 60 | loss: 0.01307 | val_logits_ll: 0.01877 |  0:01:40s\n",
      "epoch 61 | loss: 0.01306 | val_logits_ll: 0.01877 |  0:01:42s\n",
      "epoch 62 | loss: 0.01308 | val_logits_ll: 0.01878 |  0:01:44s\n",
      "epoch 63 | loss: 0.01303 | val_logits_ll: 0.01877 |  0:01:45s\n",
      "epoch 64 | loss: 0.01305 | val_logits_ll: 0.01878 |  0:01:47s\n",
      "epoch 65 | loss: 0.01304 | val_logits_ll: 0.01877 |  0:01:48s\n",
      "epoch 66 | loss: 0.01306 | val_logits_ll: 0.01879 |  0:01:50s\n",
      "epoch 67 | loss: 0.01305 | val_logits_ll: 0.01877 |  0:01:52s\n",
      "epoch 68 | loss: 0.01303 | val_logits_ll: 0.01877 |  0:01:54s\n",
      "epoch 69 | loss: 0.01304 | val_logits_ll: 0.01879 |  0:01:56s\n",
      "epoch 70 | loss: 0.01303 | val_logits_ll: 0.01879 |  0:01:57s\n",
      "epoch 71 | loss: 0.01304 | val_logits_ll: 0.0188  |  0:01:59s\n",
      "epoch 72 | loss: 0.01302 | val_logits_ll: 0.01879 |  0:02:01s\n",
      "epoch 73 | loss: 0.01302 | val_logits_ll: 0.01878 |  0:02:03s\n",
      "epoch 74 | loss: 0.01304 | val_logits_ll: 0.01879 |  0:02:05s\n",
      "epoch 75 | loss: 0.01304 | val_logits_ll: 0.01878 |  0:02:06s\n",
      "epoch 76 | loss: 0.01303 | val_logits_ll: 0.01879 |  0:02:08s\n",
      "epoch 77 | loss: 0.01305 | val_logits_ll: 0.01879 |  0:02:09s\n",
      "epoch 78 | loss: 0.01304 | val_logits_ll: 0.01877 |  0:02:11s\n",
      "epoch 79 | loss: 0.01304 | val_logits_ll: 0.0188  |  0:02:13s\n",
      "epoch 80 | loss: 0.01302 | val_logits_ll: 0.01879 |  0:02:14s\n",
      "epoch 81 | loss: 0.01304 | val_logits_ll: 0.01879 |  0:02:16s\n",
      "epoch 82 | loss: 0.01302 | val_logits_ll: 0.01879 |  0:02:17s\n",
      "epoch 83 | loss: 0.01303 | val_logits_ll: 0.01879 |  0:02:19s\n",
      "epoch 84 | loss: 0.01305 | val_logits_ll: 0.01879 |  0:02:20s\n",
      "epoch 85 | loss: 0.01303 | val_logits_ll: 0.01879 |  0:02:22s\n",
      "epoch 86 | loss: 0.01303 | val_logits_ll: 0.01879 |  0:02:24s\n",
      "\n",
      "Early stopping occured at epoch 86 with best_epoch = 36 and best_val_logits_ll = 0.01757\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_raw_step1_fold6_3.zip\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fold_4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2888\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fold_4'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4d5d4127b247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moof_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_k_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNFOLDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moof\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moof_\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredictions_\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-70744a224291>\u001b[0m in \u001b[0;36mrun_k_fold\u001b[0;34m(NFOLDS, seed)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNFOLDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moof_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred_\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mNFOLDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-2602330d05c9>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(fold, seed)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrn_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'fold_{seed}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'fold_{seed}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fold_4'"
     ]
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "SEED = [42, 0, 1,2,3,4] #<-- Update\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(cfg.NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:31.595365Z",
     "iopub.status.busy": "2020-11-26T04:03:31.594459Z",
     "iopub.status.idle": "2020-11-26T04:03:31.598220Z",
     "shell.execute_reply": "2020-11-26T04:03:31.598696Z"
    },
    "papermill": {
     "duration": 1.21071,
     "end_time": "2020-11-26T04:03:31.598822",
     "exception": false,
     "start_time": "2020-11-26T04:03:30.388112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:34.073423Z",
     "iopub.status.busy": "2020-11-26T04:03:34.071987Z",
     "iopub.status.idle": "2020-11-26T04:03:34.181797Z",
     "shell.execute_reply": "2020-11-26T04:03:34.181190Z"
    },
    "papermill": {
     "duration": 1.340599,
     "end_time": "2020-11-26T04:03:34.181915",
     "exception": false,
     "start_time": "2020-11-26T04:03:32.841316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols+['fold_42','fold_0','fold_1','fold_2','fold_3']).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "valid_results = valid_results.add_prefix('pre_')\n",
    "valid_results.rename(columns={'pre_sig_id':'sig_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:36.625115Z",
     "iopub.status.busy": "2020-11-26T04:03:36.623880Z",
     "iopub.status.idle": "2020-11-26T04:03:43.272630Z",
     "shell.execute_reply": "2020-11-26T04:03:43.271372Z"
    },
    "papermill": {
     "duration": 7.88142,
     "end_time": "2020-11-26T04:03:43.272771",
     "exception": false,
     "start_time": "2020-11-26T04:03:35.391351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof = train_targets_scored.drop(columns=['fold_42','fold_0','fold_1','fold_2','fold_3'],axis=1)\n",
    "oof = oof.merge(valid_results, on=['sig_id'], how='left')\n",
    "oof.to_csv('moa_nn_oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:45.797580Z",
     "iopub.status.busy": "2020-11-26T04:03:45.793365Z",
     "iopub.status.idle": "2020-11-26T04:03:45.802025Z",
     "shell.execute_reply": "2020-11-26T04:03:45.802548Z"
    },
    "papermill": {
     "duration": 1.314855,
     "end_time": "2020-11-26T04:03:45.802676",
     "exception": false,
     "start_time": "2020-11-26T04:03:44.487821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>pre_tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>pre_trpv_agonist</th>\n",
       "      <th>pre_trpv_antagonist</th>\n",
       "      <th>pre_tubulin_inhibitor</th>\n",
       "      <th>pre_tyrosine_kinase_inhibitor</th>\n",
       "      <th>pre_ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>pre_vegfr_inhibitor</th>\n",
       "      <th>pre_vitamin_b</th>\n",
       "      <th>pre_vitamin_d_receptor_agonist</th>\n",
       "      <th>pre_wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 413 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_000644bb2                            0                       0   \n",
       "1  id_000779bfc                            0                       0   \n",
       "2  id_000a6266a                            0                       0   \n",
       "3  id_0015fd391                            0                       0   \n",
       "4  id_001626bd3                            0                       0   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0               0                               0   \n",
       "1               0                               0   \n",
       "2               0                               0   \n",
       "3               0                               0   \n",
       "4               0                               0   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                                  0                               0   \n",
       "1                                  0                               0   \n",
       "2                                  0                               0   \n",
       "3                                  0                               0   \n",
       "4                                  0                               0   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                           0                              0   \n",
       "1                           0                              0   \n",
       "2                           0                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  pre_tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                           0  ...                                        0.0   \n",
       "1                           0  ...                                        0.0   \n",
       "2                           0  ...                                        0.0   \n",
       "3                           0  ...                                        0.0   \n",
       "4                           0  ...                                        0.0   \n",
       "\n",
       "   pre_trpv_agonist  pre_trpv_antagonist  pre_tubulin_inhibitor  \\\n",
       "0               0.0                  0.0                    0.0   \n",
       "1               0.0                  0.0                    0.0   \n",
       "2               0.0                  0.0                    0.0   \n",
       "3               0.0                  0.0                    0.0   \n",
       "4               0.0                  0.0                    0.0   \n",
       "\n",
       "   pre_tyrosine_kinase_inhibitor  pre_ubiquitin_specific_protease_inhibitor  \\\n",
       "0                            0.0                                        0.0   \n",
       "1                            0.0                                        0.0   \n",
       "2                            0.0                                        0.0   \n",
       "3                            0.0                                        0.0   \n",
       "4                            0.0                                        0.0   \n",
       "\n",
       "   pre_vegfr_inhibitor  pre_vitamin_b  pre_vitamin_d_receptor_agonist  \\\n",
       "0                  0.0            0.0                             0.0   \n",
       "1                  0.0            0.0                             0.0   \n",
       "2                  0.0            0.0                             0.0   \n",
       "3                  0.0            0.0                             0.0   \n",
       "4                  0.0            0.0                             0.0   \n",
       "\n",
       "   pre_wnt_inhibitor  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "\n",
       "[5 rows x 413 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:48.566787Z",
     "iopub.status.busy": "2020-11-26T04:03:48.565859Z",
     "iopub.status.idle": "2020-11-26T04:03:49.532716Z",
     "shell.execute_reply": "2020-11-26T04:03:49.533220Z"
    },
    "papermill": {
     "duration": 2.467242,
     "end_time": "2020-11-26T04:03:49.533390",
     "exception": false,
     "start_time": "2020-11-26T04:03:47.066148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  9.99200722162639e-16\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "for i in (target_cols):\n",
    "    score_ = log_loss(oof[i], oof['pre_'+i])\n",
    "    score += score_ / 206\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:52.013630Z",
     "iopub.status.busy": "2020-11-26T04:03:52.012545Z",
     "iopub.status.idle": "2020-11-26T04:03:52.823276Z",
     "shell.execute_reply": "2020-11-26T04:03:52.822749Z"
    },
    "papermill": {
     "duration": 2.052473,
     "end_time": "2020-11-26T04:03:52.823382",
     "exception": false,
     "start_time": "2020-11-26T04:03:50.770909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC:  0.995169082125608\n"
     ]
    }
   ],
   "source": [
    "auc_score = 0\n",
    "for i in (target_cols):\n",
    "    score_ = roc_auc_score(oof[i], oof['pre_'+i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV AUC: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:55.366231Z",
     "iopub.status.busy": "2020-11-26T04:03:55.365373Z",
     "iopub.status.idle": "2020-11-26T04:03:55.399087Z",
     "shell.execute_reply": "2020-11-26T04:03:55.398585Z"
    },
    "papermill": {
     "duration": 1.287298,
     "end_time": "2020-11-26T04:03:55.399187",
     "exception": false,
     "start_time": "2020-11-26T04:03:54.111889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['calcineurin_inhibitor', 'cholesterol_inhibitor', 'hsp_inhibitor', 'ribonucleoside_reductase_inhibitor', 'glutamate_inhibitor', 'laxative', 'tlr_antagonist', 'bacterial_cell_wall_synthesis_inhibitor', 'gonadotropin_receptor_agonist', 'flt3_inhibitor', 'antiviral', 'ikk_inhibitor', 'atp_synthase_inhibitor', 'aurora_kinase_inhibitor', 'fungal_squalene_epoxidase_inhibitor', 'anticonvulsant', 'nrf2_activator', 'dna_inhibitor', 'nicotinic_receptor_agonist', 'diuretic', 'ppar_receptor_antagonist', 'acetylcholine_receptor_agonist', 'hcv_inhibitor', 'glutamate_receptor_agonist', 'potassium_channel_antagonist', 'jak_inhibitor', '11-beta-hsd1_inhibitor', 'antihistamine', 'acetylcholinesterase_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'immunosuppressant', 'ras_gtpase_inhibitor', 'nfkb_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'monoamine_oxidase_inhibitor', 'free_radical_scavenger', 'thrombin_inhibitor', 'sigma_receptor_antagonist', 'gamma_secretase_inhibitor', 'p-glycoprotein_inhibitor', 'glucocorticoid_receptor_agonist', 'autotaxin_inhibitor', 'trpv_agonist', 'antiprotozoal', 'transient_receptor_potential_channel_antagonist', 'mucolytic_agent', 'gaba_receptor_agonist', 'chloride_channel_blocker', 'corticosteroid_agonist', 'histamine_receptor_agonist', 'progesterone_receptor_antagonist', 'glutamate_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'lipoxygenase_inhibitor', 'prostaglandin_inhibitor', 'fatty_acid_receptor_agonist', 'vegfr_inhibitor', 'focal_adhesion_kinase_inhibitor', 'raf_inhibitor', 'dna_alkylating_agent', 'acat_inhibitor', 'opioid_receptor_antagonist', 'adrenergic_receptor_agonist', 'antifungal', 'acetylcholine_receptor_antagonist', 'atpase_inhibitor', 'bcr-abl_inhibitor', 'mek_inhibitor', 'opioid_receptor_agonist', 'tlr_agonist', 'alk_inhibitor', 'ppar_receptor_agonist', 'nitric_oxide_donor', 'integrin_inhibitor', 'fgfr_inhibitor', 'leukotriene_receptor_antagonist', 'hmgcr_inhibitor', 'rna_polymerase_inhibitor', 'radiopaque_medium', 'estrogen_receptor_antagonist', 'apoptosis_stimulant', 'tubulin_inhibitor', 'caspase_activator', 'estrogen_receptor_agonist', 'histone_lysine_methyltransferase_inhibitor', 'serotonin_receptor_agonist', 'pi3k_inhibitor', 'nitric_oxide_synthase_inhibitor', 'kit_inhibitor', 'catechol_o_methyltransferase_inhibitor', 'pdgfr_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'gsk_inhibitor', 'tachykinin_antagonist', 'igf-1_inhibitor', 'cyclooxygenase_inhibitor', 'thymidylate_synthase_inhibitor', 'cannabinoid_receptor_antagonist', 'sodium_channel_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'wnt_inhibitor', 'retinoid_receptor_antagonist', 'aromatase_inhibitor', 'hiv_inhibitor', 'egfr_inhibitor', 'erbb2_inhibitor', 'protein_kinase_inhibitor', 'sigma_receptor_agonist', 'src_inhibitor', 'bacterial_dna_inhibitor', 'progesterone_receptor_agonist', 'protein_phosphatase_inhibitor', 'topoisomerase_inhibitor', 'rho_associated_kinase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'androgen_receptor_antagonist', 'steroid', 'atp-sensitive_potassium_channel_antagonist', 'atr_kinase_inhibitor', 'calcium_channel_blocker', 'cc_chemokine_receptor_antagonist', 'neuropeptide_receptor_antagonist', 'chk_inhibitor', 'hdac_inhibitor', 'histamine_receptor_antagonist', 'lxr_agonist', 'serotonin_reuptake_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'mdm_inhibitor', 'protein_synthesis_inhibitor', 'dihydrofolate_reductase_inhibitor', 'analgesic', 'p38_mapk_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'benzodiazepine_receptor_agonist', 'cytochrome_p450_inhibitor', 'tgf-beta_receptor_inhibitor', 'vitamin_d_receptor_agonist', 'tropomyosin_receptor_kinase_inhibitor', 'casein_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'monoacylglycerol_lipase_inhibitor', 'gaba_receptor_antagonist', 'insulin_sensitizer', 'bcl_inhibitor', 'btk_inhibitor', 'mineralocorticoid_receptor_antagonist', 'pdk_inhibitor', 'phospholipase_inhibitor', 'elastase_inhibitor', 'beta_amyloid_inhibitor', 'sphingosine_receptor_agonist', 'tnf_inhibitor', 'pkc_inhibitor', 'antioxidant', 'protein_tyrosine_kinase_inhibitor', 'trpv_antagonist', 'potassium_channel_activator', 'cck_receptor_antagonist', 'vitamin_b', 'histone_lysine_demethylase_inhibitor', 'leukotriene_inhibitor', 'angiotensin_receptor_antagonist', 'dopamine_receptor_agonist', 'antibiotic', 'anesthetic_-_local', 'adenosine_receptor_agonist', 'bromodomain_inhibitor', 'cannabinoid_receptor_agonist', 'adenosine_receptor_antagonist', 'faah_inhibitor', 'proteasome_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'imidazoline_receptor_agonist', 'akt_inhibitor', 'coagulation_factor_inhibitor', 'serotonin_receptor_antagonist', 'farnesyltransferase_inhibitor', 'bacterial_antifolate', 'chelating_agent', 'angiogenesis_inhibitor', '5-alpha_reductase_inhibitor', 'retinoid_receptor_agonist', 'syk_inhibitor', 'nitric_oxide_production_inhibitor', 'androgen_receptor_agonist', 'parp_inhibitor', 'membrane_integrity_inhibitor', 'prostanoid_receptor_antagonist', 'smoothened_receptor_antagonist', 'insulin_secretagogue', 'dopamine_receptor_antagonist', 'lipase_inhibitor', 'tyrosine_kinase_inhibitor', 'cholinergic_receptor_antagonist', 'mtor_inhibitor', 'adenylyl_cyclase_activator', 'ampk_activator', 'antiarrhythmic', 'orexin_receptor_antagonist', 'antimalarial', 'adrenergic_receptor_antagonist', 'atm_kinase_inhibitor', 'phosphodiesterase_inhibitor', 'cdk_inhibitor', 'anti-inflammatory'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e4cc04406341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sig_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtarget_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sig_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['calcineurin_inhibitor', 'cholesterol_inhibitor', 'hsp_inhibitor', 'ribonucleoside_reductase_inhibitor', 'glutamate_inhibitor', 'laxative', 'tlr_antagonist', 'bacterial_cell_wall_synthesis_inhibitor', 'gonadotropin_receptor_agonist', 'flt3_inhibitor', 'antiviral', 'ikk_inhibitor', 'atp_synthase_inhibitor', 'aurora_kinase_inhibitor', 'fungal_squalene_epoxidase_inhibitor', 'anticonvulsant', 'nrf2_activator', 'dna_inhibitor', 'nicotinic_receptor_agonist', 'diuretic', 'ppar_receptor_antagonist', 'acetylcholine_receptor_agonist', 'hcv_inhibitor', 'glutamate_receptor_agonist', 'potassium_channel_antagonist', 'jak_inhibitor', '11-beta-hsd1_inhibitor', 'antihistamine', 'acetylcholinesterase_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'immunosuppressant', 'ras_gtpase_inhibitor', 'nfkb_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'monoamine_oxidase_inhibitor', 'free_radical_scavenger', 'thrombin_inhibitor', 'sigma_receptor_antagonist', 'gamma_secretase_inhibitor', 'p-glycoprotein_inhibitor', 'glucocorticoid_receptor_agonist', 'autotaxin_inhibitor', 'trpv_agonist', 'antiprotozoal', 'transient_receptor_potential_channel_antagonist', 'mucolytic_agent', 'gaba_receptor_agonist', 'chloride_channel_blocker', 'corticosteroid_agonist', 'histamine_receptor_agonist', 'progesterone_receptor_antagonist', 'glutamate_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'lipoxygenase_inhibitor', 'prostaglandin_inhibitor', 'fatty_acid_receptor_agonist', 'vegfr_inhibitor', 'focal_adhesion_kinase_inhibitor', 'raf_inhibitor', 'dna_alkylating_agent', 'acat_inhibitor', 'opioid_receptor_antagonist', 'adrenergic_receptor_agonist', 'antifungal', 'acetylcholine_receptor_antagonist', 'atpase_inhibitor', 'bcr-abl_inhibitor', 'mek_inhibitor', 'opioid_receptor_agonist', 'tlr_agonist', 'alk_inhibitor', 'ppar_receptor_agonist', 'nitric_oxide_donor', 'integrin_inhibitor', 'fgfr_inhibitor', 'leukotriene_receptor_antagonist', 'hmgcr_inhibitor', 'rna_polymerase_inhibitor', 'radiopaque_medium', 'estrogen_receptor_antagonist', 'apoptosis_stimulant', 'tubulin_inhibitor', 'caspase_activator', 'estrogen_receptor_agonist', 'histone_lysine_methyltransferase_inhibitor', 'serotonin_receptor_agonist', 'pi3k_inhibitor', 'nitric_oxide_synthase_inhibitor', 'kit_inhibitor', 'catechol_o_methyltransferase_inhibitor', 'pdgfr_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'gsk_inhibitor', 'tachykinin_antagonist', 'igf-1_inhibitor', 'cyclooxygenase_inhibitor', 'thymidylate_synthase_inhibitor', 'cannabinoid_receptor_antagonist', 'sodium_channel_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'wnt_inhibitor', 'retinoid_receptor_antagonist', 'aromatase_inhibitor', 'hiv_inhibitor', 'egfr_inhibitor', 'erbb2_inhibitor', 'protein_kinase_inhibitor', 'sigma_receptor_agonist', 'src_inhibitor', 'bacterial_dna_inhibitor', 'progesterone_receptor_agonist', 'protein_phosphatase_inhibitor', 'topoisomerase_inhibitor', 'rho_associated_kinase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'androgen_receptor_antagonist', 'steroid', 'atp-sensitive_potassium_channel_antagonist', 'atr_kinase_inhibitor', 'calcium_channel_blocker', 'cc_chemokine_receptor_antagonist', 'neuropeptide_receptor_antagonist', 'chk_inhibitor', 'hdac_inhibitor', 'histamine_receptor_antagonist', 'lxr_agonist', 'serotonin_reuptake_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'mdm_inhibitor', 'protein_synthesis_inhibitor', 'dihydrofolate_reductase_inhibitor', 'analgesic', 'p38_mapk_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'benzodiazepine_receptor_agonist', 'cytochrome_p450_inhibitor', 'tgf-beta_receptor_inhibitor', 'vitamin_d_receptor_agonist', 'tropomyosin_receptor_kinase_inhibitor', 'casein_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'monoacylglycerol_lipase_inhibitor', 'gaba_receptor_antagonist', 'insulin_sensitizer', 'bcl_inhibitor', 'btk_inhibitor', 'mineralocorticoid_receptor_antagonist', 'pdk_inhibitor', 'phospholipase_inhibitor', 'elastase_inhibitor', 'beta_amyloid_inhibitor', 'sphingosine_receptor_agonist', 'tnf_inhibitor', 'pkc_inhibitor', 'antioxidant', 'protein_tyrosine_kinase_inhibitor', 'trpv_antagonist', 'potassium_channel_activator', 'cck_receptor_antagonist', 'vitamin_b', 'histone_lysine_demethylase_inhibitor', 'leukotriene_inhibitor', 'angiotensin_receptor_antagonist', 'dopamine_receptor_agonist', 'antibiotic', 'anesthetic_-_local', 'adenosine_receptor_agonist', 'bromodomain_inhibitor', 'cannabinoid_receptor_agonist', 'adenosine_receptor_antagonist', 'faah_inhibitor', 'proteasome_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'imidazoline_receptor_agonist', 'akt_inhibitor', 'coagulation_factor_inhibitor', 'serotonin_receptor_antagonist', 'farnesyltransferase_inhibitor', 'bacterial_antifolate', 'chelating_agent', 'angiogenesis_inhibitor', '5-alpha_reductase_inhibitor', 'retinoid_receptor_agonist', 'syk_inhibitor', 'nitric_oxide_production_inhibitor', 'androgen_receptor_agonist', 'parp_inhibitor', 'membrane_integrity_inhibitor', 'prostanoid_receptor_antagonist', 'smoothened_receptor_antagonist', 'insulin_secretagogue', 'dopamine_receptor_antagonist', 'lipase_inhibitor', 'tyrosine_kinase_inhibitor', 'cholinergic_receptor_antagonist', 'mtor_inhibitor', 'adenylyl_cyclase_activator', 'ampk_activator', 'antiarrhythmic', 'orexin_receptor_antagonist', 'antimalarial', 'adrenergic_receptor_antagonist', 'atm_kinase_inhibitor', 'phosphodiesterase_inhibitor', 'cdk_inhibitor', 'anti-inflammatory'] not in index\""
     ]
    }
   ],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T04:03:58.429449Z",
     "iopub.status.busy": "2020-11-26T04:03:58.428409Z",
     "iopub.status.idle": "2020-11-26T04:03:58.560536Z",
     "shell.execute_reply": "2020-11-26T04:03:58.561849Z"
    },
    "papermill": {
     "duration": 1.765629,
     "end_time": "2020-11-26T04:03:58.562050",
     "exception": false,
     "start_time": "2020-11-26T04:03:56.796421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-05b71092c0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5809.14493,
   "end_time": "2020-11-26T04:04:01.644674",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-26T02:27:12.499744",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
